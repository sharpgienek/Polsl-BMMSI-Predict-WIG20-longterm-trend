FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 4 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (4, 5, 5.00000000000000000000e-001) (4, 5, 5.00000000000000000000e-001) (4, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 1.67098268919125830000e+000) (1, 1.41959347554805350000e-001) (2, -3.10843887922182920000e+000) (3, 1.43160948978731220000e+000) (4, -7.30713318043449300000e-001) (5, 2.24186336264984130000e-001) (6, 1.08107856058312440000e+001) (7, 8.43735463026027750000e-001) (8, -5.24815449872412020000e+000) (9, 4.03172088767298710000e-001) (10, 1.62553720368421920000e-001) (11, 3.15980524860622610000e-001) (12, 7.64406056058957620000e-001) (13, 2.87500598838356440000e-001) (14, 3.51055201807905620000e-003) (15, 3.63858064229970020000e-001) (16, 3.54359686725500280000e-001) (17, -9.64264098456465300000e-002) (18, 1.35104497192703590000e-002) (19, 5.39397524712327600000e-001) (0, 4.61019390025989570000e-001) (1, -1.41869038993017150000e+000) (2, 2.17596664500754460000e+000) (3, 7.20718535197179990000e-001) (4, 6.88388622843974060000e-001) (5, 4.54368820763234540000e-001) (6, 5.40637112121028100000e+000) (7, 1.35558990523838990000e-001) (8, 3.39726602883799920000e+000) (9, -3.97250476237938740000e-001) (10, 7.61359777293867830000e-001) (11, 7.70560927337713200000e-001) (12, -2.64289968892033040000e-001) (13, -1.58673257758641520000e-001) (14, 2.11031990782161620000e-001) (15, 2.62260971590673850000e-001) (16, -1.15194086160961090000e-001) (17, 5.60654144887429820000e-001) (18, 1.09020915308113600000e+000) (19, -1.86782225330547180000e+000) (0, 7.87917044833168400000e+000) (1, 8.52551509841404710000e-001) (2, 3.17863247442719120000e+000) (3, -4.35242961279874760000e+000) (4, -7.09783976655483340000e+000) (5, -5.63235992173073010000e+000) (6, 9.33569631299013450000e+001) (7, -6.67051459253203040000e+000) (8, 2.22530130103159250000e+000) (9, 9.70244553174586020000e-001) (10, -1.14009741698449400000e+000) (11, 6.96400020264530070000e-001) (12, 5.73779260832900210000e-001) (13, -9.73292170503815890000e-001) (14, 5.02292784735385570000e+000) (15, 8.16343087762966490000e-001) (16, -3.99425939434324560000e-002) (17, 6.82455572613273470000e-002) (18, 2.78074884755845630000e+000) (19, 4.93165063988913080000e-001) (20, 1.08614825620105430000e+000) (21, 2.14427208258422610000e+000) (22, 1.27054198807268630000e+000) (23, 3.00298610199215050000e-001) (20, 1.57140260484750670000e+000) (21, -1.61768340677398370000e-001) (22, -2.40296299623756720000e+000) (23, 1.09745382537618650000e+000) (20, 9.23048996693647680000e-002) (21, -3.97735534876762300000e+000) (22, 2.07101214607708920000e+000) (23, 1.77894507882998540000e+000) 

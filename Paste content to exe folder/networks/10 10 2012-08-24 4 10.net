FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 5 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 4.72897574458687020000e-001) (1, 3.50044711452493500000e-001) (2, -1.44103012793817880000e-001) (3, -2.07536851629680720000e-001) (4, 3.55201269814097550000e-001) (5, -2.45030817449617880000e-001) (6, -1.09081039615699760000e-001) (7, -2.22917711233873380000e-001) (8, 5.81979607334135900000e-001) (9, 2.80220491662031560000e-001) (10, -9.05141448077814710000e-002) (11, 2.89599818768807800000e-001) (12, 2.42403108286008640000e-001) (13, -1.77009539398857880000e-001) (14, 4.15861950604215660000e-001) (15, 9.36123635585427550000e-001) (16, 6.09877168634086250000e-001) (17, 4.27803892278767190000e-002) (18, 5.10140319286925550000e-002) (19, 7.53688205933287450000e-001) (0, 3.35074417399919670000e-001) (1, 1.57048599935161580000e+000) (2, -4.23821046460995220000e-001) (3, -1.10759559761414610000e+000) (4, 1.10334101172005460000e-001) (5, -3.02905899275160430000e-001) (6, -9.77773818335900930000e-001) (7, -1.05542649018074310000e-001) (8, -2.34726158240886770000e-001) (9, 1.66961319809435120000e+000) (10, -1.12494178927887440000e+000) (11, -1.56264560357671920000e-002) (12, 9.98284103264578840000e-001) (13, 1.41054878144665580000e-001) (14, 2.38955361557238680000e-001) (15, 7.67684100985591590000e-001) (16, 6.68071671029869250000e-002) (17, 5.14291618044256270000e-001) (18, -1.97058800146479580000e-003) (19, 1.02253191147746380000e+000) (0, -1.30420488201505730000e-001) (1, 8.10389044102569840000e-001) (2, 6.07174659226405460000e-001) (3, -6.76477254602088300000e-001) (4, 1.87683903773075200000e-001) (5, -1.63212924232146200000e-001) (6, -7.97790732972300190000e-001) (7, -3.56980362336939140000e-001) (8, 1.07178531479701690000e-001) (9, 2.59567769447086040000e-001) (10, -9.82341242360200430000e-002) (11, 7.01107987728783020000e-001) (12, -1.00692727367291480000e-001) (13, 3.58313447569157810000e-004) (14, 5.05888728972508690000e-001) (15, 1.38358803785186670000e-001) (16, 7.10901462276289230000e-002) (17, 9.74190098498984860000e-002) (18, 9.76520618290617870000e-001) (19, 2.05881245209594500000e-001) (0, 2.26285353278836370000e-001) (1, 1.62739517988712870000e+000) (2, 1.19745758563068890000e-001) (3, -1.87391466873160110000e-001) (4, 1.38390450750686680000e-001) (5, 5.76730487032257160000e-001) (6, -1.01864295956989850000e+000) (7, 3.12039929941354820000e-001) (8, 1.39410819761556730000e-001) (9, 1.67950699706990520000e+000) (10, -1.12203041228236660000e+000) (11, -9.48987897136618490000e-002) (12, 1.29322164202784800000e+000) (13, 8.09524254326958290000e-001) (14, -4.72181708319933500000e-001) (15, -5.03652701529868520000e-001) (16, 6.21180879740024670000e-001) (17, 3.10279394144709450000e-001) (18, -6.25739044363607060000e-001) (19, 1.10403215683253730000e+000) (20, 3.38052933069303010000e-001) (21, -2.83781125169830840000e-001) (22, 2.51880955774504920000e-001) (23, -6.61267398692347520000e-001) (24, 4.91145158327240980000e-001) (20, -4.34846963251842020000e-003) (21, -1.04951093609384080000e+000) (22, -9.91829548797790280000e-001) (23, 4.06750624221306030000e-001) (24, 1.12055643669473960000e+000) (20, 4.31360674942027620000e-001) (21, 1.68489640144708860000e+000) (22, 5.53061149871967310000e-001) (23, 2.12026154140330370000e+000) (24, 1.02878191157059610000e+000) 

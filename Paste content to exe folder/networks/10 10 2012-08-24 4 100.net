FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 5 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 1.45592228114506890000e-001) (1, -2.80718707544623300000e-001) (2, 3.42784048967852970000e-001) (3, -4.71914448185884570000e-001) (4, 1.88672359132723870000e+000) (5, 2.15561633394480650000e-001) (6, 4.08592305723685990000e-001) (7, -1.13389409691705150000e-001) (8, 9.76007423424446690000e-001) (9, -9.64868271803666410000e-001) (10, 9.86315930890307220000e-001) (11, 4.91248588566917730000e-002) (12, -8.91153219203044470000e-001) (13, -3.93551589538923830000e-001) (14, 1.95311205972540240000e+000) (15, 7.83024189388111250000e-001) (16, -3.53199461511347310000e+000) (17, 8.37172164254959440000e-001) (18, 1.22893172667334500000e+000) (19, -6.97109282613930570000e-001) (0, 5.80954046833371820000e-001) (1, 2.54171447229306540000e+000) (2, 2.47897129854544430000e-001) (3, -1.10542616232992690000e+000) (4, -3.07354864624285430000e-002) (5, -1.16386641427607960000e-001) (6, -1.45182172351413200000e+000) (7, -6.61887398086532940000e-002) (8, -3.19170439770643170000e-002) (9, 1.50947012579038530000e+000) (10, -7.13116144800927780000e-001) (11, 1.75433254487645820000e-001) (12, 1.61675201696142330000e+000) (13, -1.61184604802806050000e-001) (14, 4.77840312814596400000e+000) (15, 2.36564928737850840000e+000) (16, 7.44546901768627480000e-001) (17, 2.08581215013957380000e-002) (18, -2.67254084592881300000e-001) (19, 7.37344892545083620000e-001) (0, 1.96742498819309940000e+000) (1, 9.54410847710216180000e+000) (2, -7.99108490870362040000e-002) (3, -7.63542504837798290000e-001) (4, 4.83158430518753070000e-001) (5, 1.11872706151650950000e+000) (6, -4.36841964543924650000e-001) (7, -1.72479185624868950000e+001) (8, 9.79016658409715970000e-002) (9, 8.20290633781869260000e+000) (10, -5.81435869796648520000e-001) (11, -1.05625811672192980000e+001) (12, 1.18976233295096030000e+000) (13, 4.19862452074733050000e-001) (14, 4.80614533708606180000e-001) (15, 2.02764544400312940000e-001) (16, 2.79082194550540790000e-001) (17, -6.87017806406445270000e-001) (18, 2.04777621056573550000e+001) (19, 5.24343226183243250000e-001) (0, 3.20957976150785750000e-001) (1, 6.12722564022392670000e-001) (2, -1.65026026484981880000e+000) (3, 1.55041791839239470000e+000) (4, 9.35528822201711850000e-001) (5, 1.13007856195107940000e+000) (6, 6.81086917775855280000e-001) (7, 8.76232319354171450000e-001) (8, -4.66287056882761060000e+000) (9, 2.68611977434868250000e-001) (10, 1.79211249850076140000e-001) (11, -3.42396595402534920000e-001) (12, 4.62249437045472520000e+000) (13, 5.05725236383055780000e-001) (14, -5.61940134872329080000e-001) (15, -4.26326299592122810000e-001) (16, 1.52146155475081040000e-001) (17, 5.07238531108939350000e-001) (18, 4.98469327269713960000e+000) (19, 4.31743808342493040000e-001) (20, -8.42514907073825680000e-002) (21, 4.59865847727479520000e-001) (22, -8.06169153041327190000e-001) (23, -1.42228007805235520000e+000) (24, 8.31542427986845300000e-001) (20, -3.13343288089764530000e-001) (21, -6.43749561245448240000e-001) (22, -1.02470401289918220000e+000) (23, 1.30952553835519800000e+000) (24, 8.48408800966814700000e-001) (20, -1.16621796288521180000e+000) (21, 1.16704275706044250000e+000) (22, 1.65827695255227600000e+000) (23, 5.46015724488211900000e-001) (24, 1.23401743744656470000e+000) 

FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 5 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -7.03890092827107680000e-001) (1, 9.75902292926293380000e-001) (2, 4.59944684356098500000e+000) (3, -4.90284740935178380000e-001) (4, 6.37646927372685910000e-001) (5, 6.45145919603589910000e-001) (6, -6.53375643566562610000e-002) (7, -3.86607322987968020000e-001) (8, 4.85343374937619560000e+000) (9, -5.02815622248702220000e-002) (10, 2.35061508438775520000e-001) (11, -5.80480475759720060000e-001) (12, 1.04336736035894860000e+000) (13, 8.10547550625763810000e-002) (14, 2.86405366761436410000e-001) (15, 1.84578363081439320000e-001) (16, -6.55469069083932100000e-001) (17, 9.54148136135304250000e-001) (18, 1.07227300210883690000e+001) (19, -1.29094412094660910000e-001) (0, -1.44241709471268910000e-001) (1, -2.44341430358443560000e+000) (2, 1.22257240840142850000e+001) (3, 1.78652927418939410000e+000) (4, 1.71848249434824770000e-001) (5, 1.52300478312849040000e-001) (6, 2.15738682156924310000e+000) (7, 1.42651653934937640000e+000) (8, 1.05097518950281900000e+000) (9, -1.89579453249295900000e+000) (10, 2.02676626394349270000e+000) (11, 4.52065422351755400000e-001) (12, -2.03600096312664710000e-001) (13, 3.47418853023286230000e-001) (14, 1.38704459522491970000e-001) (15, 5.87210456527406550000e-001) (16, 8.37985981675448580000e-001) (17, -2.72311928847542350000e-001) (18, 2.50627887429425440000e+001) (19, -6.69292655799515580000e-001) (0, 1.70443211770958420000e-001) (1, 1.30442795706695460000e+000) (2, -3.29838573299229890000e-001) (3, -3.52700489967019970000e-001) (4, 2.18498840810792770000e-001) (5, 6.73548932110605740000e-001) (6, 5.92663992754799420000e-002) (7, 7.30446639126650980000e-001) (8, -1.39189529254852350000e+000) (9, 2.61372967257344600000e+000) (10, 1.55880942442519180000e-001) (11, -1.45977664660495050000e-001) (12, 1.54649883429265870000e+000) (13, 2.17848850258776490000e+000) (14, -9.12922471552177210000e-002) (15, -5.66584574200626020000e-001) (16, 5.11850249285095370000e-001) (17, 9.68929900914706210000e-002) (18, -5.42618755434658650000e-001) (19, 6.78924411526630860000e-001) (0, 3.39375419085650520000e+000) (1, -7.96630579656072760000e-001) (2, -2.82032451920529680000e+000) (3, 2.02545626461248940000e+000) (4, 1.84704319843801620000e+000) (5, 1.36368418648207610000e+000) (6, 3.46411758502957620000e+000) (7, 1.50761977463475990000e+000) (8, -1.17550965197722170000e+000) (9, -2.04301735122821850000e-001) (10, 1.21872404129522470000e+000) (11, -4.80830301183342690000e-001) (12, 3.31958664759505810000e-001) (13, 3.59148237281079140000e-001) (14, 1.00691679663744790000e+000) (15, -1.08464005054238610000e+000) (16, 2.36053779409155170000e-001) (17, 1.86764612807352260000e-001) (18, 2.58183311983224020000e+000) (19, -3.93062537868414330000e-001) (20, -1.40227157824511850000e+000) (21, 6.80542080420113440000e-001) (22, -1.05515387486877990000e+000) (23, -1.54855933443367390000e+000) (24, 4.89883733603806880000e-001) (20, -1.48109040464174150000e+000) (21, 3.40451331815111340000e-001) (22, 8.07188092383353320000e-001) (23, 2.29107885318851330000e+000) (24, 1.24935619043864740000e+000) (20, 4.13293411440813440000e-001) (21, -2.15467385680854930000e+000) (22, 1.74934124378585150000e+000) (23, -1.67740572575429200000e+000) (24, 2.56576823479355200000e+000) 

FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 8 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (8, 5, 5.00000000000000000000e-001) (8, 5, 5.00000000000000000000e-001) (8, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 9.51671152500904500000e-001) (1, 8.26463564653950210000e-001) (2, 3.37389426109034470000e-002) (3, -8.02462754189872780000e-001) (4, -9.03232853830894520000e-002) (5, -7.27107939010125380000e-002) (6, -7.04006942175891880000e-001) (7, -1.61350957627399650000e-001) (8, 4.70047458435842890000e-001) (9, 7.56639344863488270000e-001) (10, -5.33154814468369540000e-001) (11, 5.02524039743905690000e-001) (12, 9.30495136377250010000e-001) (13, 2.76120530629102410000e-001) (14, 6.62199073334978770000e-002) (15, 1.16122209909098380000e+000) (16, 1.35690841036537030000e-001) (17, 8.68558939691230350000e-002) (18, 1.43081899898514440000e-001) (19, 4.84451893816606470000e-001) (0, 1.04709351720405850000e+000) (1, 1.14548081615134350000e+000) (2, -6.93057358577676360000e-001) (3, -8.01034531512340610000e-001) (4, 3.58409622781366620000e-001) (5, 5.23113026212353870000e-003) (6, -3.39768426830822530000e-001) (7, -4.16776255036558570000e-001) (8, 1.03799211678596380000e+000) (9, 1.37599134809159280000e+000) (10, -5.49973881460937620000e-001) (11, -3.02317111729765220000e-001) (12, 8.41519918088033720000e-001) (13, -1.38134710461882580000e-001) (14, 1.79235669988634470000e-001) (15, 7.47224582790096960000e-001) (16, 1.94540939569721820000e-001) (17, 3.13567092816285860000e-001) (18, -1.11456694648806280000e-001) (19, 3.09914505818546520000e-002) (0, -4.30420504592783300000e-001) (1, -1.17463248867851580000e+000) (2, 4.56712551690652340000e-001) (3, 1.10950701042974400000e+000) (4, 5.77176321657546090000e-002) (5, 2.65960935424463200000e-001) (6, 1.05089921870730190000e+000) (7, 1.18930080076487770000e-001) (8, 3.84986685538696030000e-002) (9, -3.69878903675496360000e-001) (10, 6.68223816202244250000e-001) (11, 4.46470498043159800000e-001) (12, -5.68205082904478690000e-001) (13, 3.31711521132488090000e-002) (14, -1.38012640147563590000e-001) (15, 3.95088285259420860000e-001) (16, -1.03572235620798440000e-001) (17, 1.82358350671148030000e-001) (18, 2.12744457358062800000e-001) (19, -6.81189572177189230000e-001) (0, -3.86382882368732690000e-001) (1, 3.12274009234699890000e-001) (2, 3.02606561367252110000e-001) (3, 8.01095305242734530000e-001) (4, -2.22895967867610570000e-001) (5, 4.83681085317152090000e-001) (6, -2.34107070124977950000e-001) (7, 1.20201768222818340000e+000) (8, -7.79411477448682000000e-001) (9, 1.41674033067487670000e+000) (10, -1.50870475561897780000e-001) (11, -2.45845724807509660000e-002) (12, 1.17696733582023820000e+000) (13, 1.00886635778139540000e+000) (14, -9.76702489654898360000e-001) (15, -5.41090318183881310000e-001) (16, 2.27749749024530660000e-001) (17, 8.34275005183767460000e-002) (18, -4.85425429830754930000e-001) (19, 4.43521293555698970000e-001) (0, 5.49421935415421950000e-001) (1, 7.61148316216729630000e-001) (2, -3.68816767265514210000e-001) (3, 3.54272387115017740000e-003) (4, -6.48100949598854610000e-002) (5, -2.65862538884187590000e-001) (6, 2.18553975119498280000e-002) (7, 3.77028500172151230000e-001) (8, -2.56632477486077590000e-001) (9, 2.92546289478759440000e-001) (10, -1.47131561550496980000e-001) (11, 1.20827984668672130000e-001) (12, 2.77460427489585830000e-001) (13, -2.95465379367418790000e-001) (14, 5.39506732376783040000e-001) (15, 4.81986977573862610000e-001) (16, 9.89716280565151600000e-002) (17, -1.04971228370832990000e-001) (18, -4.98503317936993400000e-002) (19, 4.56466093829005100000e-001) (0, 4.98410774062795160000e-001) (1, 2.52584248110415690000e-002) (2, 4.91758574414418890000e-001) (3, 4.12332752116947530000e-001) (4, -1.23918264543215170000e-001) (5, -4.42229127705106920000e-001) (6, 6.10290593119001470000e-001) (7, -1.81771887078886890000e-001) (8, 4.94473273317788340000e-001) (9, 2.18332766768884750000e-001) (10, 2.09231483933680240000e-001) (11, 3.10815501410536880000e-001) (12, 9.20998547316177890000e-002) (13, 3.39603398649505890000e-001) (14, -1.22947944729863940000e-001) (15, 8.10341807386014000000e-002) (16, 6.11367195046061270000e-002) (17, -1.53996529176898880000e-001) (18, -5.38197584275567080000e-002) (19, 8.96401378980901530000e-002) (0, 2.24198931944950830000e-001) (1, 1.82876578634677940000e+000) (2, -2.14632772172578530000e-002) (3, -1.88854577910282760000e-001) (4, -1.06819305981683560000e-001) (5, 4.95541488068960640000e-001) (6, -5.30878203106320390000e-001) (7, 1.65207654816980580000e-001) (8, -9.55920029547859600000e-001) (9, 1.07677995916082290000e+000) (10, -5.58594267972446930000e-001) (11, -1.06084817835824240000e-001) (12, 1.73923836718586640000e+000) (13, 7.41079862702195720000e-001) (14, -2.97270957041073480000e-001) (15, -8.13985907002650500000e-001) (16, 5.74110247454142080000e-001) (17, 1.34951597810804850000e-001) (18, -2.72236551577695660000e-001) (19, 1.10420451499916620000e+000) (20, 4.27147351163006230000e-001) (21, 2.66316040401564870000e-001) (22, 6.63122182172408260000e-001) (23, -3.82199452772893800000e-001) (24, 4.73161866962507010000e-001) (25, 2.98024731642913380000e-001) (26, -5.03367681774181120000e-001) (27, 3.01241519429644000000e-001) (20, -8.94960650869955800000e-001) (21, -9.66274128495109410000e-001) (22, 2.39474404306209860000e-001) (23, 1.41194907083785640000e+000) (24, 8.90823455504528430000e-002) (25, 2.91421852864711780000e-001) (26, 4.08176397718198190000e-001) (27, 3.41513235395035470000e-001) (20, 7.67919422569355530000e-001) (21, 1.15741960879994730000e+000) (22, -6.61853546854880070000e-001) (23, 5.72576969510149690000e-001) (24, 3.58415574192281370000e-001) (25, 6.13634547412907630000e-002) (26, 6.42089909999098210000e-001) (27, 9.71227218095774840000e-001) 

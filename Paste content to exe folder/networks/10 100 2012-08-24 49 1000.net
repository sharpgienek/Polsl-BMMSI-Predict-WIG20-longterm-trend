FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 5 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 3.21168683246124260000e+001) (1, -1.91429922728950070000e+000) (2, -8.55656049402611250000e+001) (3, 2.91102473535352550000e+000) (4, 3.70469180734398260000e+001) (5, 1.82325971060125600000e+000) (6, -1.93225833876548730000e+001) (7, 5.17075479920099210000e+000) (8, 1.79346703332777420000e+002) (9, 9.12065897874204780000e-001) (10, 5.16310877951055860000e+000) (11, 1.64692110451333850000e+001) (12, -2.54445541604365790000e+002) (13, 5.20603677255892410000e-001) (14, 1.71027004553426140000e+002) (15, -1.99166091668056500000e+000) (16, 4.56612354553179430000e+001) (17, -7.91945930302359540000e+000) (18, -3.16446097024016520000e+001) (19, 8.01365232935202170000e+000) (0, -3.35560432995974480000e+001) (1, 4.07119227458693830000e-001) (2, 4.56089227677581020000e+001) (3, -1.06261056636755060000e+001) (4, -1.64254195697370510000e+002) (5, -4.00495952265378200000e+000) (6, 7.52356842543200520000e+000) (7, -4.87856901125721090000e+000) (8, -2.06074149181696740000e+001) (9, 5.48963143004684300000e-001) (10, -1.04830635916621150000e+002) (11, -1.79476938943606010000e+000) (12, -4.20401271016316790000e+000) (13, 1.05388076833363310000e+000) (14, -6.52938555580971920000e+001) (15, -1.54798628283963800000e+001) (16, -1.49903415055218550000e+001) (17, -3.39887823367377160000e-001) (18, -1.77380407947103010000e+001) (19, 8.93734502276892330000e+000) (0, 1.24121193064477650000e+002) (1, -5.67836911654906240000e-001) (2, 5.47529558028923090000e+001) (3, 1.74445628977246140000e+000) (4, -8.42469748379329890000e+000) (5, 6.44556984569373090000e+000) (6, -3.34821989227162150000e+001) (7, 8.20754565070117240000e-001) (8, -6.45791778624065960000e+001) (9, 6.82950269899984710000e+000) (10, 7.87729180995349480000e+001) (11, -2.60192210017021000000e+000) (12, 2.75930583981784960000e+001) (13, 9.57796833211464920000e-001) (14, -3.64183503473362360000e+001) (15, 6.23810329003605620000e+000) (16, -6.91336247302003670000e+001) (17, 1.50461130948809820000e+000) (18, -5.64849705140450380000e+001) (19, -2.80807381442485580000e+000) (0, -9.49747380829014960000e+001) (1, -3.43015353106045640000e-001) (2, -7.01508328463097540000e+001) (3, -3.82206980725257360000e+000) (4, -1.53111518382494880000e+001) (5, -2.55112794647371820000e+000) (6, -1.86633422916452890000e+001) (7, -3.96257336552204190000e-001) (8, 4.08463894291237250000e+001) (9, -8.67688817047812750000e-001) (10, -1.33224253897476930000e+002) (11, 5.71566810171922230000e-001) (12, -1.64416809241629520000e+002) (13, -5.48468576500992900000e-001) (14, -3.42808595333948740000e+001) (15, -5.84249772522905350000e+000) (16, 4.52578292841384450000e+001) (17, -1.73342148081108440000e+000) (18, -1.77348049203101470000e+001) (19, 1.27272744504710540000e+001) (20, -2.05899440523619900000e+000) (21, -2.09133394664085910000e+000) (22, -2.08228653043498380000e+000) (23, -4.33936307990511750000e-002) (24, 2.07048020314971380000e+000) (20, 1.32045593183462720000e+000) (21, 1.38089742504561540000e+000) (22, 3.35522939300653410000e-002) (23, -1.34612493896318570000e+000) (24, 1.30216372749175440000e+000) (20, 5.13959714884099990000e-002) (21, 1.49455560256608140000e-001) (22, 1.35182086268430290000e+000) (23, 1.32481695085953000000e+000) (24, 1.76942988788613390000e-002) 

FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 6 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (6, 5, 5.00000000000000000000e-001) (6, 5, 5.00000000000000000000e-001) (6, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 1.20366182268884200000e+000) (1, 7.63565167535508290000e-001) (2, 4.32836950111379080000e+001) (3, -7.10500022181479230000e-001) (4, -6.60027639542409790000e+000) (5, 3.28583590915568190000e+000) (6, -1.51547860377130330000e+000) (7, -2.76161656236297630000e-001) (8, 6.33673200879413210000e+000) (9, 6.58067605761845800000e+000) (10, 1.49790432305261980000e+001) (11, 2.63164725535449630000e+000) (12, -2.29864243489814380000e+000) (13, 2.63277457537670180000e+000) (14, -8.06186501508008390000e+000) (15, 3.12899216113206370000e-001) (16, -1.14342302365562890000e+001) (17, -1.98837948336505320000e+000) (18, 3.90268643771582690000e+001) (19, 2.90111705075203950000e-001) (0, -1.01282821284611370000e+001) (1, 2.40496376700115810000e+000) (2, -3.81014518610489860000e+001) (3, 1.07263826516251890000e+001) (4, 1.66791564157491120000e+001) (5, 4.64445725687456460000e-001) (6, -2.52725100900839490000e-001) (7, 1.98228019074047630000e+000) (8, 5.65333635840884270000e+001) (9, -3.64411635224036970000e-001) (10, 7.17990409452316940000e+000) (11, 8.95115218767702280000e-001) (12, 7.53925942128456940000e+000) (13, -3.49178545849895060000e-002) (14, 5.55270452106383910000e+001) (15, 1.04510278673109620000e+000) (16, 2.16548475006492890000e+001) (17, 1.13262731353278270000e+000) (18, 3.41203871827165410000e+001) (19, 2.29798817890287040000e+000) (0, -1.29511100793928050000e+001) (1, 2.66445531804399010000e-001) (2, 9.19579180176629580000e+000) (3, 4.80428747410305610000e-001) (4, -1.19073523485532600000e+001) (5, 3.15904783520586730000e+000) (6, -2.90428771446014550000e+001) (7, -9.17597903703355480000e-002) (8, -4.71411073588172510000e+000) (9, 3.04579616456780800000e+000) (10, -4.27878335855347700000e+001) (11, 4.92758159725783800000e-001) (12, -2.58987194879123380000e+001) (13, 8.04875798984658000000e-001) (14, 3.00474560025658090000e-001) (15, 1.96571766635507990000e+000) (16, -2.02849615164735900000e+001) (17, 1.44138190517402540000e-001) (18, -1.24970207769603280000e+001) (19, -3.34193508395288110000e-001) (0, 8.00306690559272300000e+000) (1, 8.45747992146018700000e-001) (2, 3.22268395119341380000e+001) (3, 1.68885419089644010000e+000) (4, -1.42675817080535680000e+001) (5, 1.86794751004248470000e+000) (6, -1.82169412937857360000e+000) (7, 1.19475124596103880000e+000) (8, -1.88279808678148950000e+000) (9, 2.20857663563217340000e+000) (10, 7.50608259814830530000e+000) (11, 1.13504835505355210000e-001) (12, -1.82581942892236130000e+000) (13, -2.17264165808337180000e+000) (14, 4.48277920035498580000e+000) (15, -1.73730397525716170000e+000) (16, 6.56329569296276370000e+000) (17, -8.92127813746408260000e-001) (18, -2.37995124208140090000e+000) (19, -3.32245019176454600000e-001) (0, -2.11195936034468000000e+000) (1, 6.25599513857143520000e-001) (2, 2.21911726436336740000e+001) (3, 1.18374174857909040000e+000) (4, -2.20958756422593080000e+000) (5, 5.90293946466994730000e-001) (6, -8.04079323724976900000e+000) (7, 4.32303217668489110000e-001) (8, -7.62890280544702030000e+000) (9, -1.38548695058795630000e-001) (10, 2.33167176672798470000e+000) (11, -4.33532526964285010000e-001) (12, -6.14532331619428440000e+000) (13, -2.67500199011090300000e+000) (14, 7.33327072791099540000e-001) (15, -1.86931236212930840000e+000) (16, 2.48768737883958080000e+000) (17, -2.67311682842713060000e-001) (18, -3.10692664969473040000e+001) (19, 5.39157174115628250000e-001) (20, -1.19334614342545910000e+000) (21, -5.05337314252555030000e-003) (22, 1.13121528808658190000e-001) (23, 2.05233699484031050000e+000) (24, -2.07112382618959320000e+000) (25, 8.82081119952349170000e-001) (20, 1.77471097412145170000e+000) (21, 9.34739447667303770000e-001) (22, -1.37383385274520210000e+000) (23, -1.45123053799243910000e+000) (24, 1.33094326129124200000e+000) (25, 5.06342641424634920000e-001) (20, -3.51830287940480880000e-001) (21, -1.27344654755662880000e+000) (22, 8.46365690541799780000e-001) (23, -3.72724114081637190000e-001) (24, 5.51826012408829560000e-001) (25, 1.42513183655484020000e+000) 

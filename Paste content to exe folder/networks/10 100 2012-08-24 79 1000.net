FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 8 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (8, 5, 5.00000000000000000000e-001) (8, 5, 5.00000000000000000000e-001) (8, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -1.44663182395191140000e+002) (1, 2.95273617579247360000e-001) (2, 3.51298888803953560000e+001) (3, 4.12762921523117750000e+000) (4, -1.22179315590607080000e+000) (5, 3.63622736088339770000e+000) (6, 9.95492694687264820000e+000) (7, 9.38354749423693900000e-001) (8, -6.82682181094233350000e+000) (9, -4.00733982826739150000e+000) (10, -1.78543143223306910000e+001) (11, 1.47011438181966360000e+000) (12, 6.32191811939305110000e+000) (13, 6.34574293952535840000e-001) (14, 1.54402789656075080000e+001) (15, -2.01276884640994870000e+000) (16, 6.18702910208200480000e+001) (17, -1.67121472524966520000e+000) (18, -3.13661899958365070000e+001) (19, -3.32460527413614140000e+000) (0, 1.49755181807030940000e+001) (1, -7.69233686327107310000e-001) (2, -1.78413941263656390000e+001) (3, -9.11412678840489220000e+000) (4, -4.05695892875127410000e+001) (5, -1.65574012691149370000e+000) (6, -1.55912386185966500000e+001) (7, -1.45186616292893870000e-001) (8, 5.24443647221835360000e+000) (9, 1.16442572962583420000e+000) (10, 6.16557609626105400000e+000) (11, -5.53235090830373190000e+000) (12, 9.95070170613517040000e+001) (13, 1.08796181410482480000e+000) (14, 1.23310196006472880000e+001) (15, 1.35927862785564790000e+001) (16, -1.98541772729148280000e+001) (17, 1.45102906237252150000e+000) (18, 4.86787352530619890000e+000) (19, -1.25601458184088410000e+000) (0, 1.03191771607141320000e+001) (1, 2.81300876660603590000e+000) (2, -3.02692336531095520000e+001) (3, -7.87513896612730750000e-001) (4, -3.81536838735100540000e+000) (5, -3.97200411536873910000e-001) (6, 1.06158843261277680000e+001) (7, 5.71415696162453580000e-001) (8, 1.59058765823936190000e+001) (9, 1.86581918842897520000e+000) (10, -2.18797768138587580000e+001) (11, 2.39663686651411690000e-001) (12, -2.15344018061497700000e+001) (13, 5.89170028866120690000e-001) (14, 3.15801396544084500000e+000) (15, 1.03383043302985670000e+000) (16, -2.70719309838667000000e+001) (17, 1.22468685708629900000e+000) (18, -2.03358497300567900000e+000) (19, 1.67580079490175790000e+000) (0, 2.91463931742648830000e+001) (1, 9.92823152180399400000e-001) (2, 1.29011886115343530000e+001) (3, 4.79776583391612390000e-001) (4, 6.20119428421136210000e+000) (5, 9.82317375064622360000e-001) (6, -1.41824424340436380000e+001) (7, -9.85341830778321600000e-001) (8, -2.72632480050898880000e+001) (9, 2.22134086256527490000e+000) (10, -2.16189345889805580000e+001) (11, -2.22231297636776850000e-001) (12, -1.19794212727285490000e+001) (13, 6.63073038457938520000e-001) (14, 1.34341036840613270000e+001) (15, 2.51228976628112880000e+000) (16, 1.29509265231128590000e+001) (17, 8.20509091219944420000e-001) (18, -4.81529699181031750000e+000) (19, -1.46483976035059470000e+000) (0, 2.73171764656331040000e+001) (1, 1.39230063866179180000e+000) (2, 2.13291846881778260000e+001) (3, 4.79987771231421160000e+000) (4, 1.56863171503076090000e+001) (5, 1.58163394573477790000e+000) (6, -1.50430224151996640000e+001) (7, -2.46808482704695490000e-001) (8, -9.52310839357472720000e+000) (9, 1.41921896566724710000e+000) (10, 3.07400161732773470000e+001) (11, 4.87583598584839630000e-001) (12, 1.02065542358724070000e+001) (13, -2.50912778826679010000e-001) (14, 4.71664343872617580000e+000) (15, -1.39105367921983490000e+000) (16, 2.05319753293658210000e+001) (17, 4.39764503705464100000e-001) (18, 2.91428393918151230000e+001) (19, -2.62661118864715260000e+000) (0, -4.89732000981455240000e+000) (1, 1.35222475095383300000e+000) (2, -6.89579223250481470000e+000) (3, -3.91140643978977580000e-001) (4, -8.69547565815916100000e+000) (5, 7.05508707639402540000e-001) (6, -8.65561615119144530000e+000) (7, 9.57983085021406750000e-002) (8, -6.35840071040296980000e+000) (9, -1.11124847772651190000e-002) (10, 1.66525565727849990000e+001) (11, 7.63037453996670780000e-001) (12, 1.00107899896276710000e+001) (13, -1.59610436408909650000e-001) (14, 1.01470948267971220000e+001) (15, 4.16038499274179460000e-001) (16, -1.92647750912179210000e+001) (17, -5.57980983506493720000e-001) (18, -1.10097789481478310000e+001) (19, 1.64355432863055140000e+000) (0, -4.52670426499853990000e+001) (1, -7.31349739042439050000e-001) (2, -3.24603769781680360000e+001) (3, -1.58904164366073570000e+000) (4, -2.78279258928124840000e+001) (5, -4.23914814255215160000e-001) (6, -1.90757517692527440000e+001) (7, 1.94800285495471500000e+000) (8, -8.46118018899302800000e+000) (9, -1.45931369886264030000e+000) (10, -7.10434363760441290000e+000) (11, 1.03061773616836480000e+000) (12, -1.75757582763505130000e+001) (13, -1.56114319143672820000e+000) (14, -2.02364340636365260000e+001) (15, -3.23535192727179010000e+000) (16, -4.13439595592709490000e+001) (17, -1.40637001622084680000e+000) (18, -2.15249031676578110000e+001) (19, 2.52684766548673910000e+000) (20, 9.77452211203004630000e-001) (21, 1.91629592084563180000e+000) (22, 2.51865889942314160000e+000) (23, -2.62894910707022780000e+000) (24, 2.09575584283435300000e+000) (25, 1.51068346266185070000e-001) (26, -9.15722435980068420000e-001) (27, 1.12412034687346410000e-001) (20, -1.65223871373604700000e+000) (21, -2.52677243249979360000e+000) (22, -2.43476644940038560000e+000) (23, -1.50934379183800870000e-001) (24, -1.76042427773624580000e+000) (25, 1.84987101187495710000e+000) (26, -1.74534151615949700000e+000) (27, 1.56244299265031210000e+000) (20, 7.02628548629470000000e-001) (21, 8.57458811308798860000e-001) (22, -2.33527992005804460000e-001) (23, 2.70357520026620660000e+000) (24, -1.94294958847417760000e-003) (25, -1.95422610966751460000e+000) (26, 2.89827473860214060000e+000) (27, 1.54722255948784860000e+000) 

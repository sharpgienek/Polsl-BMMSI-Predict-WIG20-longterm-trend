FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 7 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (7, 5, 5.00000000000000000000e-001) (7, 5, 5.00000000000000000000e-001) (7, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -2.41435924563515370000e-001) (1, -9.68198353546230600000e-002) (2, 2.67035354116315680000e-001) (3, -1.18196403521365440000e-001) (4, -6.94619208376553440000e-002) (5, 1.80795412007576040000e-001) (6, 4.11916196004518400000e-001) (7, 9.43452154068677300000e-002) (8, 2.07563164155704140000e-001) (9, -7.01718857387876730000e-002) (10, -8.91963898497920570000e-002) (11, -1.65797722590055370000e-001) (12, 6.50168409318593320000e-002) (13, -1.60943371466388210000e-001) (14, 5.62087096679739110000e-001) (15, -4.28830669727775920000e-002) (16, -4.27110719276700080000e-001) (17, -1.70506076234434260000e-001) (18, 6.28808070498228650000e-002) (19, -9.78980476089956820000e-003) (0, -1.70511678664300840000e-001) (1, 2.76985084159753410000e-002) (2, 1.27817037227231660000e+000) (3, 2.03105273107091550000e-002) (4, 5.76948791294354060000e-001) (5, 8.80714371125473900000e-003) (6, -7.09182438644082260000e-001) (7, -1.28539286880899160000e-001) (8, 4.08580489837713170000e-001) (9, -1.59111797102090060000e-001) (10, 6.57485254152132660000e-001) (11, 7.15617487775368490000e-002) (12, 4.03694394181412450000e-001) (13, -1.18811969568256570000e-002) (14, 2.45097792915545290000e-001) (15, -3.50029085339735590000e-002) (16, 8.67413005859616560000e-001) (17, 5.44236302665624530000e-003) (18, -1.81794119802450920000e-001) (19, -1.25194560268558850000e-001) (0, 1.73798460972826280000e-001) (1, -5.12138462755353520000e-002) (2, 7.97282537654038500000e-002) (3, -8.67506882433487230000e-003) (4, 5.16908507196559120000e-001) (5, -1.93307100048465820000e-002) (6, -7.49142156036403440000e-001) (7, -2.20251968886260700000e-001) (8, 3.63544703499209310000e-001) (9, -5.02723108101478240000e-002) (10, 5.52065330706253410000e-001) (11, 2.62797656809077370000e-002) (12, 6.14638341257712880000e-001) (13, 1.11641754102559710000e-001) (14, 3.19920792077369840000e-001) (15, -7.96379189647120210000e-002) (16, 6.81056983894535280000e-001) (17, -2.69123737835918890000e-002) (18, -3.89072279332783880000e-002) (19, -1.03081522829673940000e-001) (0, -7.41593245102504240000e-002) (1, 3.17157728263776030000e-001) (2, 1.22043320395655690000e+000) (3, 1.71174948217971800000e-001) (4, 1.09393784074350410000e+000) (5, 1.57520889102795180000e+000) (6, -3.12168948579433000000e+000) (7, -6.19600034313371520000e-001) (8, -3.45328114332843050000e-001) (9, 1.95716221379050560000e+000) (10, -3.09599645163478320000e-001) (11, 2.82610992585536070000e-002) (12, -1.33933702147466920000e+000) (13, 1.86063589995617560000e-001) (14, -3.15274313773629470000e-001) (15, 3.98485241360907520000e-001) (16, -3.23109395091540350000e-001) (17, 1.82677532315154740000e-001) (18, -1.10402206464102410000e+000) (19, 4.63153824189287360000e-001) (0, -1.41098774601122830000e+000) (1, -1.43378188443877120000e-001) (2, 3.02880008011329500000e+000) (3, -1.86794238076142190000e+000) (4, -4.70354128124202080000e-001) (5, 7.64069463334229320000e-001) (6, -1.27143906885997500000e+000) (7, 2.26168059971618600000e-001) (8, -7.07742008935118180000e-001) (9, 8.46411047754089490000e-001) (10, 9.71866136808422150000e-001) (11, -1.75429988175356000000e+000) (12, -1.31463975309746540000e+000) (13, 2.10276091796066660000e-002) (14, 1.29023557886997490000e-001) (15, 9.41921467228130860000e-001) (16, 1.87643988363942470000e-002) (17, 3.12053726473233100000e-001) (18, -7.17567956219755220000e-001) (19, 2.30929340471176110000e-002) (0, -2.05216739130932880000e+000) (1, -1.23845683295334030000e-001) (2, -3.73984120619225810000e-001) (3, 1.31638640267440350000e-003) (4, -8.16683660949100170000e-001) (5, -4.40226565372450200000e-001) (6, -5.04853747441841310000e-002) (7, 6.49683862296414620000e-002) (8, 4.35224392067319890000e-001) (9, -1.43328105460644730000e-001) (10, 3.08466555946138050000e+000) (11, 1.09843552967383640000e-002) (12, 3.84748317096417770000e-001) (13, -2.76970313695053210000e-001) (14, 1.33260762894286990000e+000) (15, -1.51409160268561830000e-001) (16, -6.10496589620526220000e-001) (17, 2.38060594343308880000e-002) (18, 3.18125369566628090000e+000) (19, -1.29644023225486020000e-001) (20, 1.10950123333122570000e-001) (21, -5.34067419875837610000e-001) (22, -5.32492712821122650000e-001) (23, -1.61622946310807260000e-001) (24, 1.30717576715930830000e-002) (25, -7.94769878810748510000e-002) (26, 5.15291844058453190000e-001) (20, 3.32166065614791340000e-001) (21, -4.79041152391871780000e-001) (22, -3.31594173637120460000e-001) (23, 5.52166657855616000000e-002) (24, -5.10099010333425970000e-001) (25, -4.61310439237038570000e-001) (26, 7.45248860490760330000e-001) (20, 1.46192313631832190000e-001) (21, -1.97044746924233520000e-001) (22, -2.63072579939374880000e-001) (23, 3.98708561897641090000e-001) (24, 1.84518348799800840000e-001) (25, -1.53781188207043700000e-001) (26, 5.08367975425007890000e-001) 

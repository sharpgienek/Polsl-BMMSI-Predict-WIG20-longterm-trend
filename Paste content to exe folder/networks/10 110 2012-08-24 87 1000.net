FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 8 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (8, 5, 5.00000000000000000000e-001) (8, 5, 5.00000000000000000000e-001) (8, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -5.95458701345575530000e+001) (1, 4.66082852062465400000e+000) (2, -1.83641372555174290000e+002) (3, 2.42685441263936280000e-002) (4, -6.72436169588210840000e-001) (5, -9.64265537799735420000e-002) (6, 3.29784439053732610000e+001) (7, 9.40819270144092280000e+000) (8, 2.19165041312781000000e+002) (9, -3.39051188766523830000e+000) (10, 7.04699929561511450000e+001) (11, 3.66636261294287500000e+000) (12, 8.69305771547452930000e+001) (13, 5.47229914333033740000e+000) (14, 9.16858512909659140000e+001) (15, 1.44758792744384640000e+001) (16, -7.44390138831508210000e+001) (17, 1.82617020567087910000e+000) (18, 1.71803054217475530000e+002) (19, 1.00707853996292030000e+000) (0, 3.36346923177798370000e+001) (1, -1.34851374392797330000e+000) (2, -1.27096751891598230000e+001) (3, -3.14081409757460790000e-001) (4, -4.04821469267983080000e+001) (5, 1.87699386370116760000e+000) (6, -1.23423789605518780000e+001) (7, 4.91185317765527600000e-001) (8, -1.50117616859399340000e+001) (9, -1.43682008128378750000e+000) (10, 2.39150971734184520000e+001) (11, -5.73298960946142340000e+000) (12, 6.90728264525744430000e+000) (13, 7.65708363789031070000e-001) (14, -3.45491664509200720000e+000) (15, 4.83532069460191490000e+000) (16, 1.55867089193539830000e+001) (17, 1.37887125226359950000e+000) (18, 4.63112272270504800000e+001) (19, 1.93826574866144910000e-001) (0, 6.71893479022937700000e+000) (1, 2.12632263341886500000e+000) (2, -1.00669295912352080000e+002) (3, 3.26268954904384790000e-001) (4, -3.56760213095254030000e+001) (5, -3.64554172153321920000e-001) (6, -3.22183737135761130000e+001) (7, 4.28711747736065620000e+000) (8, -1.56646049253475880000e+001) (9, -6.20768558333403280000e-001) (10, 1.64471036397373390000e+001) (11, 2.70025184773237560000e-001) (12, -1.00177987258530190000e+001) (13, 1.33055032539544290000e+000) (14, 1.62638802653624030000e+001) (15, 2.36448822744172230000e-001) (16, -4.58760176068983650000e+001) (17, 6.21189502453372010000e-001) (18, -3.62437212347944420000e+001) (19, -2.24700614551790200000e+000) (0, -4.09699376769536560000e+001) (1, 4.81140738688075940000e-001) (2, -5.21674672578755650000e+001) (3, -5.02512078424091490000e-002) (4, 1.45933454877909390000e+001) (5, -2.26360013869398110000e-001) (6, -2.47587369036733060000e+001) (7, 1.72208798033221820000e+000) (8, -2.61984227403083880000e+001) (9, 6.05309880122921750000e-001) (10, -2.00693386522775050000e+001) (11, 1.33653461758302560000e-001) (12, 1.12516356543785490000e+000) (13, 1.21875385774964290000e-001) (14, 1.45551128650305250000e+001) (15, 8.35984911255366890000e-001) (16, -2.65392348142009200000e+000) (17, 8.09772528843810790000e-001) (18, 5.32493038310998480000e+000) (19, 1.96678838790414100000e+000) (0, -4.57100132531402820000e+001) (1, 1.36654737571881690000e+001) (2, 3.44106226080222650000e+002) (3, -7.98517575417056700000e-001) (4, 1.11269858960296690000e+002) (5, 5.66625053456855280000e+000) (6, -5.59314803002303760000e+001) (7, -1.58667368139665130000e+000) (8, -1.25370436767885720000e+002) (9, 1.68760175479449440000e+001) (10, 3.07271603447181820000e+001) (11, 6.04598639918695730000e+000) (12, 9.27889567250458640000e+001) (13, 1.87878299908341640000e+000) (14, -7.70193541032850600000e+000) (15, 1.67972700340297290000e+001) (16, 1.06818345625495310000e+002) (17, 6.31540264078631890000e+000) (18, -1.66826727008962510000e+002) (19, 4.70963891148703870000e+000) (0, -5.39948669538821380000e+001) (1, -4.68358815727185450000e+000) (2, -7.91725713433207400000e+001) (3, 2.57527056723311220000e-001) (4, -3.88587349176927220000e+001) (5, 1.57889683335938420000e-001) (6, -4.51628438944160280000e+001) (7, 6.02901629173088340000e+000) (8, -9.68540803731388710000e+001) (9, 9.91415697596545260000e+000) (10, -1.79941396392334780000e+002) (11, -5.90451545400116910000e+000) (12, 1.71330918364820020000e+001) (13, -2.63163128763843000000e+000) (14, 7.78689602966507690000e+001) (15, 2.84280267705507350000e-001) (16, 3.57357175639929000000e+001) (17, 6.07814601842267610000e+000) (18, 5.77048270155242880000e+001) (19, 6.75506939323310540000e+000) (0, -1.50667983952134610000e+001) (1, 6.78843883617878690000e+000) (2, -1.27732594737429790000e+002) (3, 2.11709182349612980000e+000) (4, -1.30189915009548450000e+001) (5, 5.13477722305816680000e-001) (6, -2.29133292194942730000e+001) (7, 3.58019562902928300000e-001) (8, 2.40971879262241300000e+001) (9, 1.53688486817081470000e+000) (10, 4.46592097154361770000e+001) (11, 2.30145946900130970000e+000) (12, 9.31320571687989410000e+000) (13, -8.88453286040227110000e-001) (14, 9.18593382577222780000e+000) (15, -3.36394930349453560000e+000) (16, 1.35670540888923500000e+001) (17, 1.52468929674069930000e+000) (18, 1.09375603120008320000e+001) (19, 4.13539522079318460000e+000) (20, 4.22775149917506700000e-001) (21, 8.55227447270946790000e-001) (22, -1.09338937387237340000e+000) (23, 8.24607672200000310000e-001) (24, -1.18165048555142940000e+000) (25, -6.37951440323635930000e-001) (26, 1.42481097207380030000e+000) (27, 4.40942009044762020000e-002) (20, 1.15438424349803140000e+000) (21, -1.44816407082014110000e+000) (22, 1.29355352161525920000e+000) (23, -3.83509259995232150000e+000) (24, 1.28422611746143130000e+000) (25, 2.05047527829855580000e+000) (26, -6.73256250185868650000e-001) (27, 2.53969043133043200000e+000) (20, -1.47956133630939910000e+000) (21, 4.77564135001487980000e-001) (22, 2.09381374913905130000e-001) (23, 2.05439721855033100000e+000) (24, 1.22040539050614390000e-001) (25, -9.22114465154273750000e-001) (26, -7.50138822927964850000e-001) (27, 6.32132641373813640000e-001) 

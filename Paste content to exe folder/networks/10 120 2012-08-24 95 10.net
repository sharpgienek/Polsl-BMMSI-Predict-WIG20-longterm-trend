FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 8 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (8, 5, 5.00000000000000000000e-001) (8, 5, 5.00000000000000000000e-001) (8, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 1.05943482598170840000e+000) (1, 9.57255674969808010000e-002) (2, 3.16123416411796660000e+000) (3, -7.39234342094520210000e-002) (4, 7.62874427450977670000e-001) (5, -6.71522072689555580000e-002) (6, 3.09398330983072960000e-001) (7, -2.11760351785849040000e-001) (8, 1.70879041809599760000e-001) (9, -1.80653185522751890000e-001) (10, 4.11483248761584650000e-001) (11, 2.83488575086146070000e-002) (12, 3.26738419132085400000e-001) (13, -1.15780699359542300000e-001) (14, 4.16353825832451010000e-001) (15, -4.08283896159566810000e-002) (16, 2.58531631006350070000e-001) (17, -7.88910503723437020000e-002) (18, -4.22379373158346950000e-002) (19, 3.54454639046465280000e-002) (0, -1.34004502929771930000e+000) (1, 3.63430269529914470000e-001) (2, 3.30070372905757910000e-001) (3, 3.53551192142712740000e-001) (4, -4.81717565580139010000e-001) (5, 4.26272038870868900000e-001) (6, -3.36963748125213460000e-001) (7, 3.59108980403022250000e-001) (8, -1.99693701215416940000e-001) (9, 4.18703679383091550000e-001) (10, 3.54606506083874780000e-001) (11, 2.46983779271773670000e-001) (12, -3.61801792308879770000e-001) (13, 3.50230851122774910000e-001) (14, -3.67196831887361960000e-003) (15, 4.97068585453168180000e-001) (16, -5.17844275102843920000e-001) (17, 3.35085514074583810000e-001) (18, 1.97452222139961530000e-001) (19, 4.82430515443781290000e-001) (0, 4.10999538190716440000e-001) (1, -1.84791369178165630000e-001) (2, 1.67970645052728430000e+000) (3, 7.32890437251505490000e-002) (4, 6.82057775856090730000e-001) (5, -1.35279649690383530000e-001) (6, 5.08857124084449940000e-001) (7, -8.73853618673280510000e-002) (8, 2.28178847350933360000e-001) (9, -5.24602974369736690000e-003) (10, 2.52513018807579210000e+000) (11, -6.97468470781252710000e-002) (12, 2.17375624533702790000e-001) (13, 4.33861378510967880000e-002) (14, 3.78261784249210360000e-001) (15, -6.33503626078102260000e-002) (16, 2.87970971904323030000e-001) (17, -6.03058450172775650000e-002) (18, 2.40646309548787520000e-001) (19, -1.16731065429613100000e-001) (0, 4.64940488749745970000e-001) (1, -1.28062227945721640000e-001) (2, -3.93893699710774480000e-001) (3, -7.01556313211271630000e-002) (4, 4.77596774253741860000e-001) (5, -2.03019504453297060000e-001) (6, 3.63717378025556650000e-001) (7, -9.65192302552334430000e-002) (8, 3.58305341967126770000e-001) (9, -3.39765331843605580000e-002) (10, 3.28921234928928720000e-001) (11, -9.56403524625980550000e-002) (12, 2.16504814679570110000e-001) (13, -1.98179416490549130000e-001) (14, 3.69711030458619030000e-001) (15, -6.58612708140192540000e-002) (16, -9.72631681823779900000e-001) (17, -1.08329561636057000000e-001) (18, -1.74594930678271030000e-001) (19, 3.68608986854103290000e-002) (0, -2.44481578905774160000e-001) (1, -6.52824500007988810000e-002) (2, 3.10379397771516620000e+000) (3, -1.51731709329878410000e+000) (4, -1.86974253830098250000e-001) (5, 1.62076873768016160000e-003) (6, -2.06596133682737460000e+000) (7, -8.03878893432171000000e-001) (8, -1.07156967157932080000e+000) (9, 2.70611814400016690000e-001) (10, 3.25108214628690990000e-001) (11, -7.53036483922113970000e-001) (12, -8.22597967677855200000e-001) (13, -1.50371190714829270000e-001) (14, 4.09385559634521280000e-001) (15, -3.62876105876331730000e-001) (16, -1.75902476321365910000e-001) (17, -1.92556848586201010000e-002) (18, -3.07252566670235710000e+000) (19, -6.88957313046409530000e-002) (0, 1.18815170933490010000e+000) (1, -8.25934551391294130000e-002) (2, 2.67327331067076960000e-001) (3, 8.85812308107751250000e-002) (4, 1.15793930654095020000e+000) (5, 9.25469442081902210000e-002) (6, 3.76856229656583220000e-001) (7, -4.04560286640270110000e-001) (8, -1.76515307258911690000e+000) (9, -1.34883809091548290000e-002) (10, -1.21931748843623880000e+000) (11, 8.38254787090931150000e-001) (12, 1.48012817860319220000e-001) (13, 7.53976157796102160000e-001) (14, -3.91324208768238260000e-001) (15, -6.14745985408035600000e-001) (16, 8.15928362516872840000e-002) (17, -2.14398817878177110000e-001) (18, -1.30593613968652140000e+000) (19, 7.69794561075789920000e-001) (0, 1.52877160605405640000e+000) (1, 2.25078627520953150000e-001) (2, 1.21791855548158570000e+000) (3, 2.50737836061266040000e-001) (4, 3.11129519853006810000e+000) (5, -1.90364031785355330000e-001) (6, -1.17469104027309530000e-001) (7, -6.62635219859354360000e-002) (8, -1.31869859104857180000e+000) (9, -3.76691886654125020000e-002) (10, -1.10769802264937620000e+000) (11, -2.26586612124300910000e-002) (12, 2.44499648375382240000e-001) (13, -1.64447524695334690000e-001) (14, 3.05333044086715910000e-001) (15, -2.36646686766671050000e-001) (16, 1.04241410753111610000e-001) (17, -3.58936727651434640000e-001) (18, -1.03624862613112120000e+000) (19, 3.79420186394172150000e-002) (20, -1.11073106360927950000e-001) (21, 3.31663956314856620000e-001) (22, -1.57545275022167200000e-001) (23, 1.69806815936840690000e-001) (24, -1.45482646508374890000e-001) (25, -1.81514304029086580000e-001) (26, -3.12070948983717960000e-001) (27, 3.02226700016832330000e-001) (20, 1.19164108196613270000e-002) (21, 4.02157076747613860000e-001) (22, -1.36671251273620010000e-001) (23, 4.41513926092774680000e-001) (24, -3.65952277797170270000e-001) (25, 3.26766422153744680000e-001) (26, 2.46557448335557120000e-001) (27, 3.73525484524094900000e-001) (20, 8.92240580669114300000e-002) (21, 5.78474261191045950000e-001) (22, 2.03275726652738170000e-002) (23, 4.43601845656663550000e-001) (24, 5.81374723531173700000e-001) (25, 9.60843319562129930000e-002) (26, 1.12622660373429440000e-001) (27, 4.48133683076945070000e-001) 

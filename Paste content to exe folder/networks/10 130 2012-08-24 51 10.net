FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 4 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (4, 5, 5.00000000000000000000e-001) (4, 5, 5.00000000000000000000e-001) (4, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 5.90276126520145490000e-001) (1, 1.30713985179754080000e-001) (2, -3.10017459289560810000e+000) (3, 1.10298325103454120000e+000) (4, -2.38764385732218640000e-001) (5, 8.07364072408185420000e-002) (6, 1.44965173182820360000e+000) (7, 5.85660630403398440000e-001) (8, 3.19024652141267850000e-001) (9, 4.31570609777176470000e-002) (10, -2.73785500080417560000e-002) (11, 7.29110011078799690000e-001) (12, 3.02062981966644270000e-001) (13, 5.51951852263112690000e-002) (14, 2.82458456577277920000e-001) (15, 2.36156202990898760000e-001) (16, -1.05744003269689780000e+000) (17, -1.17348357892665960000e-003) (18, 5.16826418394409570000e-001) (19, 1.45447686736276340000e-001) (0, -2.96392324495366790000e-001) (1, -8.90492586296452450000e-002) (2, 3.18545148516033110000e-001) (3, 3.72113919524883320000e-001) (4, -2.16027333063460170000e-001) (5, 7.84188771437192570000e-002) (6, 3.00126524803309810000e-001) (7, 6.05337761979074520000e-001) (8, 5.49052870557665050000e-001) (9, 2.24547444540019930000e-001) (10, 3.43238050918028060000e-001) (11, 1.59690441630519210000e-001) (12, 3.42126459126136600000e-001) (13, -3.94154118866214540000e-002) (14, 3.86457045702666260000e-001) (15, 6.04953843581514720000e-001) (16, -1.25766026869334550000e-001) (17, -4.65348123798631220000e-002) (18, 4.54838441171196980000e-001) (19, -4.81434963013518820000e-002) (0, 3.20165164518898400000e+000) (1, -2.45685079420870230000e-003) (2, -3.08807132123088040000e+000) (3, 1.08327193457764980000e-001) (4, 3.15870120509584760000e+000) (5, -2.02818057454564740000e+000) (6, 3.19979617641133540000e+000) (7, -1.71566808801819380000e-001) (8, 9.53539337634133480000e-002) (9, -5.81611569221208570000e-002) (10, -1.38660556404129160000e-001) (11, 5.35478762422351150000e-001) (12, 2.66898858735384440000e+000) (13, -2.27511633854212250000e-001) (14, 1.63857071716303220000e+000) (15, -3.05644290279083020000e+000) (16, -7.88415037050048120000e-002) (17, -6.82798749391725130000e-001) (18, -3.50046131981271990000e-001) (19, -3.84956292853316780000e-002) (20, 8.83565185056396500000e-002) (21, 1.31180840898228460000e-001) (22, -1.39515937709747650000e-002) (23, 6.17019555068142770000e-001) (20, 1.69973190631705320000e-001) (21, -7.60749755484325520000e-002) (22, 4.18805702397277990000e-001) (23, 9.89539813339352920000e-001) (20, -3.25255162974888710000e-001) (21, -2.16069370332268670000e-001) (22, -1.33348167853895750000e-001) (23, 6.82246750746408500000e-001) 

FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 4 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (4, 5, 5.00000000000000000000e-001) (4, 5, 5.00000000000000000000e-001) (4, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 4.63777577540185530000e+001) (1, -2.09878062369173700000e+000) (2, 1.99182103804447230000e+001) (3, -1.79536417354389540000e+000) (4, 3.50427520761529500000e+000) (5, -3.26989742231575340000e-001) (6, -3.27558293106472080000e+001) (7, -1.49123367007525890000e+000) (8, -8.27861907804440560000e+000) (9, -5.52046946255079880000e-001) (10, -1.15963678650075920000e+001) (11, -1.39673546885662710000e+000) (12, -2.89055446221849270000e+000) (13, -7.23725794815558770000e-001) (14, 2.68776544499515460000e+001) (15, -2.45236468886707200000e-002) (16, -2.70736239031966020000e+001) (17, 5.00711973182939030000e-001) (18, -1.25622502513428680000e+000) (19, 1.18710737658911400000e+000) (0, 2.56293454446519180000e+001) (1, 3.37252151985693360000e-001) (2, -2.75897609436490720000e+001) (3, -5.06097326235263380000e-001) (4, 1.81622836373952370000e+001) (5, -7.33941248886312800000e+000) (6, 7.72909902377798520000e+001) (7, -3.93408452264616400000e-001) (8, 2.52467822047570190000e+000) (9, 1.24887393628135970000e-001) (10, 3.42028200276877850000e+001) (11, 4.08630415792462910000e+000) (12, 4.00322265269617890000e+000) (13, -2.23511880590181120000e-001) (14, -2.08571157232842670000e+001) (15, -4.20639710435598690000e+000) (16, 1.00183424797802360000e+001) (17, -1.69519201588210520000e+000) (18, -1.74184195708428260000e+001) (19, -4.74558911689891890000e-001) (0, -1.81173875502878340000e+001) (1, 5.56412688159617200000e-001) (2, 7.18111052397753370000e+000) (3, 2.78307629754870720000e-001) (4, 2.04560269215747860000e+001) (5, -2.74168757406947880000e+000) (6, 1.86601770214894170000e+001) (7, -3.82983511394910380000e-003) (8, -2.25843802517873440000e+001) (9, -1.11131171562144070000e-001) (10, 1.43399107013814180000e+000) (11, 1.17121077490515260000e+000) (12, -2.36595776593950410000e+000) (13, -1.01570894440450000000e-001) (14, -1.55662380024620220000e+001) (15, -1.91051284276874920000e+000) (16, 2.84389863064336840000e+001) (17, -1.50138247141336940000e+000) (18, -8.61873918898217180000e+000) (19, -4.01913718214648270000e-001) (20, -1.20769945255461390000e+000) (21, 1.77873110322468770000e-001) (22, -1.21945661924161630000e+000) (23, 1.48894397081063470000e-001) (20, 3.64433242266063940000e-001) (21, 9.05843324299966390000e-001) (22, -1.53265727405484830000e-001) (23, 1.23013851404175020000e+000) (20, 7.49194618741260370000e-001) (21, -9.07675124156936300000e-001) (22, 1.11362630448442390000e+000) (23, 8.74243616440930180000e-001) 

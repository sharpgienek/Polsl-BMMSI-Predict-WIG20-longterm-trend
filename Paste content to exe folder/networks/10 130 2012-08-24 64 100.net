FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 5 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -3.38495757789601650000e+001) (1, -6.55208971325517250000e-001) (2, 3.40112916693227870000e+001) (3, 8.94916827648552520000e-001) (4, -5.46066648843187340000e+000) (5, -1.48927278090687800000e+000) (6, 5.58974416114525570000e+001) (7, 8.84057309848278240000e-001) (8, 6.27372576179059930000e+000) (9, -2.96251818084755400000e+000) (10, -1.57603197321917460000e+001) (11, 1.58271758336248620000e+000) (12, 4.43627135259788690000e+001) (13, 1.89019878241218780000e+000) (14, -3.16598910229781440000e+000) (15, 1.29415656640100310000e+000) (16, 1.37061049465349480000e+001) (17, -8.15997103738497540000e-001) (18, -6.10572830746124140000e+000) (19, 6.22428197944102760000e-001) (0, -1.85720399609569210000e+001) (1, 1.16457128376494510000e+000) (2, -4.01429738046323550000e+001) (3, 1.04081056362378170000e+000) (4, -5.83034768329060430000e-001) (5, -9.08501315742289150000e-002) (6, -1.38919744107417870000e+001) (7, -2.75812051064095070000e-001) (8, 3.46605846132807650000e+000) (9, 2.22367595577699010000e+000) (10, -2.02373074763335790000e+000) (11, 5.08989218728241100000e-001) (12, -1.54655784951008770000e+001) (13, -3.61978573259056140000e-001) (14, -6.00035440958620380000e+000) (15, -9.64473884535426370000e-001) (16, -1.09565894602005400000e+001) (17, 1.07006444639625030000e+000) (18, -4.07275037721844320000e+000) (19, -3.16380454588148900000e-001) (0, 2.94268514148876070000e+000) (1, 1.69731770963891380000e+000) (2, -2.92905518935216880000e+001) (3, 6.90677781675036770000e+000) (4, 2.12340882570865050000e+001) (5, -5.09802643170308090000e-002) (6, 2.71101088352946360000e+001) (7, 1.60299081301770780000e-001) (8, 1.40725864783806590000e+001) (9, 1.36373267315309920000e+000) (10, -3.29601980232347680000e+000) (11, 4.70627427735009270000e+000) (12, 2.47158966099920220000e+001) (13, -1.40304936671944300000e-001) (14, 1.05743501326382230000e+001) (15, -3.07533368337737970000e+000) (16, 6.52757036548263560000e-001) (17, -2.09747860120382350000e-001) (18, -1.71831179913936260000e+001) (19, 7.30370582088125000000e-001) (0, -1.62495762055579330000e+001) (1, 1.24244622888029800000e-001) (2, 5.03323018582881420000e+000) (3, 2.06329202674051080000e+000) (4, 7.40354685979710330000e+000) (5, -9.18977708767130250000e-001) (6, 2.18437824714387800000e+000) (7, -2.37317466351462640000e-001) (8, -1.23154464231226330000e+000) (9, -5.60218664216452720000e-001) (10, -1.89517632172142680000e+001) (11, 9.51330483109877110000e-001) (12, -4.67165450654507720000e+000) (13, -2.34520668459133580000e-001) (14, -3.90154633188885840000e+000) (15, -1.34700333353691030000e+000) (16, -1.36655925574700100000e+000) (17, -9.07351656213225140000e-002) (18, -7.91641726403722150000e+000) (19, 1.75326632529465300000e+000) (20, 1.39988061617855750000e+000) (21, 1.18995802941848770000e+000) (22, 1.15506058856757620000e+000) (23, -2.32910609679128690000e+000) (24, 8.41032946295210640000e-001) (20, -1.03160595700281950000e+000) (21, -1.23330408828924250000e+000) (22, 4.30725590768796570000e-001) (23, 8.57068610153071340000e-001) (24, 8.69025500802260260000e-001) (20, -1.83793486125698350000e-001) (21, 2.67018789743857790000e-001) (22, -1.33760580347965270000e+000) (23, 1.11482173078086280000e+000) (24, 5.66356977773894710000e-001) 

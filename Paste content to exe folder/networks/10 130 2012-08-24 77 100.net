FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 6 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (6, 5, 5.00000000000000000000e-001) (6, 5, 5.00000000000000000000e-001) (6, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 1.74090334759930950000e+001) (1, 1.82135532352243080000e-001) (2, 3.01956927280963190000e+000) (3, -1.17743159583521550000e+000) (4, -5.19173180969634270000e+000) (5, 2.09471198686695590000e+000) (6, 9.66809975753903570000e+000) (7, 2.18236465291142180000e-001) (8, -9.12389167648580470000e-001) (9, 9.20885923571822860000e-001) (10, 2.75149710312563580000e+001) (11, -8.06935382078513840000e-002) (12, 2.34111641198630520000e+001) (13, 7.16093245446309810000e-001) (14, -1.24855325869033040000e+001) (15, 2.87889268652235300000e-001) (16, 1.18182308912210240000e+001) (17, -8.53126934915486440000e-002) (18, -6.37239001017326160000e+000) (19, 2.91914877091996430000e-001) (0, 4.19539067579133460000e+000) (1, -1.24704141888077260000e+000) (2, 3.17531380634605260000e+001) (3, -4.82170610719247070000e+000) (4, -1.07354062149835180000e+001) (5, -4.61936721709692520000e-001) (6, -1.15320561443967660000e+001) (7, -8.82861877829759670000e-001) (8, -7.85922723777602310000e-002) (9, -1.36239592086117960000e+000) (10, -9.15191817990497820000e+000) (11, -2.87620636153086770000e+000) (12, -2.48288002862395420000e+001) (13, -8.59121268172522550000e-001) (14, 5.13959501475061640000e-001) (15, -4.52033243105305570000e-001) (16, 1.32757967450986300000e+001) (17, 1.54912240414065760000e-003) (18, 2.74173470011272520000e+000) (19, -5.89983407991106280000e-002) (0, -2.86656844807612890000e-001) (1, 3.67071876447592370000e-001) (2, -1.77016920700665170000e+001) (3, 1.90448682526739990000e+000) (4, 8.27470097232792410000e-001) (5, 1.17401867933760960000e-001) (6, 1.08491422456446200000e+001) (7, 4.62853309940646380000e-001) (8, 3.25052898587711920000e+001) (9, 1.10076323846024630000e-001) (10, -1.74896632489631760000e+001) (11, 2.19820283991197220000e+000) (12, -3.84908795469327600000e+001) (13, 4.62841386724355960000e-002) (14, 6.44446370948819740000e-001) (15, 6.23686822450305050000e-001) (16, -1.74650778668390030000e+001) (17, -2.24083958131324840000e-001) (18, -8.30409676453545490000e+000) (19, 7.19860835866780510000e-001) (0, -1.99069839609654090000e+001) (1, 1.66492216060794120000e-001) (2, 8.09918809064297030000e+000) (3, 4.99148688155832740000e-001) (4, -1.70514265995240850000e+001) (5, -4.90715498296238780000e-001) (6, -1.36516881472848280000e+001) (7, 3.50415229810249810000e-001) (8, 2.04499672265609260000e+001) (9, -2.13792477878895900000e+000) (10, -3.09956368562905700000e+001) (11, -2.25520453352013160000e+000) (12, -2.52136010021291490000e+001) (13, -1.02690257461122170000e+000) (14, 4.32080402571347830000e-001) (15, 3.43114565156381890000e-002) (16, 1.38517228391845780000e+001) (17, 5.95124853894698820000e-001) (18, 2.49590945779857020000e+001) (19, -1.74058809350325760000e+000) (0, -7.83654667217006120000e+000) (1, -7.19355144305082960000e-001) (2, 1.12334995789468790000e-001) (3, -6.45837884961376480000e+000) (4, -1.65173583999181250000e+000) (5, 3.09236324680340640000e+000) (6, -2.72742644953355220000e+001) (7, -2.76702878912513000000e-001) (8, 1.67907876064112500000e+000) (9, 1.88138208591008030000e+000) (10, 2.82457135989783840000e+001) (11, -1.06486650072489050000e+000) (12, -4.85945533315223390000e-001) (13, 4.80676900904915330000e-001) (14, 4.69580740319072990000e-001) (15, 2.62817016569577700000e+000) (16, 9.68226192170269230000e-002) (17, -3.77561729050309160000e-002) (18, -4.29709704696492240000e+001) (19, -2.70154771000321650000e-001) (20, -2.28911707916441640000e-001) (21, -1.44568444270306080000e+000) (22, -3.06742276753186990000e-001) (23, 1.41386849355376270000e+000) (24, 7.57497826801882930000e-001) (25, 8.68472366797690240000e-001) (20, 1.64027271269311270000e+000) (21, 8.58429847669750390000e-001) (22, 1.10322595945566590000e+000) (23, -8.92371466083335730000e-001) (24, -1.72752560585364460000e+000) (25, 2.93623485764964550000e-001) (20, -6.72729943015074830000e-001) (21, 6.71250885971844680000e-001) (22, -4.31225639832896580000e-001) (23, -3.62120346769008730000e-001) (24, 3.65278643196312700000e-001) (25, 1.08476817112918430000e+000) 

FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 8 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (8, 5, 5.00000000000000000000e-001) (8, 5, 5.00000000000000000000e-001) (8, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -8.09410436969371540000e+000) (1, 4.42904631928179260000e+000) (2, -6.84416705687237230000e+001) (3, 2.00309020647667670000e+001) (4, -2.90674873808880060000e+000) (5, 8.46688026681385250000e+000) (6, -1.45201270923457770000e+002) (7, 1.18333714283594970000e+001) (8, 7.74098522955216030000e+001) (9, 4.35841349780873340000e+000) (10, -1.64249740301229590000e+000) (11, 7.72778693158173710000e+000) (12, 3.42998813474977240000e+001) (13, 1.46550525468123330000e+000) (14, 1.29772309712427120000e+001) (15, 1.82426501207294760000e+000) (16, -1.09563714926713320000e+002) (17, -2.03355983040733570000e+000) (18, 2.01776947338396870000e+001) (19, 2.81391541772267170000e+000) (0, 4.04048817641118210000e+001) (1, -4.49283211336383090000e-002) (2, -1.50018889195879500000e+001) (3, -4.68291073010010720000e+000) (4, -3.48895541555699250000e+001) (5, -3.49705544040159920000e+000) (6, -1.30193084107519010000e+002) (7, -1.04182338496117310000e+000) (8, -9.13810834333987240000e-001) (9, -4.37658346629861920000e-001) (10, -6.31589098352895280000e+001) (11, -1.68039283351940430000e+000) (12, -2.89166908227369990000e+001) (13, -1.98835510848338410000e+000) (14, 2.62291830785562650000e+001) (15, 2.42500801739724150000e+000) (16, 4.34106864831523750000e+001) (17, -5.51635357306398900000e-001) (18, 9.63187516618712180000e+000) (19, 4.31196356808665250000e-001) (0, 1.67632016555868690000e+001) (1, 6.85296742169764970000e+000) (2, -1.04932522464374400000e+002) (3, -5.07632406138654440000e+000) (4, -1.27988541033149960000e+001) (5, 2.96058211121857260000e+000) (6, 1.78820090144538100000e+000) (7, -5.44656878791798920000e-001) (8, -2.95755987639919700000e+001) (9, 9.26161412974783940000e-001) (10, -2.08286329446663170000e+000) (11, 3.42560516803642610000e+000) (12, -3.83365578821387270000e+001) (13, -1.06457620310761000000e+000) (14, -1.93636754306419480000e+001) (15, -4.83089679661173050000e-001) (16, 5.23245326411004270000e+000) (17, 1.34655663840796900000e+000) (18, 1.38362547434463480000e+001) (19, 1.95943981099545010000e+000) (0, -2.34274953634586930000e+001) (1, -2.36387137840681360000e-001) (2, 1.96650386489582050000e+001) (3, 4.36533034164829190000e-001) (4, 3.58822101467645350000e+001) (5, 1.78065660691156210000e+000) (6, -5.93487117457589090000e+000) (7, 1.17976735141185030000e+000) (8, -7.75364299790978320000e+001) (9, 1.87975830004024010000e-001) (10, 6.90488755689246410000e+000) (11, 1.60749200859040650000e+000) (12, -8.72292443334452370000e+000) (13, 1.15642000550486590000e+000) (14, -2.51445051780239550000e+001) (15, -1.67913501414572370000e+000) (16, -3.37585853044634020000e+001) (17, -1.81992252459870860000e-001) (18, -1.20494131765756870000e+001) (19, -1.03347809928979520000e+000) (0, -1.19366081532838690000e+002) (1, -1.16788014781887360000e+000) (2, 4.61976141407810270000e+001) (3, 1.79255756100042710000e-001) (4, -3.48054481163102510000e+000) (5, -5.03677641749563090000e-001) (6, -8.50602951814354780000e+001) (7, -1.68756466194513070000e-002) (8, 3.98899906228040920000e+001) (9, -1.56045012514206660000e+000) (10, 1.26057022389684420000e+001) (11, -1.15295957923115930000e+001) (12, 1.07695861225646980000e+002) (13, -3.20270101612437910000e-001) (14, 2.04014531678157450000e+001) (15, -5.19918547405952110000e-001) (16, -2.72442121473687710000e+001) (17, 4.82903475759806520000e+000) (18, -9.46885285742284390000e+000) (19, 5.79284756884807890000e+000) (0, 7.52287125535359100000e+000) (1, 1.54147974585161780000e+000) (2, 1.80974007126187890000e+001) (3, -3.47255956709959700000e+000) (4, -1.36720836361568690000e+000) (5, 9.01067508842706030000e-001) (6, 7.37101926889300070000e+001) (7, 3.91573008010418370000e+000) (8, -2.48258366139409300000e+002) (9, 1.08726341820556320000e+000) (10, -1.95971926277521650000e+001) (11, 1.57984772756458950000e-001) (12, -1.42287316604196410000e+001) (13, -4.63956523265752260000e-001) (14, 2.49074254467462910000e+001) (15, -5.99933339493202310000e+000) (16, -2.69881948622537140000e+001) (17, -8.19254532990576380000e-001) (18, -5.61959857881754770000e+001) (19, 3.13187942937157040000e+000) (0, -2.03359963851314550000e+001) (1, 1.84609203355207140000e-001) (2, -2.10637267800527010000e+001) (3, 1.04097750377816680000e+000) (4, -4.73467337014672860000e+001) (5, -6.99810149253697000000e-001) (6, 3.86580908579729070000e+000) (7, 1.82760790776666090000e+000) (8, -3.21347847366997360000e+001) (9, 2.19197320819679490000e+000) (10, -7.54821362867290020000e+001) (11, 2.29747116814315920000e+000) (12, -7.92916128959615630000e+001) (13, -3.39974071735800940000e+000) (14, 5.21430856770610870000e+001) (15, -3.44597149231688490000e-001) (16, 8.75569668294078870000e+000) (17, -5.29227216689980380000e+000) (18, -2.25762046880453800000e+000) (19, -2.44075959355887570000e+000) (20, 3.38066821656781200000e-001) (21, -2.41753480964431100000e+000) (22, 1.54895193271528760000e+000) (23, -1.45402368045978100000e+000) (24, 8.61230204908955300000e-001) (25, -8.93096859091522370000e-001) (26, 8.19365945577167690000e-001) (27, 2.54973089618132530000e-001) (20, 1.45370445247603610000e+000) (21, 1.43479835108358910000e+000) (22, -1.58901853288961090000e+000) (23, -1.48928708473472450000e-001) (24, -1.43641773847647690000e+000) (25, 1.85672088127829630000e+000) (26, -1.78029197090092480000e+000) (27, 3.01155681634760200000e-001) (20, -2.21623139563841810000e+000) (21, 8.36023874793232700000e-001) (22, -6.71526088592883550000e-004) (23, 1.36157961422782050000e+000) (24, 4.42620964502961510000e-001) (25, -9.39073075319404650000e-001) (26, 8.29044916277183910000e-001) (27, 2.42593224530984570000e+000) 

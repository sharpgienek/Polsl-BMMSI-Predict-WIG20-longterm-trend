FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 4 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (4, 5, 5.00000000000000000000e-001) (4, 5, 5.00000000000000000000e-001) (4, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -6.60425519769738290000e-001) (1, 6.01072173339918690000e-001) (2, -2.28881358461689110000e+000) (3, 8.49160119322682210000e-002) (4, -8.66741770589106180000e-002) (5, 7.67910484175614800000e-001) (6, -2.71290020982513370000e-001) (7, 4.21440744968182290000e-002) (8, 2.67790646541815720000e-001) (9, 5.68436674806736870000e-001) (10, -1.00210555921609990000e+000) (11, 2.32033645283031280000e-002) (12, 4.10260599318517990000e-001) (13, 4.48001317858296500000e-001) (14, -1.09808745405499920000e-001) (15, 6.16355376692656170000e-001) (16, 5.97963720937599200000e-001) (17, 5.08145361723262520000e-001) (18, 8.11238523822395280000e-002) (19, 5.27128080441190750000e-001) (0, -2.30133002848388860000e-001) (1, -5.00861514868942150000e-002) (2, -3.07018802018314840000e+000) (3, 8.66657431126066190000e-001) (4, 2.52715969689660590000e-001) (5, -8.68303342305837280000e-002) (6, 1.18586289094141910000e+000) (7, 6.51505523700808940000e-001) (8, 5.96022802939927270000e-001) (9, 3.36902065567416200000e-001) (10, 7.19113926984563110000e-002) (11, 5.17710355691480720000e-001) (12, 6.04451758143653480000e-001) (13, 2.40541678780575360000e-001) (14, 2.42511957308481270000e-001) (15, 2.77858573867890420000e-001) (16, -1.65097956771396760000e+000) (17, -2.96159779092733940000e-002) (18, 7.56089099739855500000e-001) (19, 2.43636161248561740000e-001) (0, -8.78580227391840380000e-001) (1, 2.28687620522734110000e-001) (2, 1.42589952514873670000e+000) (3, 1.32352897403731020000e-001) (4, 4.42452045531037400000e-001) (5, 2.26247102185890390000e-001) (6, -1.78700862247533320000e+000) (7, 1.93962342520405990000e-002) (8, -2.79833486357995880000e-001) (9, 2.37860223534171410000e-001) (10, 2.16923679768976640000e-001) (11, 1.67968127693653090000e-002) (12, -2.02550770362643740000e-001) (13, 2.50812864993050940000e-001) (14, 1.46999292912354750000e-001) (15, 3.04993557196587000000e-001) (16, 6.83405741599369180000e-001) (17, 2.71064330138571230000e-001) (18, -5.61934961328856790000e-001) (19, 1.43357433574638430000e-001) (20, 5.90342192711279520000e-001) (21, 6.32499175761342510000e-001) (22, -2.62700230402122650000e-001) (23, 3.26924736708408210000e-001) (20, 2.63045341225281060000e-001) (21, 3.27791435940073040000e-001) (22, -3.08555943976050960000e-001) (23, 5.73527006913838730000e-001) (20, 4.17593602561436240000e-001) (21, -3.35239002854378580000e-001) (22, -3.68018703465180920000e-002) (23, 3.73190525727903840000e-001) 

FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 5 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 9.79902658952525840000e+000) (1, -9.25566436841248480000e-002) (2, -4.26644785291827380000e+001) (3, 4.67939521994472680000e+000) (4, 3.25137007025299100000e+001) (5, 1.19243835346686550000e-001) (6, 5.17831225434902150000e+001) (7, 4.30527656800429790000e+000) (8, 2.26172545673888090000e+001) (9, -9.41789589571377710000e-001) (10, 1.51336413006104230000e+001) (11, 2.53014083547425320000e+000) (12, 1.98218169555195200000e+001) (13, -7.80660702407233180000e-002) (14, -1.32499179165842910000e+001) (15, 3.24000666308334790000e+000) (16, 2.12368113136219930000e+000) (17, -2.91216667472812200000e-002) (18, 1.93954782214679650000e+001) (19, -3.73871500462905130000e-001) (0, -1.56787811777512670000e+001) (1, 3.08804863862634220000e+000) (2, -3.66985671377144910000e+001) (3, 2.90441699901135750000e+000) (4, 1.14559483107537070000e+001) (5, 3.06369353628268200000e+000) (6, 1.38341349426909210000e+001) (7, 4.46955906597672900000e-002) (8, 1.74122738685434760000e+001) (9, 6.86972771222348680000e-001) (10, 3.55922407792806620000e+000) (11, 1.07558768680729310000e+000) (12, -1.86449226031831290000e+000) (13, -6.93964784091262860000e-002) (14, 1.99777187855149310000e+001) (15, -1.03656461896553980000e+000) (16, 6.45177447178514370000e+000) (17, 2.27458448641366180000e-002) (18, -2.26079061173419630000e+000) (19, -6.63367613265057050000e-002) (0, -4.78250913638296550000e+000) (1, 1.53955834735427820000e+000) (2, 2.16807990021921740000e+001) (3, -1.83880593669158900000e+000) (4, 4.40545435406816120000e+000) (5, 1.77934135465086230000e+000) (6, 8.39836135881087390000e+000) (7, -1.22208145463181730000e+000) (8, -3.69599195844379050000e+001) (9, 3.45417687115614890000e+000) (10, -3.39430655069365980000e+001) (11, 4.12080030759075340000e-001) (12, -4.67390020686818360000e+000) (13, 9.11236420562849370000e-001) (14, -2.60078999633987580000e-002) (15, -9.48634270476813770000e+000) (16, -4.65566611856263800000e+000) (17, 5.92966390738415840000e-001) (18, -3.75027153679626220000e+001) (19, 1.11590527888971280000e+000) (0, 2.32383499309141730000e+000) (1, 6.07547895945108500000e-001) (2, 8.73111105206457870000e+000) (3, 6.23067267969829920000e-001) (4, 3.27734255213393690000e+001) (5, 1.52790788143147100000e+000) (6, -3.58466852198854870000e+001) (7, -3.97371504098463670000e-001) (8, -1.03000288578601500000e+001) (9, -5.15337357999960500000e-001) (10, 4.40466270624159240000e+000) (11, 1.59476898921235240000e+000) (12, -2.35422162477522970000e+001) (13, -1.58910284519310370000e+000) (14, 4.70763005149111320000e+001) (15, 1.34247303907867140000e+000) (16, -8.18638093920153300000e-001) (17, -2.24805597887912260000e+000) (18, -4.58238969406777630000e+001) (19, 2.74110662120856310000e+000) (20, -1.47032356842152820000e-001) (21, 1.11590058536807970000e+000) (22, -1.16357554992580800000e+000) (23, -1.00884777616315360000e+000) (24, 9.66934896424440750000e-001) (20, 8.02260172867616750000e-001) (21, -4.27793749646660000000e-001) (22, 9.35052501423171760000e-001) (23, 5.65652287200720140000e-001) (24, 4.80081886962300360000e-001) (20, -4.70348631330113340000e-001) (21, -4.48057107109630430000e-001) (22, 2.45822232018579940000e-001) (23, 3.66216348060743270000e-001) (24, 7.76872108312555380000e-001) 

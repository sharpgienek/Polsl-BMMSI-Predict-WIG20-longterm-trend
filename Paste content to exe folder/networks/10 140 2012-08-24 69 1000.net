FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 5 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -1.63634213756032560000e+001) (1, 2.56453929565198810000e+000) (2, 1.38149821799597930000e+001) (3, 1.82389320853451050000e+000) (4, -4.78212833833565090000e+001) (5, 5.38387325953770370000e+000) (6, -9.42873685523929200000e+001) (7, 4.92983711791770200000e-001) (8, 4.45975047102042340000e+001) (9, -5.17042066931313050000e-001) (10, -3.67487951923181480000e+001) (11, -2.56190956410750510000e+000) (12, -5.53661363832083500000e+001) (13, -1.95402279096859390000e+000) (14, -2.92937830284378510000e+001) (15, -4.57996752525625350000e+000) (16, -2.85828847376351440000e+001) (17, 1.13793032709166010000e+000) (18, 4.53143609938728530000e+001) (19, 3.15383889628829130000e-001) (0, -7.93885149153681890000e+000) (1, -1.54022676673506440000e+000) (2, -1.96839771716297850000e+001) (3, -1.68264292730158570000e+000) (4, 3.93846652134614320000e+001) (5, -2.41287879197426920000e-001) (6, 5.45347073064514090000e+001) (7, -2.13717778906176660000e-001) (8, -2.07402175005130830000e+001) (9, 3.04559184458228230000e+000) (10, 2.59082351250452060000e+001) (11, 1.66472677730305030000e+000) (12, 6.39331768858156480000e+001) (13, 5.37630554938021720000e+000) (14, 4.27423692950752710000e+001) (15, 3.25126599037852900000e+000) (16, 5.26975405905709100000e+001) (17, 5.90781267073890340000e-001) (18, 1.70171964751573870000e+001) (19, -4.40796530329756210000e+000) (0, 2.82045749413985050000e+001) (1, 1.36878363427270090000e+000) (2, -6.15145115722139690000e+001) (3, 4.50691543823416920000e+000) (4, -7.01666947131856580000e+000) (5, -6.43452044664741480000e-001) (6, 9.28447592795314590000e+001) (7, 2.33222795418965220000e+000) (8, 6.80916272520188810000e+001) (9, -1.48586222693838830000e+000) (10, 4.86598547563351790000e+001) (11, 4.37279491389975840000e+000) (12, 3.38108760933650320000e+001) (13, 3.91754754507108450000e-001) (14, 3.76407943162662930000e+000) (15, 2.45030682177801930000e+000) (16, -1.81108765714352420000e+001) (17, -4.92402164543383370000e-001) (18, -1.66319863838198150000e+001) (19, 1.79859192455483070000e+000) (0, -6.09795466314361080000e+001) (1, 2.80570619619865140000e+000) (2, 1.21382390515947020000e+002) (3, -7.72493232923889610000e-001) (4, -3.11707715042279470000e+001) (5, 1.68865361465111330000e+001) (6, -2.87972895822487660000e+002) (7, -1.95665872921394370000e+000) (8, -6.82609288217438600000e+001) (9, 9.03171917517101440000e+000) (10, -1.38504508046512290000e+002) (11, 1.58483878076599380000e+000) (12, 3.22094866610203150000e-001) (13, 5.23790564132289220000e+000) (14, 3.66503853526821000000e+001) (15, -8.54313784988886130000e+000) (16, 2.59576977603031620000e+001) (17, 2.14440851758817570000e+000) (18, 6.69523548886785700000e+001) (19, -3.61854436412063270000e+000) (20, 1.89772549670118270000e+000) (21, 1.92567592810859760000e+000) (22, -1.35649276806356610000e-001) (23, -1.95956726914465820000e+000) (24, 1.80525368825899250000e+000) (20, -6.13169277772967920000e-001) (21, -6.08567613961057740000e-001) (22, 9.96846679392977890000e-001) (23, 1.21824771517018050000e+000) (24, 2.43713707952736780000e-001) (20, -5.07666088865247780000e-001) (21, -6.34239632722608640000e-001) (22, -6.84533471956392910000e-001) (23, 2.43969737666706520000e-001) (24, 6.91372988468787250000e-001) 

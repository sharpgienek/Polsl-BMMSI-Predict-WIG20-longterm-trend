FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 6 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (6, 5, 5.00000000000000000000e-001) (6, 5, 5.00000000000000000000e-001) (6, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 1.47627666932335060000e-001) (1, -2.55958836976363790000e-004) (2, -8.97867337054240600000e-001) (3, 2.75061629274199400000e-002) (4, -1.52259625592541670000e+000) (5, -6.61324058897280850000e-001) (6, 1.77039071140007250000e+000) (7, 5.98387698906642340000e-001) (8, 1.17712875995189540000e+000) (9, 2.01908188114543750000e-002) (10, 2.21183761750640580000e-001) (11, -3.43322256835302260000e-001) (12, 1.16183945308344190000e+000) (13, -8.06758819103265970000e-002) (14, 1.79341927619634990000e+000) (15, 3.41064447391561390000e-001) (16, -1.23043932025473210000e+000) (17, 1.49418237661991390000e-002) (18, 3.03065554889094370000e+000) (19, -1.04497903749677380000e-001) (0, -1.75011363026837290000e-001) (1, 1.21379461025766230000e-001) (2, 1.14744125950951710000e+000) (3, -5.74650122876962270000e-001) (4, 6.90172839379591170000e-001) (5, 9.11549475454225270000e-001) (6, -3.77993406675174390000e-001) (7, -1.53209458242014480000e-001) (8, 1.79635744021545640000e-001) (9, 1.33533363691765440000e-002) (10, 1.45082989621966970000e-001) (11, -9.21429288482679800000e-001) (12, 5.64799624033523180000e-002) (13, -1.44257378081765420000e-001) (14, -5.99416290961689580000e-002) (15, 2.08845277696730450000e-001) (16, 2.23371295845793410000e+000) (17, 8.60687893725860380000e-002) (18, -6.62999020731175450000e-001) (19, 9.77588552050417810000e-002) (0, 1.29587632985186520000e+000) (1, 2.48333733119141090000e-001) (2, -3.17327640062553500000e+000) (3, 4.07916799165440210000e-001) (4, 3.94065735187488110000e-001) (5, 8.04081048650118480000e-002) (6, 3.09067141892587480000e+000) (7, 8.64833262551321650000e-002) (8, 1.22587143645946780000e+000) (9, -2.34492900871230220000e-002) (10, -2.22193866244031810000e-001) (11, 5.18904722989396610000e-001) (12, 1.07650009634303650000e+000) (13, -3.99776391163755300000e-002) (14, 2.91612746786015680000e-001) (15, -8.54488312001990820000e-002) (16, -6.49066078054106180000e-002) (17, 1.16089286210913790000e-001) (18, 1.16277849559855100000e+000) (19, -8.73592316192912670000e-002) (0, -6.13753809757267790000e-001) (1, -9.40525334855345780000e-002) (2, 3.04496218972912920000e+000) (3, -1.65056256642601330000e+000) (4, -2.12251239142750130000e-001) (5, 3.46545153073484910000e-002) (6, -4.46783740997238050000e-001) (7, -6.09222822102119420000e-001) (8, -7.35659489819871060000e-001) (9, -1.18727265143500970000e-001) (10, 3.34836380286997640000e-001) (11, -4.70806396220459330000e-001) (12, -4.81651735714424540000e-001) (13, -2.14167940393802820000e-001) (14, 5.77174183873087280000e-001) (15, -2.09268599655512750000e-001) (16, 1.12634477647215330000e+000) (17, 1.80676173694308990000e-001) (18, -6.11263210721404370000e-002) (19, -1.36972456000091250000e-001) (0, -3.42892811562179210000e-001) (1, -3.13858387301407580000e-001) (2, 3.09881663505996610000e-001) (3, -1.50254906387075780000e-001) (4, -3.71243642062764500000e-001) (5, 1.61791641998025910000e+000) (6, -9.24619033039135510000e-001) (7, 7.81668271244038150000e-002) (8, -9.48653375633546070000e-002) (9, -7.06021327291498910000e-002) (10, 4.80583742073101560000e-001) (11, -3.83578847324698380000e-001) (12, -2.07232730371910990000e+000) (13, -1.74653582332501390000e-001) (14, -2.78372061579542640000e-002) (15, 2.04759846588911750000e-001) (16, 1.96172516380076680000e-001) (17, 1.98503742980063540000e-001) (18, 5.21943495962806650000e-003) (19, -1.39004717279793410000e-002) (20, 2.90177903403733390000e-001) (21, 1.66132929633929730000e-001) (22, 1.18393259885686760000e-001) (23, -3.53165960259224250000e-001) (24, 1.42659729444981790000e-001) (25, 7.01431240534236220000e-001) (20, -3.07620594239707220000e-001) (21, -5.75506006094658950000e-002) (22, 2.36672122484209930000e-001) (23, -2.90052742415078670000e-001) (24, -7.43038115028753500000e-002) (25, 5.60677030230657180000e-001) (20, -1.16630120603891750000e-001) (21, 1.78065951578407490000e-001) (22, -1.99662894578148090000e-001) (23, 2.86518512983604310000e-001) (24, -2.97192927110750540000e-002) (25, 7.66041721742369170000e-001) 

FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 7 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (7, 5, 5.00000000000000000000e-001) (7, 5, 5.00000000000000000000e-001) (7, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 1.12382065368879270000e+000) (1, 3.20441747811765320000e-001) (2, -1.03676057286625320000e-001) (3, 7.20539995578772800000e-002) (4, 9.71352620010439200000e-001) (5, -7.29191987333605020000e-002) (6, 9.54735466029064670000e-002) (7, 2.05920150978009110000e-002) (8, 4.66723734096026240000e-001) (9, 1.88603292560591940000e-001) (10, -3.32585116954392860000e-001) (11, -1.45370179060293700000e-001) (12, 8.39197713920009520000e-001) (13, -5.15839078425464610000e-004) (14, 1.18502111399183820000e-001) (15, -2.12251227583870770000e-001) (16, 5.15617851311208140000e-003) (17, 3.16496887446282570000e-002) (18, -1.94266561830776170000e-001) (19, -2.69137890886992910000e-001) (0, -4.45169713473033580000e-001) (1, 9.33090924624989240000e-002) (2, 3.13454959340783560000e+000) (3, -1.66644391431891400000e+000) (4, -2.42750507175349630000e-001) (5, 1.12093551097356530000e-001) (6, -5.95389709779632680000e-001) (7, -1.00835676159857810000e-001) (8, -4.74565553968207210000e-001) (9, 1.14103362042716480000e-002) (10, 6.13083026636452200000e-001) (11, -9.30302376432775020000e-002) (12, -5.05595827868094320000e-001) (13, 2.09318207211528150000e-002) (14, 1.33237227718725340000e-001) (15, -2.26689901301833830000e-001) (16, 1.17149858573874740000e+000) (17, 9.61411237546994780000e-002) (18, -2.80710045644850080000e-001) (19, -1.48984174929079810000e-001) (0, -3.18710086372216050000e+000) (1, -5.95315007678055560000e-002) (2, -5.64567996715598100000e-001) (3, 5.34510137241410280000e-001) (4, -1.03957191495988920000e+000) (5, 1.53800190460466890000e-002) (6, -2.35029929826531540000e-001) (7, 4.78220986797528950000e-001) (8, 3.21297977035778630000e+000) (9, 3.12911713205993260000e-001) (10, 1.95887119819097140000e+000) (11, -4.78806538658403500000e-001) (12, 4.25538333003959880000e-001) (13, 3.59835542030212780000e-001) (14, -3.08042581326667400000e-003) (15, 4.78524380201709120000e-001) (16, 8.87592490031772870000e-001) (17, 4.21277147954000340000e-001) (18, 1.34173848845393030000e+000) (19, -3.53289297999447750000e-001) (0, -4.87557730586909770000e-001) (1, 1.77253480350975990000e-001) (2, 1.05785385583081080000e+000) (3, -3.61384992605819700000e-002) (4, 1.47035340903969440000e+000) (5, -1.38536753108323250000e-001) (6, -3.19765384239503710000e+000) (7, -2.45392814877687940000e-001) (8, -1.99897723333668400000e-001) (9, 1.87011605468297090000e+000) (10, -3.95606930397746410000e-001) (11, -7.24066055396406670000e-001) (12, -2.19490008781866200000e-001) (13, 1.47118478224494420000e-001) (14, -2.30146956689156750000e-002) (15, 1.80633503888092910000e-001) (16, 1.85688222792453870000e-001) (17, 1.20275187635287300000e-001) (18, -3.15712649804113270000e+000) (19, 3.42509193795348780000e-001) (0, 7.15449768383018330000e-001) (1, 2.76071702503374820000e-001) (2, -1.36648545937921240000e+000) (3, 2.36264153448280170000e-001) (4, 2.67070449492963380000e-001) (5, 1.60064680097731670000e-001) (6, 7.40812355756407940000e-002) (7, 2.64548914169443540000e-001) (8, 4.75558446327671080000e-001) (9, 2.25266066154272800000e-001) (10, -1.55618346437544490000e-001) (11, 8.29811974503122380000e-002) (12, 1.38799952367179650000e-001) (13, 2.78476487695458910000e-001) (14, 4.02311374149850400000e-002) (15, 3.01861890326641010000e-001) (16, -1.49495078642628320000e-001) (17, 2.66690598847960480000e-001) (18, 7.32877785325673630000e-002) (19, 5.55567766709087580000e-001) (0, 6.34251957253561320000e-001) (1, 4.09489042318598310000e-002) (2, -3.36050159969896180000e-001) (3, -2.08313492145863530000e-001) (4, 5.70445803959025670000e-001) (5, 6.76737177404439230000e-002) (6, 6.61204732237099520000e-001) (7, -5.75506006094658950000e-002) (8, 7.06996973639914760000e-001) (9, 2.05602986682042190000e-001) (10, -4.36233877406194460000e-001) (11, 2.95857372063008660000e-002) (12, 1.35607873238698230000e+000) (13, 7.75577630256634450000e-002) (14, 2.65526320616047640000e-001) (15, -7.21560776971160500000e-001) (16, -7.05293203553621910000e-002) (17, -3.06292233612757310000e-001) (18, -6.33089220034076670000e-002) (19, 1.98983095234976310000e-001) (20, -8.89665132875574190000e-002) (21, 6.03740462490777730000e-002) (22, 2.40849878166887880000e-001) (23, -1.89893467323809020000e-001) (24, 4.91053577121040310000e-001) (25, -1.71891088172189510000e-001) (26, 4.43501086178078100000e-001) (20, 1.40426574304741040000e-001) (21, -2.64479220440374360000e-001) (22, -3.22500846222417430000e-001) (23, -1.41702475214100900000e-001) (24, 5.21821399845141530000e-001) (25, 2.69101141503054030000e-001) (26, 4.42878527575051250000e-001) (20, -3.80722819469180180000e-001) (21, 3.69120610374297000000e-001) (22, -2.00026267195561320000e-001) (23, 4.00883956519900890000e-001) (24, 3.29147903639732730000e-001) (25, -1.51148881429388810000e-001) (26, 3.87154635117810060000e-001) 

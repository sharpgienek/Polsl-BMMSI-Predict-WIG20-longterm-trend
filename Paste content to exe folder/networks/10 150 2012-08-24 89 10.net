FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 6 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (6, 5, 5.00000000000000000000e-001) (6, 5, 5.00000000000000000000e-001) (6, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 3.04945437729606810000e+000) (1, 9.27172603131758010000e-002) (2, 5.07286623330380550000e-001) (3, 1.37712178521005260000e-001) (4, 3.11963260099805510000e+000) (5, -6.35756835005556460000e-001) (6, 1.24632833078220820000e+000) (7, -2.59021500713971730000e-001) (8, -9.48504484198631650000e-001) (9, -6.42956814796243060000e-002) (10, 1.48288374263346150000e-001) (11, 9.29421975329315030000e-001) (12, -5.88158084957742800000e-001) (13, 1.69035947748943370000e-002) (14, 2.86374437031897520000e-001) (15, -5.42752543238707030000e-001) (16, -1.03925765937908430000e+000) (17, -3.16440591948800360000e-001) (18, -4.92796755411748280000e-001) (19, 1.20443635380261120000e-001) (0, -3.17900149836709560000e+000) (1, 3.09199825656484350000e-001) (2, 1.87920611218778840000e-001) (3, -9.90152586814056070000e-002) (4, -4.83335347319638010000e-001) (5, 3.10728596385130120000e-001) (6, 2.13453432062272270000e-001) (7, 1.74629511151230430000e-001) (8, 1.39372237094053890000e-001) (9, 3.39671031201335820000e-002) (10, 3.29078150251242710000e-001) (11, -1.08299789604058230000e-001) (12, -7.42341917557399200000e-002) (13, 2.67012325027841610000e-001) (14, 1.99282506203029310000e-001) (15, 5.20518201864918620000e-002) (16, 4.88935268586215550000e-001) (17, 2.77551262851391200000e-001) (18, -2.60750596966821120000e-002) (19, 1.33967104610249720000e-001) (0, -3.98212443424698840000e-002) (1, -1.12567083127770250000e-001) (2, -2.33291758918034200000e+000) (3, 3.69376796590875580000e-001) (4, -2.61573944637809590000e-001) (5, 1.44757395798101650000e-001) (6, 7.17274120709076040000e-002) (7, 2.74070398686324710000e-001) (8, 1.00453743495805650000e+000) (9, 2.47393222751691900000e-001) (10, -1.88955556340882900000e-002) (11, 8.12994771493742440000e-002) (12, 9.80977864294491520000e-001) (13, 2.68399583248426330000e-001) (14, 2.11878895230801720000e-001) (15, -1.80884287565780120000e-003) (16, -3.49246988922028770000e-001) (17, -3.58677888544843700000e-002) (18, 2.93647695277376770000e-001) (19, 6.87489776260939320000e-001) (0, 3.81672803029258210000e-002) (1, -1.92639614127572860000e-001) (2, 3.10125491517733120000e+000) (3, -2.03490664886462370000e+000) (4, -1.02028867515019380000e-002) (5, 8.04402301515918520000e-002) (6, -2.07280948146066990000e+000) (7, -3.95643621241927150000e-001) (8, -7.91201482835011240000e-001) (9, -1.54327845978558030000e-001) (10, 1.18749046269318720000e-001) (11, -6.24344870905218620000e-001) (12, -7.77700506071331010000e-001) (13, -4.46445698332439410000e-001) (14, 3.59328682341262390000e-003) (15, -6.04163298400309250000e-003) (16, 1.97919230160863170000e+000) (17, 2.35604182383518380000e-002) (18, -5.35116015482552280000e-001) (19, -2.28699184977402330000e-001) (0, 5.29491213505004720000e-001) (1, 2.49062023086160660000e-001) (2, 3.07149417254636160000e+000) (3, -3.31635300210110340000e-001) (4, 5.28785039499849670000e-001) (5, 2.90924783158970360000e-001) (6, -3.21102054181296650000e+000) (7, -6.09180206226236140000e-001) (8, -1.16227158676450280000e+000) (9, 5.86913089360657340000e-001) (10, -1.19282578643854590000e+000) (11, -3.01936344454954930000e-001) (12, -1.22372178299268210000e+000) (13, 3.08551097079919620000e-001) (14, -1.60161431419340500000e-001) (15, -3.21497208145942510000e-001) (16, 2.70230050223994600000e-001) (17, 1.18300163100240220000e-001) (18, -1.19391221223598490000e+000) (19, 2.20427916305721230000e-001) (20, -4.00451380774016710000e-001) (21, 1.50894669500093280000e-001) (22, 6.97206573280730880000e-001) (23, -3.33167200612152770000e-003) (24, -6.64429426079401140000e-001) (25, 5.57014920801087500000e-001) (20, 3.74127269932080730000e-001) (21, -1.69619473505318930000e-002) (22, 4.60647802421743800000e-001) (23, -1.24744048362428280000e-001) (24, 1.79108632850660340000e-001) (25, 4.35971288382160090000e-001) (20, 6.60086024637161760000e-003) (21, -2.67762158346852130000e-001) (22, 2.88785325739697560000e-001) (23, 3.91917924061929210000e-001) (24, 2.99478339221334540000e-001) (25, 5.17232762056125120000e-001) 

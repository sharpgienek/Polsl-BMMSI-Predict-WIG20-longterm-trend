FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 4 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (4, 5, 5.00000000000000000000e-001) (4, 5, 5.00000000000000000000e-001) (4, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 9.11556476541282250000e-001) (1, 1.39556219298477950000e-001) (2, -9.09618985572317570000e-001) (3, 5.57023117546971760000e-001) (4, -1.31527917160574880000e-001) (5, 5.18609350433753220000e-001) (6, 3.12760802335325220000e-001) (7, 4.98672705332108890000e-001) (8, 3.13756006591683840000e-001) (9, 1.73168280346211680000e-001) (10, 4.67766513838238080000e-002) (11, 3.79467495765441750000e-001) (12, 2.62043690900190420000e-001) (13, 2.74236397086618950000e-001) (14, 6.94441993859612470000e-001) (15, 2.75023750613976440000e-001) (16, 1.23940015129625060000e-001) (17, -9.44813708935546840000e-002) (18, 4.36050389098330160000e-001) (19, 5.92827453546035170000e-001) (0, -3.10683963205742500000e+000) (1, -2.76965623232375950000e-001) (2, 3.06988894791306690000e+000) (3, -1.93266055359103820000e+000) (4, -2.31733661308060830000e-001) (5, 1.18702046857032720000e+000) (6, -1.49750508128481210000e+000) (7, -3.07804172358403790000e-001) (8, -4.72343874247601600000e-001) (9, 4.95370155168153580000e-001) (10, -9.84515226922733520000e-001) (11, -1.56042014625954350000e+000) (12, -4.32205557033667810000e-001) (13, -6.13635145547732800000e-001) (14, -1.19748334649681850000e-001) (15, 4.98729383479661670000e-001) (16, 2.03412144345291250000e+000) (17, 6.25811681293375060000e-002) (18, -1.07273544308106720000e+000) (19, 1.61329950406176540000e-001) (0, 3.04388186744740620000e+000) (1, 3.78721827872250010000e-001) (2, 3.14201419312844180000e+000) (3, -3.20897637907052560000e-001) (4, 1.15923726503132520000e+000) (5, 2.07590579108813680000e-001) (6, -5.01634272232903360000e-001) (7, -8.79549677129571440000e-001) (8, -1.26913804343506210000e+000) (9, 4.18559474950252200000e-001) (10, -3.09781863582925170000e+000) (11, 5.09578686653688610000e-002) (12, -8.58291327584727700000e-001) (13, 1.15313338374340070000e-001) (14, -4.16299795077903490000e-001) (15, -1.59980000468412500000e+000) (16, 3.87771824773307130000e-001) (17, -1.27447962765516560000e-001) (18, -1.25730332646183610000e+000) (19, 3.07658594391450320000e-001) (20, 4.65677099371686170000e-001) (21, 5.02169066258245300000e-002) (22, -3.13801522065838060000e-001) (23, 5.01413183888570260000e-001) (20, 4.55791318691537720000e-001) (21, -2.78998693260955330000e-001) (22, 3.29473045448322400000e-001) (23, 4.89305723487816260000e-001) (20, 1.95723087869807800000e-001) (21, 3.51054042411595270000e-001) (22, 1.61977818147146150000e-001) (23, 4.90327913060546460000e-001) 

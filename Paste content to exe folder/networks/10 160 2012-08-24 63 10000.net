FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 4 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (4, 5, 5.00000000000000000000e-001) (4, 5, 5.00000000000000000000e-001) (4, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -2.08085781262998460000e+001) (1, 7.23766519340978130000e+000) (2, -1.44724427139128240000e+002) (3, 1.40047499624285270000e+001) (4, -2.98817666653705590000e+000) (5, 1.55944937493256400000e+000) (6, 6.03783125412468210000e+001) (7, 2.84364771393774740000e+000) (8, 4.95115259249149200000e+001) (9, 2.46196069028584310000e+000) (10, -4.59071643368389670000e+001) (11, 6.90778977368713050000e+000) (12, 6.84586907678589970000e+001) (13, 5.42689808353786600000e+000) (14, 9.63436979480887170000e+001) (15, 2.56490524025356900000e+000) (16, -2.23027333173979120000e+001) (17, 1.65930480588712940000e+000) (18, 3.95882783414493100000e+001) (19, -2.40151904789735940000e+000) (0, -1.97773090793918980000e+001) (1, 1.26919358372269260000e+000) (2, 3.66837747669062040000e+001) (3, -4.98656265344058710000e-001) (4, 1.09930133625807930000e+001) (5, -4.54231737109509750000e+000) (6, -3.27564965366980090000e+001) (7, -2.45331598259959480000e-001) (8, -6.94464907196587830000e+001) (9, 2.77181330927734230000e+000) (10, -1.12925921759096400000e+002) (11, -3.90364901696709840000e-001) (12, 1.89465397161364950000e+001) (13, 5.88193136526016240000e-001) (14, -2.76237665149532010000e+000) (15, -1.49556291597193440000e+000) (16, 5.14782702660621750000e+001) (17, 2.38959871944550730000e-001) (18, -1.25006583729384210000e+001) (19, -9.03779949561838160000e-001) (0, -2.45328301038235820000e+002) (1, 2.97942532024648690000e+001) (2, 4.03227168542429010000e+002) (3, 7.07013952395755750000e+000) (4, 3.44988306818090450000e+002) (5, -1.06817944386057950000e+001) (6, 2.86433358464001060000e+002) (7, 2.20981242472150190000e+001) (8, -5.17528036430649080000e+002) (9, 5.33159487534731370000e+001) (10, -1.50000000000000000000e+003) (11, -3.68411541772473750000e+001) (12, 5.03541408272738120000e+002) (13, 2.73074669459673380000e+000) (14, 8.05175610592103650000e+001) (15, 8.76510794314066110000e+000) (16, 1.01348102590815880000e+003) (17, 2.60215287672270390000e+001) (18, 1.98892703035048500000e+002) (19, 1.03834102066282630000e+001) (20, 3.46485633071844880000e-001) (21, -1.52789030776886390000e+000) (22, 1.48822288620322380000e+000) (23, 2.57949815101583770000e-001) (20, 4.60346574790518940000e-001) (21, 7.43283690101180780000e-001) (22, -8.26010263603643490000e-001) (23, 8.62699554873567860000e-001) (20, -9.42393348073276900000e-001) (21, 1.80456148747296240000e-001) (22, -8.90198170519735100000e-002) (23, 1.22085097108758370000e+000) 

FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 5 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -3.21889557028894080000e+000) (1, 2.39123826896885660000e-001) (2, 1.46052647971092020000e+001) (3, -7.48971703118832990000e-001) (4, -8.71815148329334550000e+000) (5, -8.05122357052995730000e-001) (6, 3.41976892109986960000e+001) (7, 5.91632458464620430000e-001) (8, 1.33298300667060870000e+001) (9, -3.86772018957050430000e-001) (10, 8.95065077198013980000e+000) (11, -2.55286819675239760000e+000) (12, 4.22555714954152040000e+001) (13, -4.14617774602626610000e-001) (14, -1.53613033441780140000e+001) (15, 1.59196357931990540000e+000) (16, 5.30993593187512470000e+001) (17, 7.53160922724103730000e-001) (18, -1.88995192917171520000e+001) (19, 3.53727886452818920000e+000) (0, 1.34749571583789770000e+001) (1, -8.39500368964649910000e-002) (2, -6.26111677081236540000e+000) (3, -2.53076534438632140000e+000) (4, -2.93069648905276840000e+001) (5, -1.91046179403884860000e+000) (6, -3.25282731682596580000e+000) (7, -7.20712682122356290000e-001) (8, -1.08055463919168030000e+001) (9, -1.55798652474167310000e+000) (10, 1.64943239778339040000e+001) (11, -5.93373233962637410000e-001) (12, 1.69081075655857060000e+001) (13, -2.34131258854684780000e-001) (14, 5.46855780702889050000e+000) (15, -2.03132056952696600000e+000) (16, 9.13729309157108640000e+000) (17, -3.54926526994188140000e-001) (18, -8.93186881149776820000e+000) (19, 3.37313299946182400000e+000) (0, -1.70421152548547180000e+001) (1, -3.94261299283016560000e-001) (2, 9.61245594272972960000e+000) (3, 6.81300105180993040000e-001) (4, -3.69630984957923520000e+001) (5, 8.95860368891307070000e+000) (6, -1.05328108160896150000e+002) (7, 1.31519782407794410000e+000) (8, 6.51636193283832630000e+001) (9, 4.24546763727012970000e-002) (10, 6.18802057899309420000e+000) (11, -2.48525273551137940000e+000) (12, -4.75803025343073360000e+001) (13, -4.07706499808702050000e-001) (14, 1.08847638601537260000e+000) (15, 2.85180121371784170000e+000) (16, -1.21208108952851940000e+001) (17, 1.41729285688638250000e+000) (18, 3.30727475782636450000e+001) (19, 3.50619456429100800000e+000) (0, 4.41662031860868340000e+000) (1, -1.80482503318535010000e+000) (2, 5.33838728963551860000e+001) (3, 8.14836283651374170000e-001) (4, 2.93249927299301310000e+001) (5, 2.84299735345578690000e+000) (6, 2.19853621636306680000e+000) (7, 2.13391879924384660000e-001) (8, -1.00503986706501450000e+001) (9, 2.66552946226796370000e+000) (10, -2.24334659538059180000e+001) (11, -9.03079816315778380000e-001) (12, -1.14207260466961650000e+001) (13, 4.00377029627524750000e-001) (14, 3.68641327956043790000e+000) (15, 4.13496077853000710000e+000) (16, 1.42952472869729820000e+001) (17, 2.12369150022883400000e-001) (18, -3.65669165205549120000e+001) (19, 5.25060248681375910000e-001) (20, 1.01570926835367260000e+000) (21, -1.27315152764811620000e+000) (22, 2.91621066917409970000e-001) (23, -1.39854057186084210000e+000) (24, 7.33657459252548550000e-001) (20, -3.15293397372598760000e-001) (21, 2.08395111366555000000e-001) (22, -6.97439958961372450000e-001) (23, 2.83898630163923030000e-001) (24, 1.20296134035076400000e+000) (20, -4.46863835090289870000e-001) (21, 8.01567037905289270000e-001) (22, 2.88778866572141800000e-001) (23, 8.27895173250212000000e-001) (24, 3.74023796952807910000e-001) 

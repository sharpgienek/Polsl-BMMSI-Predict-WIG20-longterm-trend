FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 6 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (6, 5, 5.00000000000000000000e-001) (6, 5, 5.00000000000000000000e-001) (6, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -8.47875337289181100000e+000) (1, 6.92144025922157150000e-002) (2, -3.94189951290108410000e+000) (3, 1.25384297034711100000e+000) (4, 4.14037449029210420000e+000) (5, 1.00815369524937330000e+000) (6, 8.32274335622392950000e+000) (7, 7.44429911701879780000e-001) (8, 9.42625634121696090000e+000) (9, 9.59909533065744670000e-001) (10, -3.24446277679199090000e-001) (11, 3.24412423627592170000e+000) (12, 1.53930838212660640000e+001) (13, 8.11064273196162260000e-001) (14, 2.40858096935636930000e+001) (15, 4.39017842071079970000e-001) (16, -5.16142639724415360000e+000) (17, -1.44258424396988830000e-001) (18, -4.33304496995179370000e+000) (19, 5.49663997370532400000e-001) (0, 1.61605748234991900000e+000) (1, -4.01743662096183060000e-001) (2, 1.11999663319902180000e+001) (3, -2.38880598029285230000e+000) (4, -3.21779772349409000000e+000) (5, -9.56996272552056880000e-001) (6, -6.66172185056025510000e+000) (7, -3.54367049187316210000e-001) (8, -1.35324158656474740000e+001) (9, -4.13559399654868030000e-003) (10, 6.60768696551524570000e+000) (11, -1.84324133575442310000e+000) (12, 1.84195978761014910000e+001) (13, -3.69476756193203140000e-001) (14, -5.53912606174524940000e+000) (15, -1.17575998362333320000e+000) (16, 8.09624856554566110000e+000) (17, 1.82066789506372560000e-001) (18, 8.87252898494622850000e+000) (19, -7.82842738455468810000e-001) (0, 3.09776205215126770000e+000) (1, 1.04748263256504750000e+000) (2, 2.30520399583082190000e+000) (3, 6.80377280064800720000e-002) (4, 2.29589219845400690000e+001) (5, 4.84978814295446890000e+000) (6, -2.28273656933599440000e+001) (7, -1.03311787597797840000e-001) (8, -3.19901821391017440000e+001) (9, 2.94442077363793290000e+000) (10, 6.70121677175417310000e+000) (11, 1.28039462935814070000e+000) (12, 2.00993166387258440000e+000) (13, 8.04388738345610750000e-001) (14, 3.13681923470860830000e+001) (15, 2.23519355770225210000e+000) (16, 1.16767681055995800000e+000) (17, -3.66275779169408290000e-001) (18, -2.57424189755242200000e+001) (19, 1.19911925125869120000e+000) (0, -3.24133559834144260000e+001) (1, -3.14236656998507510000e+000) (2, 2.33940463534956860000e+000) (3, 1.76476483782595710000e+000) (4, -2.38590496971296740000e+001) (5, -9.66168846303839920000e-001) (6, 3.58010165610152380000e+000) (7, 3.91156095175328460000e-001) (8, 1.29901560548891910000e+001) (9, -6.13121333310753140000e-001) (10, -3.81390558528976770000e+001) (11, 4.21071556296346470000e+000) (12, -2.99107992914398050000e-002) (13, 6.17666783937569690000e-001) (14, 1.97329798525498720000e+000) (15, 5.65138835489156550000e-001) (16, -1.09698996635749550000e+001) (17, -2.61466863095163540000e-001) (18, -1.05140669322825830000e+001) (19, 9.59608969083260320000e-001) (0, 3.55978095778199870000e+001) (1, -5.76087997398418430000e-001) (2, -1.07673294802515220000e+001) (3, -4.48415163849737300000e+000) (4, -2.92068508160511260000e+001) (5, -4.22807179429372050000e+000) (6, 6.59736369490768480000e+001) (7, 3.38524039091231800000e-001) (8, -1.40210392104173480000e+001) (9, 1.73626902787074040000e+000) (10, -3.10348158861795160000e+000) (11, 5.82441682439440860000e-002) (12, 2.61739580446186080000e+001) (13, -7.26157026015547550000e-001) (14, -5.48539836494073720000e+000) (15, -7.78924166132503610000e+000) (16, -3.20580535994673220000e+000) (17, -4.23600133232906790000e-001) (18, 8.89831590532882190000e+000) (19, -4.44121021668511080000e-001) (20, 2.27097203606032760000e+000) (21, 7.28807744911908470000e-001) (22, -1.20428127220191120000e+000) (23, -1.17910505161154200000e+000) (24, -5.88518491698049600000e-001) (25, 6.44992617430797720000e-001) (20, -1.42791251320617720000e+000) (21, -2.14677627067455830000e+000) (22, 7.35773809409978940000e-001) (23, 4.83737417581596660000e-001) (24, 1.17443695019841780000e+000) (25, 3.19080635325508290000e-001) (20, -6.03026845330604240000e-001) (21, 7.71983395324396080000e-001) (22, 3.45315135287469600000e-001) (23, 3.73676794431090810000e-001) (24, -3.47907879856151570000e-001) (25, 1.00024355924699230000e+000) 

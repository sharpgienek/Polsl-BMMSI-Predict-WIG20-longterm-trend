FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 5 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -4.72294479505154000000e+001) (1, -3.81243421632311020000e+000) (2, 7.10341960712943890000e+000) (3, 1.90828595096759420000e-001) (4, -5.03470193944761140000e+001) (5, -9.88076711320082520000e+000) (6, 2.50109277469883380000e+000) (7, 1.71005812269632560000e+000) (8, 7.19532908534726370000e+001) (9, -8.32025114363364790000e+000) (10, 3.19709361934606200000e+001) (11, -3.52506303907562570000e+000) (12, 8.92741658249323390000e+001) (13, -1.38831936717231330000e+000) (14, -2.07026601019891350000e+001) (15, -6.01919959054199620000e+000) (16, 4.18923726347082500000e+000) (17, 7.42815439681137170000e-001) (18, 7.68468827605858710000e+001) (19, 1.27722216008635560000e-001) (0, -7.66152725610097890000e+001) (1, -3.61072681166890060000e+000) (2, 2.39464544598424940000e+002) (3, -1.13323667323494840000e+000) (4, -4.60978355647267930000e+001) (5, 1.60036351935673550000e-002) (6, -3.70908187769671710000e+002) (7, -6.16791682763755760000e+000) (8, -1.02346603112272130000e+002) (9, -4.65620441629921360000e+000) (10, 1.99079450564392290000e+000) (11, -2.09798442046134070000e+001) (12, 2.32101214412761760000e+001) (13, -2.25852314734732250000e+001) (14, -4.94715958609866680000e+001) (15, -1.43673316709009120000e+001) (16, -1.71430835005136600000e+002) (17, 2.65719570070975180000e+000) (18, 2.78745056603060700000e+001) (19, -1.25047962080690360000e+001) (0, -7.66425556712564320000e+001) (1, -1.56024247328924450000e+001) (2, 2.12320006560055110000e+001) (3, -1.76951969585282320000e+001) (4, -3.14580856862845660000e+001) (5, -1.88292718262039070000e+001) (6, -3.79194716015911980000e+001) (7, 1.12942912034353400000e+000) (8, -1.40706084720163890000e+002) (9, -1.24541346861124200000e+001) (10, -2.78200555823766590000e+002) (11, -2.46600727658175240000e+001) (12, -7.32749240965173240000e+001) (13, 2.47154696913597990000e+000) (14, -6.94998971599567600000e+001) (15, 4.44922553760276870000e+000) (16, -3.26626242893610690000e+001) (17, -4.11477234537751540000e+000) (18, -5.46247178975237090000e+001) (19, 5.47841173567061810000e+000) (0, -6.52307178954461900000e+001) (1, -2.93981565085756500000e+000) (2, 1.49279533056443030000e+002) (3, -1.85397679375752350000e+001) (4, 3.04751457953503120000e+002) (5, -1.53956162266771030000e-001) (6, -3.57385431393709890000e+001) (7, 2.41362133691879810000e-001) (8, -2.63552340459764240000e+001) (9, 7.17375219561779960000e+000) (10, 1.84366367292258570000e+001) (11, -8.03590911363488300000e+000) (12, -4.27866057984035560000e+000) (13, 1.93317406137549770000e+000) (14, 1.40780818714026330000e+001) (15, 1.27848540056250550000e+001) (16, 1.09828841991011090000e+002) (17, 8.48702541278619990000e-001) (18, 5.98660789265750640000e+001) (19, -4.01182587285340450000e-001) (20, 1.76860175729351290000e+000) (21, -1.77572828716119440000e+000) (22, -1.76019947347606600000e+000) (23, 1.65559667717387060000e+000) (24, 7.47678394559174040000e-002) (20, -7.68335546454563920000e-001) (21, 8.83785341505557570000e-002) (22, 6.77677535756002760000e-001) (23, -9.18543007169503100000e-001) (24, 7.95855139581687630000e-001) (20, -3.16712812269480450000e-001) (21, 1.62994732843388900000e+000) (22, 4.12401914926303570000e-001) (23, -1.36216694059403120000e-001) (24, 1.91427689289024450000e+000) 

FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 5 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -3.20268313934497950000e+000) (1, 1.70807512352091350000e-002) (2, 1.46316474158788460000e-001) (3, -9.73394330210595460000e-002) (4, 3.77632608574400900000e-001) (5, 2.94322364400099660000e-002) (6, -5.34503088065734770000e-002) (7, 1.50460880175852930000e-001) (8, 1.18930527380521460000e+000) (9, -1.75507158795224470000e-001) (10, 3.98261016334795550000e-001) (11, -3.94973846121207010000e-001) (12, 2.69209217667882580000e-001) (13, 5.16388572189150350000e-002) (14, -1.30818062113237930000e-002) (15, -8.24834757684385360000e-002) (16, 2.73633477976363080000e+000) (17, 3.60425407467689920000e-002) (18, 2.98896607293303140000e-001) (19, -3.20805700017961550000e-001) (0, -3.12281253268606470000e+000) (1, 5.85846581036655320000e-002) (2, 2.89816232156476450000e-001) (3, -6.58330848953283810000e-002) (4, -1.85066332776956900000e+000) (5, -1.45662259746524720000e-002) (6, -1.68426281734142260000e-001) (7, 6.57013574284626150000e-002) (8, 1.16677109378192910000e+000) (9, -1.52069658445978500000e-001) (10, 1.05021225415444210000e+000) (11, -3.93582244537970530000e-001) (12, 3.01051259157990910000e-001) (13, -2.86412388958586340000e-001) (14, 1.75382580536225300000e-001) (15, 1.24402295350292510000e-001) (16, 9.13738375819250550000e-001) (17, 6.33374630284950220000e-002) (18, 5.53498539988840580000e-001) (19, -3.04613072823547600000e-001) (0, 3.14330813846022310000e+000) (1, 4.31667003440116760000e-002) (2, -3.06621463145206530000e+000) (3, 1.70002091339121140000e+000) (4, 1.07274287600597320000e-001) (5, 1.41954024241900510000e-001) (6, 6.66173344447977040000e-001) (7, 8.42780477645271860000e-001) (8, 5.26469973225609730000e-001) (9, 3.17401546210668780000e-002) (10, 6.28392582166249820000e-001) (11, 1.11322221768286190000e+000) (12, 1.42770616580065780000e+000) (13, 1.36214797433661360000e-001) (14, 1.82710703738073970000e-001) (15, -3.33798573145414500000e-001) (16, -2.10186221626858940000e+000) (17, -2.79671384387469020000e-002) (18, 6.27025394645877140000e-001) (19, -3.68338368581972550000e-001) (0, -6.84611073936437160000e-001) (1, 4.83103421582047220000e-001) (2, -1.06760238636395370000e+000) (3, 2.54461987276577430000e-001) (4, -9.67852402772540810000e-001) (5, 2.81078147347378020000e-001) (6, -1.31010436959857430000e+000) (7, 3.49136570291346980000e-001) (8, 4.93492677812334690000e-001) (9, 3.65924448222169350000e-001) (10, -3.85873504188726350000e-001) (11, 9.55897968283016680000e-002) (12, 1.32528921225637810000e-001) (13, 8.48977013851203720000e-002) (14, 6.93575294627947650000e-001) (15, 3.66402276359169390000e-001) (16, 1.43570787490426750000e+000) (17, 3.72682794030881390000e-001) (18, 1.34558524767376900000e-001) (19, 6.52897125284045420000e-001) (20, 4.70524958914297410000e-001) (21, 3.41544905033723590000e-001) (22, 3.21514204303416100000e-001) (23, 7.81970340118150810000e-001) (24, 5.06886640711464280000e-001) (20, 1.23733600323999020000e-002) (21, -6.13860313407484440000e-003) (22, 3.39643461146287520000e-001) (23, 3.35971286892043970000e-001) (24, 5.21890479115580730000e-001) (20, 5.52627649683768290000e-002) (21, 5.89431849450943600000e-002) (22, -4.79153534875582150000e-001) (23, 4.62936620815224850000e-001) (24, 3.48691013644082670000e-001) 

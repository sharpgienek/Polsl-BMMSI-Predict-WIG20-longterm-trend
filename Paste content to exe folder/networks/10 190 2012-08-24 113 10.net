FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 6 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (6, 5, 5.00000000000000000000e-001) (6, 5, 5.00000000000000000000e-001) (6, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -1.33321275142109960000e+000) (1, 4.31361755136883130000e-002) (2, 4.08738772762284150000e-001) (3, -2.62198082478375340000e-001) (4, -1.16968125484366550000e+000) (5, 6.70985782145059330000e-002) (6, 2.11601339778289540000e-001) (7, 2.46097336822346970000e-001) (8, 1.36349718046886940000e+000) (9, -1.89183257112926910000e-002) (10, 3.04056765841364560000e+000) (11, -8.86441555633983300000e-001) (12, 9.18236289154702460000e-002) (13, 6.72082326021676830000e-002) (14, -3.72855387711321320000e-001) (15, 5.53791801851692740000e-001) (16, 4.79106691269841790000e-001) (17, 4.47719770409647000000e-001) (18, 6.88642216116525760000e-001) (19, -1.24960531605251520000e-001) (0, 1.54194397976118890000e-001) (1, 5.20262593446187750000e-001) (2, 1.19680649462011650000e+000) (3, -2.78727023487620520000e-001) (4, 3.03251712118430830000e+000) (5, 1.56717452677982950000e-001) (6, -3.05208499256964230000e+000) (7, -1.07602523805196390000e+000) (8, -4.45517963943615090000e-001) (9, 1.98576939195116080000e+000) (10, -3.20546023899573650000e+000) (11, -1.10296615642216490000e+000) (12, -1.44333776116832200000e+000) (13, 1.84574203481247260000e-002) (14, -2.68891503353109460000e-001) (15, 3.94718249581387510000e-002) (16, 7.70388769834535640000e-001) (17, 1.53261102629691340000e-001) (18, -2.26413096706159140000e+000) (19, 5.37938374959577410000e-001) (0, -3.09841067685369880000e+000) (1, 8.17900988216144900000e-002) (2, -6.21906232086381180000e-001) (3, 3.60602881222712830000e-001) (4, -1.24441514506022300000e+000) (5, 3.59542360398077190000e-001) (6, 2.94597560656630510000e-001) (7, 5.46855030248354800000e-001) (8, 6.23672626403624530000e-001) (9, 3.28975953692602240000e-001) (10, 1.32763708222409620000e+000) (11, 2.83663720871679680000e-003) (12, -1.79000196603735620000e-001) (13, 3.32528199839284830000e-001) (14, -1.22323754720909390000e-001) (15, 6.27687190636691650000e-001) (16, 7.43769245701139670000e-001) (17, 2.41767430233176650000e-001) (18, 1.02146940903194360000e+000) (19, -1.26242352474722540000e-001) (0, 1.25523946372749440000e+000) (1, 8.93031683891300360000e-002) (2, -2.06086490120455680000e+000) (3, 6.17041940149654300000e-001) (4, 2.15886366699011120000e-001) (5, 2.98746291858588420000e-001) (6, 9.34096760080283640000e-001) (7, 3.03691502414452590000e-001) (8, 9.08431476494716120000e-001) (9, 2.27719679472084490000e-001) (10, 1.11861997857368010000e-001) (11, 3.51440805428221260000e-001) (12, 1.79583644381154040000e-001) (13, 2.11661301152960060000e-001) (14, 3.03495454708628470000e-002) (15, 1.47005805967716310000e-001) (16, -6.51687605458773130000e-001) (17, 1.21912938929017440000e-002) (18, 8.91230712852865590000e-001) (19, 6.65280301025819250000e-001) (0, 3.06297976812261210000e+000) (1, 1.68606148086461490000e-001) (2, -5.75937069372833840000e-003) (3, -8.53974846394151000000e-002) (4, -9.62190180037935110000e-002) (5, 2.29489699029709840000e-001) (6, 1.97457702257843370000e-001) (7, 2.14238881615563790000e-001) (8, -2.39762779441274610000e-002) (9, 2.96524583667521490000e-001) (10, -8.52551544481947860000e-002) (11, 7.37021024751089330000e-001) (12, 1.43322657512698240000e-001) (13, 2.08353195634915440000e-001) (14, 4.25077487770341920000e-002) (15, 1.05764350275045760000e-001) (16, -8.33109242793246270000e-002) (17, 7.80286609313407540000e-003) (18, 7.55582863789005660000e-002) (19, 5.60037379535702540000e-001) (20, 4.67877298566340690000e-001) (21, -1.94709141223693150000e-001) (22, 3.38575293003851200000e-001) (23, 5.96870257558917650000e-001) (24, 2.25787670443046820000e-001) (25, 2.36493237008822190000e-001) (20, -2.11438555815018970000e-001) (21, -1.72467966069430740000e-002) (22, -3.01079691327870990000e-001) (23, 4.30809739352609800000e-001) (24, 4.68567898423551840000e-001) (25, 3.76813746665683460000e-001) (20, 1.69683260656223220000e-001) (21, 5.12572163644091420000e-001) (22, -3.08603677769521250000e-001) (23, 2.06113205367161990000e-001) (24, 2.81699143993480230000e-001) (25, 3.62795452737926180000e-001) 

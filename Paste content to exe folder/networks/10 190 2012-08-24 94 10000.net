FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 5 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -1.43626372209799110000e+003) (1, -3.59335686476216760000e+001) (2, -1.37384490927427520000e+002) (3, -2.09515959519494860000e+001) (4, -3.92909800447975210000e+002) (5, 1.58125116389975690000e+002) (6, -2.62948343192718820000e+002) (7, 3.49115444703516000000e+001) (8, 1.04496353969877650000e+003) (9, 1.24673532789494530000e+001) (10, 6.30581929874443790000e+001) (11, -3.74340484582227190000e+001) (12, -1.50000000000000000000e+003) (13, 7.71593897800135590000e+000) (14, 1.35170497856303630000e+003) (15, 8.25019552956264160000e+001) (16, 1.50000000000000000000e+003) (17, -2.93653426285615180000e+001) (18, 1.47992267341039000000e+003) (19, 1.41869785242951090000e+001) (0, -2.95758164508144030000e+002) (1, 1.21223118625526850000e+002) (2, -1.28874496096387630000e+003) (3, 3.10817439833287920000e+001) (4, -2.41356988066954120000e+001) (5, 3.35933868728351380000e+000) (6, 3.57758731239619410000e+002) (7, 5.53771179756657970000e+000) (8, 2.91908917750572850000e+001) (9, 4.11039661907556140000e+001) (10, -7.30432652001793710000e+002) (11, 4.29676320488759420000e+001) (12, 3.68273204415956280000e+000) (13, 1.13621648871588000000e+001) (14, -3.85869969409669520000e+002) (15, 4.13899851912515440000e+000) (16, -5.90434705714766550000e+002) (17, 3.52328383764174260000e+001) (18, -4.45887292307131640000e+002) (19, -8.12931626762171080000e+000) (0, 4.15103981217870630000e+001) (1, 1.39443367574159320000e+000) (2, -8.53914995621970970000e+001) (3, 2.53095990937418410000e+000) (4, -3.92970666725830210000e+001) (5, 1.87272045200817590000e+001) (6, 7.28714642502646370000e+001) (7, 5.39327159300133550000e+000) (8, 7.70990171389501030000e+001) (9, 1.22158589088716700000e+001) (10, -1.18703242975801430000e+002) (11, 8.23650500182261600000e+000) (12, -2.79838576753874800000e+002) (13, 4.87094843398788770000e+000) (14, 2.73556335999254050000e+001) (15, 4.95661103366803600000e+000) (16, 8.02988246265573200000e+001) (17, -3.63444308724002290000e+000) (18, 1.48808879416393860000e+002) (19, -5.57969495579025490000e+000) (0, -1.22775500428933030000e+002) (1, -1.30211179395084640000e+000) (2, 5.20344586520858310000e+001) (3, -6.75841457427163310000e+000) (4, 1.96605701763080670000e+001) (5, -8.59491013099510860000e+000) (6, -3.28089562750153500000e+002) (7, -1.80980492065559170000e+001) (8, -2.26173971884035840000e+002) (9, -6.58810550870135090000e+000) (10, -8.06927940371531920000e+001) (11, -1.23237321669888920000e+001) (12, -6.46886790438983330000e+001) (13, -2.30708549879688670000e+000) (14, -3.26942893614332560000e+001) (15, -8.54830298360147340000e-001) (16, -3.20166864221033040000e+000) (17, -5.01128213357886270000e-001) (18, -2.20951500137076140000e+000) (19, 1.12374084645721570000e+001) (20, 9.24750078784967180000e-001) (21, 9.67016239072999830000e-001) (22, -6.15669382136536440000e-001) (23, -6.44082658524014470000e-001) (24, 2.36559825494781080000e-001) (20, -1.54856652021792770000e+000) (21, -1.24525712558321370000e+000) (22, 1.42686573881149250000e+000) (23, 3.15974785284212980000e-002) (24, 1.45812254496481940000e+000) (20, 1.86005814606415350000e-001) (21, -5.67769468266593890000e-002) (22, -3.30820957073230000000e-001) (23, 6.24537340401981390000e-001) (24, 8.96953272107083310000e-001) 

FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 8 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (8, 5, 5.00000000000000000000e-001) (8, 5, 5.00000000000000000000e-001) (8, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 1.01827328948631710000e+000) (1, 6.73531435002892860000e-002) (2, 7.85135889815022910000e-001) (3, 7.40619831591709740000e-001) (4, -3.07222659443227550000e+000) (5, 9.82524997938001080000e-001) (6, 3.01926638856498200000e+000) (7, 1.69341144889256970000e-001) (8, 1.45356735040723840000e+000) (9, -1.94059216945227760000e-001) (10, 2.85045221310473810000e+000) (11, 1.83474312370514400000e-001) (12, -9.86197403097349710000e-001) (13, -2.45238235226904050000e-001) (14, -7.25027165442687080000e-001) (15, 2.80333572639670570000e-001) (16, 1.44933163767301140000e-001) (17, 2.41023664417793830000e-001) (18, 3.20228641082344280000e+000) (19, -1.23638000326562070000e+000) (0, -5.85445339242938110000e-002) (1, -1.31505313949789730000e-001) (2, -2.15427989120593290000e+000) (3, -3.07136599871632670000e+000) (4, 2.73309991643417760000e+000) (5, -6.32208143871367150000e-001) (6, -3.06605594004345060000e+000) (7, -4.88021364607597100000e-001) (8, 2.79522665517906220000e-001) (9, 7.37251438110235520000e-001) (10, -2.25090464850512890000e+000) (11, 8.42416311367350510000e-002) (12, 2.69689386120716530000e+000) (13, 2.93829555159960790000e-001) (14, 3.00962065284864910000e-002) (15, -1.48521222216522620000e+000) (16, 6.62518157866255230000e-001) (17, 1.04865596122708060000e-001) (18, -6.50106794888342220000e-001) (19, 1.87055241654332400000e-001) (0, 2.83466335686840380000e-001) (1, 2.89088368354729950000e-001) (2, 3.15530154684206380000e+000) (3, 7.34749539407302250000e-001) (4, -3.06653811778501060000e+000) (5, 2.66243341147491460000e+000) (6, 3.16039798246488160000e+000) (7, 5.02557104100806120000e-001) (8, 5.72617882435523810000e-002) (9, 1.95405405159509990000e-003) (10, 3.12220828463018570000e+000) (11, 2.33770175673692680000e-001) (12, -1.45189961185523760000e+000) (13, -7.30493473942646410000e-001) (14, -7.46078191146996810000e-001) (15, -1.94107885381836760000e-002) (16, 1.23871780477685770000e-001) (17, 6.63371830734356240000e-001) (18, 9.50690935807334550000e-001) (19, -9.63839820627591100000e-002) (0, 7.15155964024768550000e-001) (1, -5.03407452358081150000e-002) (2, -3.92617543625315870000e-001) (3, 7.24348125021958220000e-001) (4, 2.81773986118888260000e-001) (5, 4.91560168925464110000e-001) (6, 4.29768506870940040000e-002) (7, 1.49298995841655600000e+000) (8, 1.54472958831330160000e-001) (9, 1.16441917328689820000e-001) (10, 8.92048787196952910000e-002) (11, -1.11382022634304300000e-001) (12, 1.68114316456477230000e-001) (13, 1.17001080314771830000e-001) (14, 1.72604296460096920000e-001) (15, 6.21229388781271990000e-001) (16, -8.78627507523081250000e-003) (17, 3.01895069871054580000e-001) (18, 6.36933847870995160000e-001) (19, 5.94239642285343430000e-001) (0, 1.77160548641922100000e-001) (1, -1.78555334296581520000e-001) (2, -1.63082921956649610000e-001) (3, -8.11447658032112160000e-001) (4, 3.94155889010442850000e-001) (5, -5.13648924344209170000e-001) (6, 3.91264847880322620000e-001) (7, -5.81320659047791730000e-001) (8, 1.38517537920880370000e+000) (9, 4.23535695882554910000e-001) (10, -8.25325689393351660000e-002) (11, 3.03181966983579740000e-001) (12, 1.14152525398465740000e+000) (13, 2.88049854019908570000e-001) (14, -1.95949090226845760000e-001) (15, 2.43779967434943900000e-001) (16, 3.92989054115729760000e-001) (17, -1.65418079665392540000e-001) (18, 6.53774268644241150000e-001) (19, -6.10589812031379210000e-003) (0, -3.71679110662306440000e-002) (1, -1.34427167049810780000e-001) (2, 2.27070164389170330000e-001) (3, -8.08943348795571500000e-001) (4, -1.86924857278961550000e-001) (5, -1.07723920799139930000e+000) (6, 3.17680423270935370000e+000) (7, 1.60353903374150900000e-001) (8, 1.09955528809272730000e+000) (9, -4.80081269264406470000e-001) (10, 1.32008138409820260000e+000) (11, 2.76952464534171610000e-001) (12, -5.81660898285038420000e-001) (13, -9.19056151511362000000e-001) (14, -1.76579760788791010000e-001) (15, -5.28479703522535820000e-003) (16, 3.30274183537351440000e-001) (17, 7.13775759023847380000e-001) (18, 1.13444077103454480000e+000) (19, -1.33100280740998530000e+000) (0, 3.15936038479317020000e+000) (1, -2.82239489374155670000e-002) (2, -1.65906479828158810000e-002) (3, 6.03321173423024600000e-001) (4, -9.97936857982106520000e-001) (5, 1.88108189039119590000e+000) (6, 7.58925637398618620000e-001) (7, 1.94650215113392310000e-001) (8, -2.39584762216633770000e-001) (9, 1.08110618376418790000e-001) (10, 2.55795215668488230000e-001) (11, -3.15866443971954960000e-001) (12, -1.06266704920748220000e-001) (13, 1.50845074959711610000e-001) (14, 1.21993944143443940000e-001) (15, -2.73597081852340050000e-001) (16, 2.32933618283301420000e-001) (17, -2.31458488498175300000e-001) (18, 1.46302235877431940000e-001) (19, 1.30241629595780510000e-001) (20, 4.14403489108137620000e-001) (21, 1.54161211239594360000e-001) (22, 3.82630397142983300000e-001) (23, -6.34175615238166680000e-005) (24, 8.00546232245539670000e-001) (25, 6.92183820454420620000e-001) (26, 1.13416883831156930000e-001) (27, 4.85512995103969780000e-001) (20, 1.75265429312250890000e-001) (21, -5.39908679925459320000e-001) (22, 2.35281299347182030000e-001) (23, 5.39295765339904860000e-001) (24, -4.81166000884689470000e-001) (25, -2.13734590777970410000e-001) (26, 5.20095273638239710000e-001) (27, 4.69227433391266440000e-001) (20, -4.24136273988769190000e-001) (21, 1.11681291874173550000e+000) (22, -6.21191925155224700000e-001) (23, 5.12721086383122300000e-001) (24, 1.86577473271098280000e-001) (25, -3.34936243079015090000e-001) (26, -3.56929710837790020000e-002) (27, 1.10903239593048220000e+000) 

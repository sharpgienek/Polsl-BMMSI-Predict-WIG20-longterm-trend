FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 8 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (8, 5, 5.00000000000000000000e-001) (8, 5, 5.00000000000000000000e-001) (8, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 5.33919517956327280000e+000) (1, -3.05299936987501850000e-001) (2, -7.18857593011892430000e+000) (3, 1.41870155640632740000e+000) (4, 4.22498426069591910000e+000) (5, 2.43157443898953620000e+000) (6, -4.05407153066853710000e+001) (7, 1.78192115258109310000e-002) (8, 2.32243558549113130000e-001) (9, 2.31032458932261520000e+000) (10, -2.47953900981848680000e+000) (11, -1.27575116311840820000e-001) (12, 4.48443909434858060000e+000) (13, 1.70649729346666310000e+000) (14, 1.08968396015321500000e+001) (15, -4.77435245862597650000e-002) (16, 4.43769280075766160000e-001) (17, -9.67671927301126940000e-001) (18, -9.08942560025098880000e-001) (19, 5.66588501769911380000e-001) (0, 5.30453597575215690000e+000) (1, 1.03189878180052620000e-001) (2, 9.82060343818690010000e-001) (3, 2.00934155701993110000e+000) (4, -4.95432562339850690000e+001) (5, 3.07304247400608690000e+000) (6, 1.48339567739531260000e+001) (7, 3.36351771485784770000e-001) (8, 3.05436173343250590000e+000) (9, 1.17889279632742270000e+000) (10, 1.10726879401078300000e+001) (11, -2.46869517820924620000e-002) (12, -1.00738061719446340000e+001) (13, -5.49106730654494560000e-001) (14, 3.52989441370074180000e+001) (15, 7.10324519213038190000e-001) (16, -5.65904506358985590000e-001) (17, 8.56326587030204370000e-001) (18, 2.73632212864468190000e+001) (19, -9.27685210444592180000e-001) (0, -7.08178845766434150000e+000) (1, -3.15588605353903650000e-002) (2, -1.91076192084307210000e-003) (3, 5.97450428736697510000e-001) (4, -8.70655197226094660000e-002) (5, 9.98999206074289380000e-001) (6, -1.44406635482040710000e+001) (7, 1.03395569956609280000e+000) (8, -1.26177292065058080000e+000) (9, 4.10121324120786450000e-001) (10, 1.94811982625544640000e+000) (11, 5.88848687251211370000e-002) (12, -1.50886885516161580000e+000) (13, 2.98444592034319940000e-001) (14, 3.58510191665875940000e-001) (15, 7.24933300516080960000e-001) (16, -3.33995458582399960000e+000) (17, -1.54941792427638310000e-001) (18, -1.84503060204826850000e-001) (19, 7.12416837762040870000e-002) (0, -4.83678466519285920000e-001) (1, 1.01889897470271760000e-001) (2, -3.32912623673733750000e-001) (3, -1.50133697833956050000e+000) (4, 1.16135687600109260000e+000) (5, -3.23609587669469350000e+000) (6, 1.24702480416190080000e+001) (7, -7.00112505439094760000e-001) (8, 2.43326985231856960000e+000) (9, 1.09122459639813820000e+000) (10, -1.94917828564645070000e+001) (11, 2.06328355365304580000e+000) (12, -6.12468174939436590000e+000) (13, 2.93420397392353930000e-001) (14, 2.11097551556699240000e-001) (15, 2.52244045341611100000e-001) (16, 2.27523198164997930000e+000) (17, -1.29349712960516980000e+000) (18, -3.31277847641496520000e+001) (19, 4.93870291120377500000e-001) (0, -6.74461072819700650000e+000) (1, -5.75651571174714990000e-001) (2, -1.23347600482080890000e-001) (3, -6.31226348852962690000e-001) (4, 1.52997125316929580000e+001) (5, -1.42646045823353820000e-003) (6, -9.06026427413829330000e-001) (7, -3.81449726634450260000e-001) (8, -1.93464566074002930000e+000) (9, -4.73291421194295070000e-001) (10, 6.12898761375761270000e-001) (11, 3.84988429497631070000e-001) (12, 5.65198667940218690000e+000) (13, -1.43886938140077030000e-001) (14, -3.06770501431572720000e-002) (15, -4.96830936153625090000e-002) (16, -6.10087537681875620000e-001) (17, 2.25645915397994970000e+000) (18, 4.91069986399527990000e+001) (19, 1.95765449167730720000e-002) (0, -1.66465277368943720000e+000) (1, -1.35884218432912790000e-001) (2, 3.25605040110698110000e+000) (3, 4.57930463300016690000e-001) (4, -1.10289230138127190000e+001) (5, 1.85476829250017010000e+000) (6, 2.82921044420892860000e+001) (7, 3.04325722660103710000e-001) (8, 4.93460227157242670000e+000) (9, -5.42358686831502790000e-001) (10, 4.59687608971508950000e+000) (11, 5.81684116596377310000e-001) (12, 3.75251519217431850000e+000) (13, 1.80635417121969590000e-001) (14, 5.34908269208470680000e-001) (15, 2.56077372843297510000e-001) (16, 1.29527659132528420000e+000) (17, 8.75489311392023910000e-001) (18, 1.52384237527832110000e+001) (19, -2.73110928442567770000e+000) (0, 1.17235357010750470000e+001) (1, -2.53906233437663660000e-001) (2, -1.74972080844309100000e-001) (3, 3.42196282794869970000e-001) (4, 9.68588777181471270000e+000) (5, 2.96500866867897890000e-001) (6, 3.63203774685181900000e+000) (7, 8.00036686702333740000e-001) (8, 2.67441167906729840000e-001) (9, 2.00726363290007280000e+000) (10, 5.39723079331666520000e+000) (11, 3.76430874847987100000e-001) (12, -3.85748224806691130000e-001) (13, 9.34019120844198360000e-001) (14, 1.23783865295657400000e+001) (15, 6.96641320399223550000e-001) (16, 6.16505478822804600000e-001) (17, 6.69468379959055500000e-001) (18, -1.90252598469395220000e+000) (19, 3.01340482159164200000e-001) (20, -4.75797583564474550000e-001) (21, -1.76119475929718470000e-001) (22, 5.96277457460494540000e-001) (23, 1.44561316878274180000e+000) (24, 4.72277281045974770000e-001) (25, 2.72494212648420260000e+000) (26, 1.45440300114708410000e+000) (27, 3.16798437001351680000e-001) (20, -1.63699489464557630000e-002) (21, 2.72670888703712230000e+000) (22, 2.56237193570336560000e+000) (23, -2.37515616945808270000e+000) (24, -2.68336352872722110000e+000) (25, -7.06871704428934790000e-002) (26, 4.10679290958721470000e-001) (27, 9.14822225057982720000e-001) (20, 1.67995698939404290000e+000) (21, -2.77409164548438000000e+000) (22, -1.21027256584419050000e-001) (23, 2.38638032346934460000e+000) (24, 1.19463606731000630000e+000) (25, -1.78169887767853010000e+000) (26, 1.92820620178954540000e-001) (27, 1.89901304090090720000e+000) 

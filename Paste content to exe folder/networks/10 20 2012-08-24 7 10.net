FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 4 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (4, 5, 5.00000000000000000000e-001) (4, 5, 5.00000000000000000000e-001) (4, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 2.09238678512856250000e+000) (1, -1.03776527430796820000e+000) (2, 1.16555039063873920000e+000) (3, 1.55689957336791230000e+000) (4, -9.65118253977446110000e-001) (5, 2.20713214751497410000e+000) (6, 3.01926638856498200000e+000) (7, 3.93347544333930320000e-001) (8, 7.89157349837221740000e-001) (9, -6.29183497487188050000e-001) (10, 1.81371818422703820000e+000) (11, 1.65986987988994970000e-001) (12, -1.13669105703207720000e+000) (13, -5.73203512721278010000e-001) (14, -4.51427125889042770000e-001) (15, -1.32191224337663280000e-002) (16, 1.95217904717617240000e-002) (17, 7.67854896275192430000e-001) (18, 3.20228641082344280000e+000) (19, -1.23638000326562070000e+000) (0, 1.09867939824843770000e-001) (1, -1.75790552392626700000e-001) (2, 2.28786439712744790000e-001) (3, -5.98773084779395840000e-001) (4, -8.40566209018953490000e-002) (5, -6.87693768995916610000e-001) (6, 4.99559461617030800000e-001) (7, -6.49849058201617090000e-001) (8, 1.97038242883125210000e+000) (9, 1.71087114874190200000e-001) (10, -1.02095931926266780000e+000) (11, 5.57978655416887630000e-001) (12, 3.26888551317747520000e-001) (13, 1.20292247684065360000e-001) (14, 6.77051438197065900000e-002) (15, 2.70563886938918200000e-002) (16, -1.75218693507419840000e-001) (17, 3.73541254516128520000e-001) (18, 1.45952989674361830000e+000) (19, -3.85398122635275900000e-001) (0, 7.78103838485485590000e-001) (1, 4.13766977494346380000e-001) (2, 5.17231449604668820000e-001) (3, 2.24931942171627610000e-002) (4, -9.59429777330181200000e-001) (5, 1.03650986137213550000e+000) (6, 5.72727893417165230000e-001) (7, 9.87957126925196110000e-001) (8, 6.13816163165289200000e-001) (9, -1.77684556262971860000e-001) (10, 2.75255172562469840000e-001) (11, -5.79762433964609030000e-002) (12, -9.17653655295511130000e-002) (13, -2.84776059960961810000e-002) (14, 3.01945846788316870000e-001) (15, 6.58324818819671980000e-001) (16, 6.93885681088857180000e-003) (17, -5.04481908776204660000e-002) (18, 1.18065659203571420000e+000) (19, 1.25141781058636050000e-001) (20, 3.30861724103656770000e-001) (21, 1.36733847040687360000e+000) (22, 6.37699257450465180000e-001) (23, 1.96331166504968040000e-001) (20, 7.92548467664849540000e-001) (21, -7.09218005512673110000e-001) (22, 5.18246060481108330000e-001) (23, 1.06798479301413970000e+000) (20, -1.12867103738131960000e+000) (21, -4.65109345213182420000e-001) (22, -2.24629169014695650000e-001) (23, 7.26188904956616370000e-001) 

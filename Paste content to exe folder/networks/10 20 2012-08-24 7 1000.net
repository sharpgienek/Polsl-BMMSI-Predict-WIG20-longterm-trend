FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 4 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (4, 5, 5.00000000000000000000e-001) (4, 5, 5.00000000000000000000e-001) (4, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -1.35379606944352540000e+001) (1, 9.40230891793638330000e-002) (2, 5.23060817818512440000e-001) (3, 2.48213445316002850000e+000) (4, 1.68023157154228390000e+001) (5, 7.22189479909649320000e-001) (6, -1.86229414309999870000e+001) (7, 5.14575058532969900000e+000) (8, -1.28801483369854960000e+001) (9, 5.72145315664239100000e-001) (10, 1.16053917917965040000e+000) (11, -5.93228933441396040000e-001) (12, -2.48034982742109160000e+000) (13, 4.50404131768639860000e-001) (14, -2.50028459970467010000e+000) (15, 6.57547889715131780000e-001) (16, -2.91862107507458720000e-001) (17, -4.16790143198410060000e+000) (18, -3.20640813452316300000e+000) (19, 1.04771969576538980000e+000) (0, -2.26889825869998370000e+000) (1, 3.68022370844823810000e-001) (2, -2.95047293717601890000e-002) (3, 1.07992448668243330000e+000) (4, 2.95269135675958550000e+001) (5, 4.34636067408269270000e-001) (6, -1.87295057358437410000e+001) (7, -3.88321656625370830000e-001) (8, -3.26630618889225060000e+001) (9, -1.16750177539612700000e+000) (10, -5.82771192613748920000e+001) (11, -4.98349547142295120000e-001) (12, 9.84291245541881630000e-001) (13, 8.31068136808404630000e-001) (14, 1.38400644236074120000e+001) (15, 3.59710253459957550000e-001) (16, -9.31949744332451680000e+000) (17, -7.58738607209309080000e-001) (18, -4.53093735192412840000e+001) (19, 4.37459565869053080000e+000) (0, 4.27199716344364830000e+001) (1, 1.46886407333465850000e+000) (2, 1.52962179221174000000e+001) (3, 1.23479551993560380000e+000) (4, -2.44800155939210280000e+001) (5, 4.88180965974477310000e+000) (6, 2.73570843379734040000e+001) (7, 8.79528411018149940000e-001) (8, 2.44943293434811560000e+000) (9, 2.35573005899597120000e-001) (10, 5.10789653411540370000e+000) (11, 2.11928559087983630000e+000) (12, -2.47828805740748370000e+000) (13, -1.76197396955686760000e-001) (14, -4.64847209007264970000e+000) (15, -1.54118691666198320000e-001) (16, 1.63853687300279740000e+001) (17, 7.37385293921918540000e+000) (18, 3.35278195112036390000e+001) (19, -8.52320113089999220000e-001) (20, 2.92094362481126050000e-003) (21, -2.94754374524065230000e+000) (22, -2.72512434994201840000e-002) (23, 2.90641665080559260000e+000) (20, 2.98207610221140080000e+000) (21, 1.32895412632791340000e-001) (22, 2.86123972127217610000e+000) (23, -2.93235268159055540000e-001) (20, -2.72105274244505720000e+000) (21, 3.27581541418460360000e+000) (22, -2.87402426419866600000e+000) (23, 2.13235969888202260000e+000) 

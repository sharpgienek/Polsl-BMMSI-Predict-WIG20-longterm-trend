FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 5 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -4.07340123065578760000e-001) (1, 6.80020561471294670000e-002) (2, 3.05705612469341750000e-001) (3, -4.45645498640188540000e-001) (4, 1.94914744711861880000e-001) (5, -2.39088397745779950000e-001) (6, 1.51577271038433710000e-001) (7, -2.15602069270806380000e-001) (8, 5.24406984913618810000e-001) (9, 2.79895090793536860000e-001) (10, -9.47683640386578760000e-002) (11, 1.22023178793793050000e+000) (12, 2.99439222140374720000e-001) (13, 2.05846588789856570000e-001) (14, 5.88777954267568210000e-002) (15, -1.40123478140710380000e-001) (16, 1.60289947486009750000e-001) (17, 1.52974835442689640000e-001) (18, 1.22260615913164050000e-001) (19, -3.55606723857191050000e-001) (0, -1.49381307255683770000e-001) (1, 1.89727451758560980000e+000) (2, -4.22734498508364310000e-001) (3, -3.03875484723218790000e-001) (4, 6.58376319494067050000e-001) (5, -3.27637678072158930000e-001) (6, -1.52102765238641770000e+000) (7, 2.61163922959007850000e-003) (8, -1.81023974318283560000e+000) (9, 7.99791200975251030000e-001) (10, -4.02215086604619190000e-001) (11, 6.94730034346732890000e-002) (12, 1.08286943402744520000e+000) (13, 6.94195680607081480000e-001) (14, 1.53069972111069540000e-001) (15, 7.77899295135612510000e-001) (16, 5.40961076033874550000e-002) (17, -2.14163979096635090000e-001) (18, -2.46283917094268390000e-001) (19, 6.92682008709526010000e-001) (0, 4.37701545642572460000e-001) (1, -2.28176402396496950000e+000) (2, 1.16819321294374530000e+000) (3, 7.02429047132848710000e-001) (4, -9.59429777330181200000e-001) (5, 1.03650986137213550000e+000) (6, 3.16039798246488160000e+000) (7, 1.73899594304257610000e-002) (8, 1.16523702098010350000e+000) (9, 1.95405405159509990000e-003) (10, 9.64522236493668310000e-001) (11, 3.87804154469379270000e-001) (12, 3.59684069371837560000e-002) (13, -3.98141400554540390000e-001) (14, -4.72478151593352490000e-001) (15, -2.64187612721544780000e-002) (16, 7.70143899284533750000e-001) (17, 7.68971839651211190000e-001) (18, 5.08610896981822160000e-001) (19, -1.37903586621295720000e+000) (0, 5.46901756410560160000e-001) (1, 1.82607620390593280000e-001) (2, 1.57456673880127720000e+000) (3, 5.85006854343084640000e-001) (4, -3.06814944593402130000e+000) (5, 7.82443438159525660000e-001) (6, 3.08835818646953000000e+000) (7, 3.13689631893123540000e-001) (8, 7.13337204829667120000e-002) (9, -8.45376092635970360000e-001) (10, 2.81082054593420170000e-001) (11, -7.07648627325310200000e-002) (12, -1.53310080991495370000e+000) (13, -1.88773370885457760000e-002) (14, 1.62393722293815960000e-001) (15, 6.00366721398410300000e-001) (16, -6.08760984993911960000e-001) (17, 6.28736713111889830000e-001) (18, 1.91108677365080350000e+000) (19, 2.60731808351457320000e-002) (20, 3.36860727833645120000e-001) (21, 2.89744300430965550000e-001) (22, 2.44621467116975940000e-001) (23, 2.46666816597675200000e-001) (24, 3.92015225287236650000e-002) (20, -6.08485532194627530000e-001) (21, 2.00977449226640400000e-001) (22, 2.63114661362278070000e-001) (23, 8.59973059674929210000e-001) (24, 7.23005832429209460000e-001) (20, 2.17119157350598720000e-002) (21, 7.64166384774726200000e-001) (22, -5.33949217615457110000e-001) (23, -1.10355819744661200000e+000) (24, 6.44354371270638100000e-001) 

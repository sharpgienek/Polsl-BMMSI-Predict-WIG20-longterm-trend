FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 5 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 5.92211983334644430000e+001) (1, -5.01360649071281190000e-001) (2, 5.10976984613428090000e+000) (3, 1.16014696174758570000e+000) (4, -6.75232959203912910000e+001) (5, 3.97209647322483760000e+000) (6, 2.08499803196618670000e+002) (7, 1.19937531607617860000e+000) (8, 6.25224634363750550000e+000) (9, 1.39566146825813280000e+000) (10, 4.45380339952447600000e+001) (11, 5.56560057751278480000e-001) (12, -1.34941182181849620000e+001) (13, 8.88938560968280830000e-001) (14, 1.51935090183913600000e+001) (15, 6.61950624430451160000e-001) (16, -1.94249254392895560000e+001) (17, 2.55047804644867120000e+000) (18, 6.13594451633645900000e+001) (19, -1.97400827381187120000e+000) (0, 2.22223751074802940000e+000) (1, 2.13230557862730530000e+000) (2, 3.82709154239690930000e+001) (3, -1.97409780514948530000e+000) (4, 1.07609806436873940000e+000) (5, -1.76074863535098540000e+000) (6, 1.27847031906106000000e+001) (7, 7.30797906004188920000e-001) (8, -2.91626484793475380000e+001) (9, 1.32721957349486440000e+000) (10, 1.50827636992659000000e-001) (11, 2.23379507637260860000e-001) (12, -4.36018372833920510000e+000) (13, 3.28611140349416910000e-001) (14, 2.00717386169429340000e+001) (15, -5.60640970623137220000e-001) (16, 4.51524094535603030000e+000) (17, -7.30904108259167540000e+000) (18, -2.07845998219383570000e+001) (19, 6.22087499385601040000e-001) (0, -9.19197292080828450000e+000) (1, -4.66721435835184880000e-001) (2, -8.27709919038117730000e+000) (3, -9.37956938290958830000e-001) (4, 3.72314070847955310000e+000) (5, -1.14939424680471450000e+000) (6, 5.85927506724610860000e+001) (7, -1.06746458114503100000e-002) (8, 8.49098328947547780000e-001) (9, 4.62413354875991110000e-001) (10, 3.45775881265470060000e+001) (11, 3.32286321182954260000e-001) (12, -2.64714208017611610000e+000) (13, 3.45781548994946120000e-001) (14, 1.85782337236082060000e+000) (15, 1.16181757136090570000e-001) (16, -1.25921799979914170000e+000) (17, 1.04159817099380470000e+000) (18, 9.20699580056738580000e+000) (19, -1.30532521417404280000e+000) (0, 6.05058385028358180000e+000) (1, -4.21297140370759100000e-001) (2, 2.53377724001567280000e+000) (3, 1.36658165405474260000e-001) (4, -9.37413749173687090000e+000) (5, 6.24637787440173730000e-001) (6, 6.33055713187598370000e+000) (7, -5.29908288995769250000e-001) (8, 2.56707133636525950000e+001) (9, -1.27045869151314740000e-002) (10, -1.08243965917956140000e+001) (11, 2.86321807849643390000e-001) (12, 4.13118059045923670000e+000) (13, -1.71390016282950060000e-001) (14, -2.54975729938501240000e+000) (15, 1.44866457090052110000e-001) (16, -2.37509004376253200000e+000) (17, 1.36395762627254170000e+000) (18, 1.55838365853089480000e+001) (19, -3.39239958694233510000e-001) (20, -3.02438971172276920000e-002) (21, 1.53041163101736010000e+000) (22, 1.31890105135364700000e+000) (23, 2.24932224792607770000e+000) (24, 7.95290806576259500000e-001) (20, 3.03398401535192800000e+000) (21, -1.81765270958997190000e+000) (22, -2.09035830351376850000e+000) (23, -2.67193361200543670000e+000) (24, 1.49903906073949170000e+000) (20, -2.71518581708014480000e+000) (21, 1.77944916270430940000e+000) (22, -1.94115993689239400000e+000) (23, 1.48743649147726750000e+000) (24, 1.15137225712343110000e+000) 

FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 4 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (4, 5, 5.00000000000000000000e-001) (4, 5, 5.00000000000000000000e-001) (4, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -3.00282234755009980000e-001) (1, -1.88615586418683510000e-001) (2, 3.13511111685370290000e+000) (3, -2.43845016186366500000e-001) (4, 4.21312390138558530000e-001) (5, -1.68971197642073050000e-001) (6, -6.01657231159036470000e-001) (7, -2.87190193544298370000e-001) (8, -7.88992010145837530000e-001) (9, -1.43696747937450240000e-001) (10, -1.26589707658989290000e+000) (11, -9.67716358362318560000e-002) (12, 1.75148878810382210000e-002) (13, -3.00318855849305680000e-001) (14, -1.42163030834409230000e-001) (15, -9.00181095972919790000e-002) (16, 6.06721952791779980000e-001) (17, -9.06994904668217140000e-002) (18, -1.33039292716033100000e+000) (19, -2.90156502182249810000e-001) (0, -3.04540164286067760000e+000) (1, -1.88359238758613630000e-001) (2, -4.00802879501229280000e-001) (3, 2.24088715244125870000e-002) (4, 2.11756824916482730000e-001) (5, -3.10212711765782410000e-001) (6, 1.59423345539157050000e-001) (7, 1.41638013178385730000e-001) (8, 5.57606312487753280000e-001) (9, -1.70338594037569680000e-001) (10, 5.72010476267575400000e-001) (11, -3.15663094359202650000e-001) (12, 5.19544788483092380000e-001) (13, -6.90128463600936660000e-002) (14, -6.88192448096932030000e-001) (15, -6.59824647078829650000e-002) (16, 6.24873808531013710000e-001) (17, 1.01009243500433800000e-002) (18, 1.09524481579534070000e+000) (19, -3.03993172310307260000e-001) (0, -3.67935271499550850000e-001) (1, -9.62715939458170240000e-002) (2, -3.07557132104461580000e+000) (3, 4.18482775258401220000e-001) (4, -1.00765365500189800000e+000) (5, 3.27273228835189280000e-001) (6, 3.10177981752890290000e+000) (7, 9.82740496831785220000e-001) (8, 1.19603745163990680000e+000) (9, 6.13670036242508000000e-002) (10, 1.11453110276911760000e+000) (11, 5.71085600627458940000e-001) (12, 3.05707766842528890000e+000) (13, 3.18713372839754430000e-001) (14, 3.17641971121924890000e+000) (15, 4.06376757154119310000e-001) (16, -3.01997439638803210000e+000) (17, 4.03086559490050820000e-002) (18, 1.04465195133721210000e+000) (19, -9.76641691649158710000e-001) (20, -4.23534877211737600000e-001) (21, -2.26954596943593050000e-001) (22, 5.54307035924313850000e-001) (23, 4.57492284796597930000e-001) (20, -1.68436079515810770000e-001) (21, -3.34876961833011580000e-001) (22, 1.30620047570169370000e-001) (23, 5.29738309066509320000e-001) (20, -3.97709025649797400000e-002) (21, -7.91141648699900390000e-002) (22, -6.87702131504316470000e-001) (23, 6.03253401953964820000e-001) 

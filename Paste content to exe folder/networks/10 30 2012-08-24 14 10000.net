FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 5 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 4.13025608636545480000e+002) (1, -8.17137451522272680000e+000) (2, -8.66894845214782780000e+001) (3, 4.55819284214442320000e+001) (4, -9.81425597737565110000e+002) (5, 8.50942293321024400000e+000) (6, 2.72343964786882620000e+002) (7, -5.44547174951385050000e+000) (8, -7.43625083979903120000e-001) (9, 5.39267051105965760000e+000) (10, -1.50000000000000000000e+003) (11, 7.20313778286978130000e+001) (12, 1.58288238145879290000e+002) (13, 1.48023730181326560000e+002) (14, -1.50000000000000000000e+003) (15, 1.37023700670757190000e+002) (16, 1.50000000000000000000e+003) (17, 3.29373556156309920000e+002) (18, -1.49762059571921460000e+003) (19, 5.18344678264694720000e+000) (0, -1.50000000000000000000e+003) (1, -3.01361693035008170000e+000) (2, 2.25703757150317360000e+000) (3, 5.45641088317947690000e+001) (4, 5.65306629903754660000e+000) (5, 5.58059060766917060000e+001) (6, 1.41571550614153490000e+002) (7, 1.47576888041110550000e+001) (8, 1.31928845513963800000e+003) (9, -2.62260510068177020000e+001) (10, 1.50000000000000000000e+003) (11, -5.54630785863536160000e+000) (12, 2.25416503615437020000e+001) (13, -5.52463110909927120000e+001) (14, 5.17648249083297400000e+002) (15, -8.98411463551357950000e+000) (16, 9.72463695361136790000e+000) (17, -6.42359643832630670000e+001) (18, 9.85782656470199980000e+002) (19, -5.30468152948755360000e+001) (0, -3.94576243670373970000e+001) (1, -1.11165512222556080000e+000) (2, 4.26613329427316970000e+001) (3, -1.02533567501689090000e+000) (4, 1.44360274195773940000e+001) (5, -1.42314141517218120000e-002) (6, -3.65562104417965160000e+001) (7, -2.34803745750142650000e-001) (8, 3.85245699387295430000e+000) (9, 1.91479384247817100000e-001) (10, -1.34801434374679990000e+002) (11, -1.72363284432923010000e-001) (12, -2.77608966390879800000e+000) (13, -1.98099853535396970000e-001) (14, -1.78668313388401710000e+001) (15, -3.53125959971418580000e+000) (16, 1.31697714809320860000e+001) (17, -4.04204952989766110000e-001) (18, -4.38000149424332290000e+001) (19, 1.32168437630891370000e+000) (0, -9.77564286659266910000e+000) (1, 7.10469589170697140000e-001) (2, 5.13572501375326150000e+000) (3, -1.02960572178373220000e+000) (4, 5.58210847077342190000e+000) (5, 1.53744831674634060000e+000) (6, -5.24043278992446350000e+001) (7, -1.67830636486782290000e-001) (8, 3.00366482056653790000e+000) (9, -4.01117568682132940000e-001) (10, -1.36440295412619510000e+001) (11, 7.41062346473133490000e-002) (12, 8.31012291537394530000e+000) (13, 1.80611320636752160000e+000) (14, 3.06608844842284500000e+001) (15, 1.04037411036030370000e+000) (16, -1.86160496062761070000e+001) (17, 1.61846055262765160000e-001) (18, -1.28111167602299860000e+001) (19, 3.23960977525231540000e+000) (20, 7.88880993912716160000e-003) (21, 2.57874524739631640000e+000) (22, 1.13811939900887050000e-002) (23, 1.16466699077782240000e-003) (24, 2.55932189430805400000e+000) (20, 2.15107571260094850000e+000) (21, -2.58822102648168650000e+000) (22, -2.23632583859451420000e+000) (23, -3.55618665752234800000e+000) (24, 9.13424631768520960000e-001) (20, -2.80001776891815580000e+000) (21, 4.72558863898059620000e-002) (22, 3.88440128677469470000e+000) (23, 3.18503171854283900000e+000) (24, 3.48337301461316030000e+000) 

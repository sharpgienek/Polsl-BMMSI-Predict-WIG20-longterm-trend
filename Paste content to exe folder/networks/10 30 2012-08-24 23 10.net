FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 8 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (8, 5, 5.00000000000000000000e-001) (8, 5, 5.00000000000000000000e-001) (8, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 2.03293805885760960000e-001) (1, 1.03672898582352870000e-001) (2, 2.54342840501888210000e-001) (3, 8.08210711773212770000e-002) (4, -3.19394659915907710000e-001) (5, -2.76471873869209570000e-002) (6, 5.13158205449123940000e-001) (7, -3.86704320680875960000e-002) (8, 4.57992576706063800000e-001) (9, 2.30307749637605950000e-001) (10, -1.24234045787421370000e-002) (11, 3.01421165228637600000e-001) (12, 3.58531627484302340000e-001) (13, 1.05784715020071390000e-001) (14, 1.48054059659333460000e-001) (15, 2.07547185507703620000e-001) (16, 3.37771370005721160000e-001) (17, 1.35093797488060720000e-001) (18, 4.87788703587176740000e-001) (19, 1.54924120049180550000e-001) (0, 1.16488510742570070000e+000) (1, 4.04418326529535630000e-002) (2, 5.76720575190789180000e-002) (3, 6.26240213403786440000e-001) (4, -3.19515140095149790000e+000) (5, 4.39441689032903150000e-001) (6, 6.30473881025388590000e-001) (7, 9.33279397261795100000e-001) (8, 4.40396140896981480000e-001) (9, -6.20867099737639760000e-001) (10, 2.47639597907007850000e-002) (11, 3.46593216355288450000e-001) (12, -1.56506914371473770000e-001) (13, -1.35923879956487780000e-001) (14, 4.35751365437143930000e-001) (15, 1.13556381792627950000e+000) (16, -3.06091067629490520000e+000) (17, -6.47260680483822400000e-002) (18, 3.02834231643459880000e+000) (19, -1.18412893367428750000e-001) (0, 1.66116396978222980000e+000) (1, 7.73987047076219130000e-001) (2, -5.90977909218511450000e-001) (3, 6.94281129827104770000e-001) (4, 3.19086062940318540000e+000) (5, 1.74971200015443570000e+000) (6, -3.11201541338455010000e+000) (7, 1.13505073732356050000e+000) (8, -1.39180442109072340000e+000) (9, 4.48257064511391250000e-001) (10, 4.53092835676435430000e-001) (11, -2.45113018216982300000e-002) (12, 3.23566433940628400000e-001) (13, 9.46913593653154080000e-001) (14, 1.10633437002090610000e-002) (15, 1.02960734591883150000e+000) (16, -8.06710516269238730000e-001) (17, 1.04738877410677090000e+000) (18, 3.31753053343359250000e-001) (19, 1.13177409548810990000e+000) (0, -3.08999988791511800000e-001) (1, -2.79511388581095090000e-001) (2, -3.76681874565675480000e-001) (3, -6.13544343559337960000e-001) (4, 3.93970667561757750000e-001) (5, -3.36775166779551220000e-001) (6, -3.70794970333667150000e-001) (7, 1.40849148243684330000e-001) (8, 1.57838766378622210000e-001) (9, 6.20202135211946450000e-001) (10, 4.86652359695692820000e-001) (11, -1.71430010296970220000e-001) (12, 2.70879571685326680000e+000) (13, 1.16052109368879690000e+000) (14, -6.14987299570815270000e-002) (15, -1.91486484925671830000e-001) (16, 2.82032525953081150000e+000) (17, 1.94418281516186230000e-001) (18, -3.06024539308186670000e+000) (19, 1.72788221336189820000e-001) (0, 1.16916809927772410000e-001) (1, 2.07235048482940000000e-001) (2, 4.49249963917826010000e-002) (3, 4.04661624264029910000e-001) (4, 4.89861085961344760000e-001) (5, 5.69488765442514450000e-002) (6, 4.47002412919983720000e-002) (7, 2.55231795631406190000e-001) (8, -2.64452920440748810000e-001) (9, 3.69975687175205740000e-001) (10, 2.15614135349172620000e-001) (11, 2.34924675593088410000e-001) (12, -3.49478939686407310000e-002) (13, 1.07129986783892290000e-001) (14, 3.56464505229188880000e-001) (15, 3.69526515054561910000e-001) (16, 1.76754832340575240000e-001) (17, -1.88820164100919620000e-004) (18, -3.78609374143009940000e-002) (19, 5.47977972751236830000e-001) (0, -1.03502181947221540000e+000) (1, 2.67568414409424090000e+000) (2, 6.27636217353972900000e-001) (3, -6.11664460718825520000e-001) (4, 6.81209500872440030000e-001) (5, -9.73156584175351560000e-001) (6, -5.92993976013614250000e-001) (7, -2.56281078781192660000e-001) (8, -9.27573134645655050000e-001) (9, 1.51596015253084750000e-001) (10, -2.55360330623095380000e-001) (11, 1.20466098825725510000e-001) (12, 1.04103777112822260000e+000) (13, 1.03040968864734280000e+000) (14, 6.89705665664035680000e-001) (15, 8.72016317924669830000e-001) (16, -1.76592173597268200000e-001) (17, 6.41470239026236340000e-001) (18, -3.03537356653937260000e+000) (19, 6.77008180350967770000e-001) (0, -9.74896086154397160000e-001) (1, 1.05702748253521440000e-001) (2, 2.50691764085901050000e-001) (3, -1.77729182545554170000e+000) (4, 3.26007074120314040000e-001) (5, -3.90503889946438480000e-001) (6, -4.12710927836925200000e-001) (7, -3.26358298313793700000e-001) (8, 1.11431788006904860000e-002) (9, 1.39095079415921010000e+000) (10, -1.96619206197374580000e-001) (11, 4.32447060022425060000e-001) (12, 1.44338342118792350000e+000) (13, 5.39476565216298050000e-001) (14, 6.27440790790329270000e-002) (15, -1.02654403614276160000e+000) (16, 2.19260578011101440000e+000) (17, 1.66084829231409490000e-001) (18, -2.27556895551328080000e+000) (19, 2.60375702492114390000e-001) (20, 1.14836626117529890000e+000) (21, 4.56691587296723710000e-001) (22, -2.50445351872644770000e-001) (23, 1.13853562796556590000e-001) (24, -9.43399945989811530000e-002) (25, -1.12166531681636660000e-001) (26, 1.74195045091436740000e-001) (27, 5.48683829167524870000e-002) (20, -1.02296468325027770000e-001) (21, 4.32548587570203040000e-001) (22, 4.67705695758725390000e-001) (23, 2.48850717348007710000e-002) (24, 6.79442744690218760000e-002) (25, -4.23949343156483080000e-001) (26, -3.72106052194746960000e-001) (27, 4.62041633174324280000e-001) (20, -4.20211883606658120000e-001) (21, -5.03744599695142630000e-001) (22, 7.63745242190325690000e-001) (23, 9.06507812689887120000e-001) (24, 1.32338166557105340000e-001) (25, 9.76466730669452660000e-001) (26, 3.57564242814543240000e-001) (27, 6.22894410013359770000e-001) 

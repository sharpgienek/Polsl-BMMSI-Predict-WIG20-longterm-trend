FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 4 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (4, 5, 5.00000000000000000000e-001) (4, 5, 5.00000000000000000000e-001) (4, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 5.02417915004123250000e-001) (1, 1.63360578584344380000e-001) (2, -1.66950787920832070000e+000) (3, -4.20365025603414930000e-001) (4, 7.22661149702553020000e-001) (5, -3.00013691622387560000e-001) (6, -2.86140400400034410000e-001) (7, -2.36963780525211330000e-001) (8, 2.62653387312082540000e-001) (9, 3.27710706936652660000e-001) (10, -1.49403337815618430000e-001) (11, 3.61766302554127200000e-002) (12, 4.65446912280589150000e-001) (13, 3.86479625947634960000e-001) (14, -3.40376241051961400000e-001) (15, -4.70650391139527950000e-002) (16, 4.46049939335301740000e-001) (17, 2.66680868451548540000e-001) (18, -2.25447040120632880000e-001) (19, 4.72580131879334400000e-001) (0, 6.72243507200704250000e-001) (1, -1.90132286279419890000e-001) (2, 3.20229251433915870000e+000) (3, 4.68333875214233310000e-001) (4, -3.11374881184787980000e+000) (5, 4.63432752094325830000e-001) (6, 7.34772663535830150000e-001) (7, 1.00543115859608370000e-001) (8, -6.22886130649106700000e-001) (9, -1.52852925166898300000e+000) (10, 1.73598086542833840000e+000) (11, -8.24564320790917020000e-002) (12, 3.02053875758212520000e-003) (13, -4.06590090261852670000e-001) (14, 9.65543357436126960000e-001) (15, -3.78881259207126810000e-002) (16, -3.06548831308186730000e+000) (17, -4.76016503668874600000e-001) (18, 3.12404544286068650000e+000) (19, -2.37838890913760370000e-001) (0, 1.29298157370760950000e-001) (1, -3.51592397776023800000e-002) (2, -9.72344579433710440000e-001) (3, -1.10054902749110550000e-001) (4, -2.96411758553390190000e-001) (5, 3.23687816055474310000e-001) (6, 5.79070667248068100000e-001) (7, 3.87288510090662290000e-001) (8, 3.12114627289561050000e+000) (9, -1.09595321686453890000e+000) (10, 1.07708207679770910000e-001) (11, 3.97843271145156220000e-001) (12, 2.81414316250888020000e-002) (13, -4.12082000673013430000e-001) (14, 2.86362439324355400000e-001) (15, 4.27460348207766570000e-002) (16, -2.24021128897078550000e+000) (17, -3.95523162535642460000e-001) (18, 3.14702517953123630000e+000) (19, 4.62879433334689730000e-003) (20, -6.96815095083379300000e-002) (21, -1.29993071878144820000e-001) (22, 5.25874027858453270000e-001) (23, 4.74503567938274420000e-001) (20, -3.09021622022541800000e-001) (21, 4.20077975269364580000e-001) (22, 8.05949529970686750000e-002) (23, 8.42172994269640320000e-001) (20, 2.78434739166246610000e-001) (21, -6.65680744674856610000e-001) (22, -2.43995624381403320000e-001) (23, 9.60805195422655080000e-001) 

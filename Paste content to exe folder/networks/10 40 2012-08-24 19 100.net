FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 5 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 3.55838000035741050000e+001) (1, -5.39524696711869490000e-002) (2, 4.66732567967243920000e+000) (3, 1.54373134643760850000e-001) (4, -1.08082965656749040000e+000) (5, 1.23306960327392310000e-001) (6, -1.27637188566602550000e+001) (7, -2.49277067602422650000e+000) (8, 6.66128105640191040000e-002) (9, -6.57350067923570870000e-001) (10, 2.00257210522893070000e+000) (11, -5.34070541298914960000e-001) (12, 2.70545683768532900000e+001) (13, -4.59234179606637040000e-001) (14, 4.00815142064173740000e-001) (15, -3.96649475925987320000e-001) (16, -5.88772509244507080000e-001) (17, -9.45356556615157610000e-001) (18, 3.31349410075741200000e+001) (19, -1.54937719809755880000e+000) (0, -6.84050876885886350000e+000) (1, -1.04175407416360970000e-001) (2, 1.79449208930628460000e+000) (3, -5.19419420865421670000e-002) (4, -5.35752232362397680000e+000) (5, -1.19305305348512390000e-001) (6, -1.47739362607595570000e+001) (7, -6.60192751148312770000e-001) (8, -4.34114341543292570000e+001) (9, -6.62056539423861220000e-001) (10, -1.71113484774636150000e+001) (11, -3.20346105638247890000e-001) (12, 1.41702098148342710000e+001) (13, 1.75953221044445500000e-001) (14, 7.27239253016400510000e+000) (15, -6.20156212670866250000e-003) (16, -7.15358091089663790000e+000) (17, 4.67005647994360730000e-002) (18, -2.29058989215087830000e+001) (19, 2.21427471068017740000e+000) (0, -1.88622431488310300000e+001) (1, 8.56257249817198090000e-001) (2, -1.11050196163519460000e+001) (3, -1.09926270287685250000e+000) (4, 2.49124780863976380000e+001) (5, -6.67085006721197860000e-001) (6, -5.01473021026221700000e+001) (7, -2.41766645597617870000e+000) (8, -4.55704759102402970000e+001) (9, 2.38528896048947600000e+000) (10, -1.82780215079469470000e+000) (11, -1.65250329171894280000e-001) (12, 7.24044443506778320000e-001) (13, 7.70176061813144570000e-001) (14, 6.46340390406074490000e+000) (15, 4.19812592343140260000e+000) (16, 1.55207265305843350000e+001) (17, 1.16584970010205980000e+000) (18, -6.73695064791801310000e+001) (19, 8.92756177020881530000e-002) (0, 1.70367573820526100000e+001) (1, -2.95545535835449680000e-001) (2, 3.11802820199918640000e+001) (3, 2.02780755478386830000e+000) (4, -5.94846262528829470000e+001) (5, -8.66112856077797870000e-001) (6, 1.32213935614357800000e+001) (7, 3.02140120130868160000e-001) (8, -1.57728181502065560000e+000) (9, -7.39855907634340840000e-001) (10, 7.05501876634390790000e+000) (11, -1.67080822261040480000e-001) (12, -6.69052806200700800000e+000) (13, -3.66413587591119970000e-001) (14, 6.58737298320340690000e+000) (15, -4.56452001909316340000e-001) (16, -2.27958105822650340000e+001) (17, 5.40867960245292380000e-001) (18, 2.01651383466737250000e+001) (19, -2.02371044961989720000e+000) (20, -9.87101041171304710000e-001) (21, -1.99037087420921170000e+000) (22, -5.45153210460996990000e-002) (23, -2.34027728000519240000e-001) (24, 7.73208585580559380000e-001) (20, 8.54613477506552920000e-001) (21, 1.25326639595577730000e+000) (22, -1.35808260811906820000e+000) (23, 1.04641111870141450000e+000) (24, 1.73318920936785200000e+000) (20, -3.56725807304347430000e-002) (21, 8.25294918340968460000e-001) (22, 1.29021469448324470000e+000) (23, -1.70435979660764050000e+000) (24, 3.91558985438071770000e-001) 

FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 5 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 2.03688805455628770000e+000) (1, 3.20392100290477280000e-002) (2, 5.50309308741123830000e-001) (3, -8.94695352926757890000e-002) (4, 3.75972824698739760000e+000) (5, 7.38223881799089600000e-001) (6, -1.30220751286583030000e+001) (7, -5.52351619376639460000e-001) (8, -4.57222435178209920000e+001) (9, -1.21693492862828290000e+000) (10, -5.08277492649546550000e+001) (11, -2.40706475516835080000e-001) (12, 1.48588297998237660000e+001) (13, -1.66945139710389550000e-001) (14, -7.85631390367655770000e+000) (15, -3.30001837382998320000e-001) (16, 9.96917915041195200000e-001) (17, -3.58037502119226250000e-001) (18, -2.39308346473642960000e+000) (19, 2.39353025754544690000e+000) (0, 6.16847354382066020000e+000) (1, 2.05894613971218860000e-001) (2, 1.28425508181824630000e+001) (3, 1.05525054457416450000e+000) (4, -1.44069290681140140000e+001) (5, -1.29286063507449130000e+000) (6, 3.84327832075265260000e+001) (7, -2.36006298529921670000e-001) (8, -8.66291209006119090000e+000) (9, -1.03513450460236100000e+000) (10, 1.50080252916946130000e+001) (11, -4.26596584125714040000e-002) (12, -1.28677881222159980000e+001) (13, -1.77746681844515140000e-001) (14, 2.08548545864631160000e+001) (15, -2.76582289285453030000e-001) (16, -1.89382416504634850000e+001) (17, 5.25358188688456070000e-001) (18, -2.28846383167649770000e-001) (19, -3.06477644363324230000e+000) (0, -4.01288106475321560000e+001) (1, 1.65905425249729040000e+000) (2, -5.13372387730800880000e+001) (3, -1.60878211925457370000e+000) (4, 3.90993875541488340000e+001) (5, -6.61409459836537070000e-002) (6, 1.07678435221108570000e+001) (7, -1.00279525908209100000e+000) (8, 8.93946282983482020000e+000) (9, 2.99885952644531440000e+000) (10, -5.52222151243398760000e+001) (11, 3.25844507186606780000e+000) (12, -2.43852760077603890000e+001) (13, 2.05736781295346200000e+000) (14, 9.11394971178373510000e+001) (15, 2.15257396228706320000e+000) (16, 1.33466555803635250000e+001) (17, 1.61936128982645480000e-001) (18, -9.22017631724059750000e+001) (19, 7.98203196137243000000e-001) (0, 1.06784829501477370000e+001) (1, 1.65088173705246890000e+000) (2, -2.79959003304034620000e+001) (3, 8.81766733171770230000e-001) (4, -2.92366822436116460000e+001) (5, -1.38020306462926930000e+000) (6, 1.55780000510489150000e+002) (7, 1.20435421130759210000e-001) (8, 1.45019790058045120000e+002) (9, -6.85847978215122780000e-001) (10, 1.46723761553937760000e+001) (11, 7.84521917299602970000e-001) (12, 3.65980382049573760000e+001) (13, -5.83919584105097990000e-003) (14, -9.94625552152509410000e+000) (15, 3.58494469010056160000e-001) (16, 5.39900625751927170000e+000) (17, -1.54323204490897850000e+000) (18, 3.97941426480666680000e+001) (19, -2.17149625032351110000e+000) (20, -4.05759814677390730000e+000) (21, -2.71854339661663460000e+000) (22, 1.81058348214962690000e-001) (23, 1.08263847696406760000e+000) (24, 1.91853227088813290000e+000) (20, 1.36362990702195870000e-001) (21, 6.76811620414203840000e-001) (22, -2.13602990437072740000e+000) (23, 1.21287492183653490000e-001) (24, 2.59994469193405740000e+000) (20, 2.11519417909531620000e+000) (21, -8.06421627127196560000e-001) (22, 2.23780080292224380000e+000) (23, -4.51747967638851940000e-001) (24, -8.52284386995189820000e-001) 

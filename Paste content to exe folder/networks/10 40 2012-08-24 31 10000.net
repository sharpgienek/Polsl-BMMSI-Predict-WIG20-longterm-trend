FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 8 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (8, 5, 5.00000000000000000000e-001) (8, 5, 5.00000000000000000000e-001) (8, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 5.24781317448280760000e+000) (1, 3.05255912279210180000e+000) (2, 1.81881447059338760000e+001) (3, 3.13093801570515630000e+000) (4, 2.27569392092900920000e+001) (5, -1.39549460786620920000e+000) (6, 2.40468223563151610000e+001) (7, -4.29987999183489570000e-001) (8, 3.79600894679219490000e+000) (9, 1.75985995664592650000e+000) (10, 2.10781816067669010000e+001) (11, -2.21522436798931440000e-001) (12, -1.36150110866527600000e+001) (13, -1.33169896931241870000e-001) (14, -8.25701155061099000000e+000) (15, -8.35254506919090030000e-002) (16, 1.70753603147264850000e+001) (17, 1.17219283330604560000e+000) (18, 2.77271584286285030000e+001) (19, -3.97478652558277970000e-001) (0, 2.98452049703238880000e+001) (1, 6.08898882990512470000e+000) (2, 1.27242205211523240000e+001) (3, -3.28386940800215890000e+000) (4, 6.57549873900300240000e+001) (5, -2.89224404080794970000e-001) (6, 7.44482265037273660000e-001) (7, -2.11711830314920180000e+000) (8, -1.62048695012274810000e+001) (9, -1.18167052831185890000e+000) (10, -9.57055862702295460000e+000) (11, 4.40511008048144910000e-001) (12, 1.61650422033220620000e+000) (13, 3.07976756933023530000e+000) (14, 3.31711828949093930000e+001) (15, 3.01975860007771720000e+000) (16, -5.47786797413109260000e-002) (17, 1.98103390799913150000e+000) (18, -1.20306098668188140000e+002) (19, 1.10707367994753160000e+000) (0, 2.72836174823582520000e+001) (1, 8.46249125827028050000e-001) (2, 3.48300994683993750000e+001) (3, 3.18791761842767090000e-001) (4, -4.85242182755509170000e+000) (5, 2.33634408678148150000e+000) (6, 2.47852163210788310000e+001) (7, 5.53261297532284590000e-001) (8, -4.72687001164405630000e+000) (9, 1.21212358860278900000e+000) (10, 1.84153947619636040000e+001) (11, 1.40073577358283900000e+000) (12, -1.00515368127534860000e+001) (13, 1.16442932430265290000e-001) (14, -5.33957939486365160000e+001) (15, 1.07156870803974420000e+000) (16, 1.25616170790725650000e+001) (17, -7.77759454378463940000e-001) (18, -1.10624145351415190000e+001) (19, 1.42696201823496490000e+000) (0, -8.80137215847752150000e+000) (1, 5.91328463614364090000e-001) (2, 1.51903720682175060000e+001) (3, -2.36569270509150260000e+000) (4, 5.99344013673740720000e+001) (5, 3.76051352861535590000e+000) (6, -2.46322917375716910000e+001) (7, -1.17092720260133180000e+000) (8, -2.20797022966223400000e+001) (9, 1.69046281591368800000e+001) (10, -2.43452879274880410000e+001) (11, 6.51138609747470950000e-001) (12, 6.68280796753202560000e+000) (13, -1.02276261134645650000e+000) (14, -1.39381284987363980000e+002) (15, -1.82719531893377640000e+000) (16, 3.64662309480343080000e+001) (17, -6.77340612996349180000e-001) (18, 1.18418976836902080000e+002) (19, 9.36367076578316420000e-001) (0, 8.87979903292105450000e+000) (1, 1.46912364474923460000e+000) (2, 2.41338905921897020000e+000) (3, -5.62369752983874480000e-001) (4, -2.95318254272952420000e+001) (5, 9.54682668584538430000e-001) (6, -3.25332936908480830000e+001) (7, 4.47177996384466040000e-002) (8, 2.22309942060009750000e+001) (9, 1.48588726016130510000e+000) (10, 1.79036178938057410000e+001) (11, 3.97562151300591300000e-001) (12, 2.24175441575304730000e+000) (13, 1.23483779186352630000e+000) (14, -5.01435355890054170000e+001) (15, -6.62369062386515490000e-001) (16, 2.69411262786836510000e+001) (17, 1.57343221511928540000e+000) (18, 5.34318372905625550000e+001) (19, 2.22412272287010690000e-001) (0, -8.04556860260849400000e+000) (1, -1.10859507396531470000e-001) (2, -8.10831446502223610000e-001) (3, 1.95736113700513970000e-001) (4, 1.47507357954160810000e+000) (5, -8.10541664358647270000e-001) (6, 1.73923145200762050000e+001) (7, 1.97964398055829100000e-001) (8, 2.60004983497936490000e+001) (9, 6.13754009895722930000e-001) (10, 1.05374025244530160000e+001) (11, -1.16864284481290740000e-001) (12, -1.52392550197807960000e+001) (13, -2.24025742643067600000e-001) (14, 6.33527633516307630000e+000) (15, -1.48421983501814680000e-001) (16, 9.73561053408872470000e+000) (17, 3.50549005696199730000e-001) (18, 1.67096432920557090000e+001) (19, -8.78083297938676570000e-001) (0, -1.68151352055447490000e+001) (1, 7.45316413099848530000e-002) (2, 8.32438381147749480000e-001) (3, 2.35488876095486630000e-001) (4, 1.31433318531996580000e+001) (5, 1.66893379032505450000e+000) (6, -3.01498294146825870000e+000) (7, 2.21921483736822460000e+000) (8, -1.01301197206930810000e+001) (9, 6.51780735709986180000e-001) (10, -2.47026814671786670000e+000) (11, 3.26116258606715400000e+000) (12, 2.14264968821589260000e+001) (13, 2.16844774942209950000e+000) (14, 1.33719374163375090000e+001) (15, 2.65351058901214510000e+000) (16, 1.99421948362952750000e+001) (17, 7.78024632711633800000e-001) (18, -4.53451599786972320000e+001) (19, -4.45138293178562460000e-001) (20, -1.86526759127341710000e+000) (21, 2.71167056514696250000e-001) (22, 1.38595401314280590000e+000) (23, -6.96618142543807970000e-002) (24, 9.13358561167586960000e-001) (25, 5.06489466669045200000e+000) (26, 1.24102763467711630000e+000) (27, 1.17857897271180100000e+000) (20, 1.53834115305697620000e+000) (21, -2.73876502096273230000e+000) (22, 1.84364753975700620000e+000) (23, -3.02176634930891770000e+000) (24, 1.72783981373543320000e+000) (25, -4.49560020547665400000e+000) (26, -3.03211160089554490000e+000) (27, 8.93483776546688090000e-001) (20, -9.28945087460542380000e-001) (21, 2.71520946081666860000e+000) (22, -3.55293252217698140000e+000) (23, 4.22231719948577490000e+000) (24, -2.90022324496174160000e+000) (25, 2.94341163565021050000e-001) (26, 2.40577815351155790000e+000) (27, 4.50331798353803410000e+000) 

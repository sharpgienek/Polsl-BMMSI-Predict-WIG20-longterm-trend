FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 5 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 6.44140060589226680000e-003) (1, 1.57154227857677440000e-001) (2, 1.91900913705010590000e+000) (3, 2.87865632980785550000e-001) (4, 2.51447387911135410000e-001) (5, -1.04855331381447540000e+000) (6, -1.04655245175055420000e-001) (7, -4.91172338429921420000e-001) (8, -3.04471194558477530000e+000) (9, 8.41188600313659500000e-001) (10, -3.06877200453704810000e+000) (11, -5.90887342474389120000e-001) (12, 5.35106059394306190000e-002) (13, 2.16523153697179280000e-001) (14, -1.88031931145101750000e-002) (15, -1.97167469501176310000e-001) (16, 8.08526460823198350000e-001) (17, 2.85122406151275380000e-001) (18, -5.67890215873914080000e-001) (19, 3.11617767874211970000e-001) (0, 1.27501451313474990000e+000) (1, 3.48398690330249560000e-001) (2, 1.90405160206365000000e+000) (3, 2.11952907161489010000e-001) (4, 4.25921704297627930000e-001) (5, 1.52254262619068050000e-001) (6, -1.22026719309745470000e+000) (7, -7.41461386429802300000e-001) (8, -3.10000979797127750000e+000) (9, -3.68055807136839830000e-001) (10, -3.20168826628327970000e+000) (11, -1.78645270051852960000e-001) (12, 5.48338759218898960000e-001) (13, 2.83795947007055960000e-001) (14, 1.86585814604116940000e-001) (15, 2.32538778935830690000e-001) (16, 1.67007392435857160000e-001) (17, 1.45984794097489910000e-001) (18, -3.01678836118430650000e+000) (19, 3.71578060637808350000e-001) (0, -5.18376589777292510000e-001) (1, 4.67502050570755060000e-001) (2, 3.14064090209235310000e+000) (3, 2.69146064979517100000e-001) (4, -8.13760076921160370000e-001) (5, -2.21017421098657070000e-003) (6, 2.43576444944586380000e-001) (7, 2.91557930532777560000e-002) (8, -7.72689519668536500000e-001) (9, 9.46587576919536710000e-002) (10, -2.21601084915704580000e+000) (11, 1.30309392988814530000e-001) (12, 6.93831571373023540000e-001) (13, 6.11494902081652470000e-001) (14, -2.11463874056513290000e-001) (15, 5.43192533805569750000e-001) (16, 1.88471348224218580000e+000) (17, 6.05453392786418520000e-001) (18, -3.03766238493285370000e+000) (19, 3.78963115434283960000e-001) (0, -3.78745280377212560000e-001) (1, 1.36681901476112090000e-001) (2, 2.76532751058495780000e-001) (3, -3.03416507042761460000e+000) (4, 1.20611435608693940000e+000) (5, -5.81711813629311570000e-001) (6, 1.32522163173768220000e-001) (7, -2.37095811750470150000e-001) (8, -9.82177580403524870000e-001) (9, 7.45145851914732070000e-002) (10, -1.04393295241750160000e+000) (11, -6.82379879176741700000e-001) (12, 9.55318684224639950000e-001) (13, 2.13560987810453820000e-001) (14, -3.21865934875564250000e-001) (15, -5.96610495491944230000e-001) (16, 1.12298447203570760000e+000) (17, 2.33664100785790090000e-001) (18, -1.02052596964683540000e+000) (19, 1.97762878043640840000e-001) (20, -1.92311423330601190000e-001) (21, -1.19458902285040080000e+000) (22, 5.86942854370523960000e-001) (23, -1.18104153431940970000e-001) (24, 3.45616593241517110000e-001) (20, -3.69809315869516380000e-001) (21, 1.92998314953844040000e-001) (22, 4.52371435110916320000e-001) (23, -5.62681051626981120000e-001) (24, 3.95144871758174090000e-001) (20, 3.14768968223293810000e-001) (21, 7.31057032592187570000e-001) (22, 4.39431380786128310000e-001) (23, 4.86302736298946210000e-001) (24, 6.78472747947747150000e-001) 

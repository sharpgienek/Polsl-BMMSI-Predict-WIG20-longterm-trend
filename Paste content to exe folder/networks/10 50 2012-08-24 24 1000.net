FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 5 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -1.97106807784037730000e+001) (1, 1.40369973720237300000e+000) (2, -1.01465309136260840000e+000) (3, -1.48317758802938980000e+000) (4, 9.37009671691837910000e+000) (5, 3.45427252972269840000e-001) (6, 2.41090209208641770000e-001) (7, -1.85952109957746710000e+000) (8, -5.65154188083703880000e+000) (9, 8.29717671774414470000e-001) (10, -1.52766018684812670000e+001) (11, -4.33539146782831510000e-001) (12, 1.53523492128175110000e+000) (13, 3.86722779218903410000e+000) (14, 2.52008411246252670000e+001) (15, -9.70749661146059540000e-001) (16, -1.28222385332900310000e+001) (17, 2.68774406337643290000e+000) (18, 6.88717589079936410000e+000) (19, -1.88594273221823570000e-001) (0, 5.45097802705019700000e+001) (1, -7.81020023917852750000e-001) (2, 2.06600912857564580000e+001) (3, 1.87353379148453800000e+000) (4, -9.52259273437566660000e+001) (5, 1.58697870718511940000e+000) (6, -9.50548251414768120000e+001) (7, -4.16685633248675550000e-002) (8, -6.37714561404104290000e+001) (9, 7.01610870220069490000e-002) (10, -5.05028393729464900000e+001) (11, -5.61986592940158290000e-001) (12, -3.22213854942613910000e+001) (13, 4.37863516282446910000e-001) (14, -3.93300592137011480000e+001) (15, -6.72819682415609080000e-001) (16, -4.51003889859281910000e+001) (17, -5.24423431727568000000e-001) (18, -2.63383213143732780000e+001) (19, 2.02186062992492310000e+000) (0, -8.82547068159321670000e+000) (1, 1.21413298196688490000e+000) (2, 2.30873371435583760000e+001) (3, 2.33774110222282100000e-002) (4, -1.84112047489678050000e+001) (5, 1.51932588444921010000e+000) (6, 6.58364690952879440000e+000) (7, 8.10472149902882410000e-001) (8, -6.04527457135212940000e-001) (9, 1.28163165146499750000e+000) (10, 7.44469594961050230000e+001) (11, 7.21084329830675830000e-001) (12, 1.97803834285231370000e+001) (13, -8.98421910899809940000e-001) (14, 2.06774544290642840000e+001) (15, -1.60238792606213320000e+000) (16, 4.11487150902112650000e+000) (17, 7.67883091520162870000e-001) (18, 7.39018527375746430000e+001) (19, -1.01223263746972350000e+000) (0, 3.01900270756153760000e+001) (1, 1.27146485753254250000e+000) (2, 3.93872130908427880000e+000) (3, 7.30901543859083790000e-001) (4, -7.60211042271971140000e+000) (5, -1.75085850447241900000e-001) (6, 2.32803017669097900000e+001) (7, -1.70035770791329940000e+000) (8, 4.43716276179130700000e+001) (9, 2.73592570953581420000e-001) (10, 1.09742968036177790000e+001) (11, -4.38304095509956030000e-001) (12, 4.94702930727041730000e+001) (13, 5.96744646347461320000e+000) (14, 3.76632588183053510000e+001) (15, 6.77389360453121480000e-001) (16, -3.51903393613876190000e+000) (17, 2.11430448457161550000e+000) (18, -4.93562168448735590000e+000) (19, -3.53577842105761760000e-001) (20, 1.23474139946904360000e-002) (21, -2.46624676564734460000e+000) (22, -2.03303400390151680000e-001) (23, 8.67785630028210940000e-002) (24, 2.16005366217668370000e+000) (20, -2.47405140459514290000e+000) (21, 2.40641234626303820000e+000) (22, 3.00992630701474930000e+000) (23, 3.38159969413644970000e+000) (24, -3.33181831282592320000e-001) (20, 2.42029010012777900000e+000) (21, 3.97301160630465710000e-002) (22, -3.29636025195420320000e+000) (23, -3.38298510328899080000e+000) (24, 2.28755328036219470000e+000) 

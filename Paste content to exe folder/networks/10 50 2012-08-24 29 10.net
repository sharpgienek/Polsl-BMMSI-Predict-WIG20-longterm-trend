FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 6 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (6, 5, 5.00000000000000000000e-001) (6, 5, 5.00000000000000000000e-001) (6, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -5.17875142756196060000e-001) (1, 1.82427808708076040000e-001) (2, 9.80901552372743460000e-001) (3, -1.85616869894535540000e+000) (4, 5.87730960466594570000e-001) (5, -3.06770271961064270000e-001) (6, -2.95630383025630160000e-003) (7, -2.78351285326651270000e-001) (8, -1.29714635273990030000e+000) (9, 2.22336051470118310000e-001) (10, -3.08321902623670050000e+000) (11, -1.93757574964543850000e-001) (12, 4.44180272031974850000e-001) (13, 1.82434791307968520000e-001) (14, -3.20407459664621360000e-001) (15, -3.58227259051078270000e-001) (16, 3.19666507284905330000e+000) (17, 3.68062247687304320000e-002) (18, -3.07857425077686300000e+000) (19, -2.21443122252090670000e-001) (0, -4.84459352943761300000e-001) (1, 3.07150384973317310000e-001) (2, 1.20758931653307690000e+000) (3, 5.00505153861000810000e-001) (4, 7.12599882199568050000e-001) (5, -9.35705411742285610000e-001) (6, -6.20231101312668680000e-001) (7, 7.55015275671697130000e-002) (8, -1.01702255162588040000e+000) (9, 6.37359117889480410000e-001) (10, -2.23146495094982990000e+000) (11, 2.38806145086543960000e-001) (12, -4.48119020370998160000e-001) (13, 3.81010652813010280000e-002) (14, 5.86605078190217230000e-001) (15, 4.73778211232860240000e-001) (16, -5.54539920394293120000e-001) (17, -1.12517465611230660000e-001) (18, -9.91198350386047530000e-001) (19, 7.26815512860127020000e-001) (0, -1.27161727816482130000e-002) (1, 2.47484775487431140000e-001) (2, 1.16552637582543830000e-001) (3, -5.99378165974890080000e-002) (4, 9.20327112570653030000e-002) (5, 5.68393051654533470000e-001) (6, 4.71830960119059170000e-001) (7, 4.19760350640598300000e-001) (8, 5.92557768916169180000e-001) (9, 8.68310064005933800000e-003) (10, 3.08674075480480340000e+000) (11, 5.83534696586887950000e-001) (12, -3.25619862086042210000e-001) (13, 1.71091203004170080000e-001) (14, 3.91435451108572750000e-001) (15, 1.80613034842639960000e-001) (16, -3.85232900083720690000e-001) (17, 8.14280463221676160000e-002) (18, 5.11083010105701120000e-001) (19, 6.19582060405285520000e-001) (0, -4.32632803133784270000e-001) (1, -1.24851778679132220000e-001) (2, 7.49212146899970400000e-001) (3, -1.48664161721647680000e-001) (4, 7.97150594152833620000e-001) (5, -5.11436518472718200000e-001) (6, 1.71833932714915430000e-002) (7, -2.00985951833579450000e-001) (8, -4.29806786183421160000e-001) (9, -5.60007949972267800000e-002) (10, -3.04744632062552070000e+000) (11, -4.78722783746231060000e-001) (12, 9.13564677361374950000e-002) (13, 2.24289055613587310000e-001) (14, -3.13116824570119960000e+000) (15, -3.40172976994178090000e-001) (16, 3.10726687815754140000e+000) (17, 1.94226724925871290000e-001) (18, -3.16492679112611610000e+000) (19, 1.21887636921400670000e-001) (0, 1.03495346486456840000e+000) (1, 3.14540560532815430000e-001) (2, 3.33900912383807150000e-001) (3, 7.01692995012633410000e-002) (4, 4.80361453995296890000e-001) (5, 7.56464396238883400000e-002) (6, -9.69299162242871490000e-001) (7, -9.65640010854314390000e-001) (8, -9.76605070554863010000e-001) (9, -4.04869454322366150000e-001) (10, -1.07260726925103220000e+000) (11, -2.85292337495511190000e-001) (12, 1.05613876791462900000e+000) (13, 2.91559080337155270000e-001) (14, 3.86534557589078510000e-001) (15, 2.18248616321160080000e-001) (16, -1.49842790947166790000e-001) (17, 8.87747374387239940000e-002) (18, -9.18871915397696970000e-001) (19, 3.20329788229557880000e-001) (20, -2.23324996349344380000e-001) (21, 6.04882288891706730000e-002) (22, 4.26279873610672490000e-001) (23, 8.82729503777174710000e-002) (24, -1.01068036910897030000e+000) (25, 2.93687098197386310000e-001) (20, -4.28161342124427300000e-001) (21, -6.99526053223159250000e-002) (22, 4.10312108312308520000e-001) (23, -6.01985287957141350000e-001) (24, 5.19389894995108350000e-001) (25, 3.75674656624295280000e-001) (20, 6.15614633226306500000e-001) (21, 4.34069192177666540000e-001) (22, 2.75297205345230560000e-001) (23, 8.57711092358265590000e-001) (24, 9.28621008911735930000e-001) (25, 8.26430992860138320000e-001) 

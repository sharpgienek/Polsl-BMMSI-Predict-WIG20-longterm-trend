FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 8 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (8, 5, 5.00000000000000000000e-001) (8, 5, 5.00000000000000000000e-001) (8, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -3.16263186921691910000e+000) (1, 3.64199067887815860000e-001) (2, 2.01139913841152150000e-001) (3, 7.53520320371414700000e-002) (4, -3.13544558323115340000e-001) (5, 2.76148370255828330000e-001) (6, 1.40333825457557900000e+000) (7, 8.50948121943462700000e-001) (8, 3.12539431983391140000e+000) (9, 3.55230841288663170000e-001) (10, 3.13916385128909340000e+000) (11, 1.80628349301219100000e-001) (12, -3.08243777622505900000e+000) (13, -1.04566758607725810000e-001) (14, 4.41988591839438780000e-001) (15, 7.91857824284933120000e-001) (16, 8.34871905086093100000e-001) (17, 3.60563326652756020000e-002) (18, 2.70213067769145000000e+000) (19, -4.66495397384561030000e-001) (0, 3.09337569573783500000e-001) (1, 4.30644944428279160000e-001) (2, 1.68269849774566050000e-001) (3, 3.53825038546636330000e-001) (4, -2.19471121333768140000e-002) (5, 3.05419112484143940000e-001) (6, 3.05567646231183520000e-002) (7, 2.14207476717941550000e-001) (8, -4.11302159142993060000e-001) (9, 4.59449245943923330000e-001) (10, -4.47258090694810650000e-002) (11, 9.05162684487080540000e-002) (12, 1.99513060890497920000e-001) (13, 4.10792018656374160000e-001) (14, -3.69804476259411870000e-002) (15, 4.19751979727387980000e-001) (16, 3.48815109224544620000e-001) (17, 4.93186749457824310000e-001) (18, -2.57030407217766270000e-001) (19, 4.33771755326923910000e-001) (0, -4.48765861103011390000e-002) (1, 4.44361593611874370000e-002) (2, -2.18249367723460580000e-003) (3, -6.25134548905670610000e-002) (4, 4.52534638028888710000e-001) (5, -3.26451406155384010000e-002) (6, 3.83033229180235970000e-001) (7, 1.05178496043602090000e-001) (8, 2.46922385226489130000e-001) (9, -1.40679179639922660000e-001) (10, 4.13410426898516480000e-001) (11, 2.06685207873252450000e-001) (12, -1.09172348047338600000e+000) (13, -1.39757548766814290000e-001) (14, 2.90333032486395470000e-001) (15, -1.06192504992729520000e-001) (16, -5.70121862056873490000e-002) (17, 3.25573882567637210000e-003) (18, -1.14074474283612210000e-001) (19, -8.38841550509342030000e-002) (0, -2.93104417703931750000e-001) (1, 4.23450710251253360000e-001) (2, -1.84595180413951450000e-001) (3, 8.18401263113704150000e-002) (4, -1.97614810306717810000e+000) (5, 5.90744159116077140000e-001) (6, 1.37760877975631950000e+000) (7, 3.22294636531690600000e-001) (8, -3.08317538467777760000e-001) (9, -1.85755973703801420000e-001) (10, 3.09071414353588650000e+000) (11, 5.24313494063683110000e-001) (12, -1.07101114014957210000e+000) (13, -7.38217881369254560000e-002) (14, 3.20322024872798310000e+000) (15, 3.62468673881123580000e-001) (16, -1.37620993843647250000e+000) (17, -1.48091451968547280000e-001) (18, 3.07307498311679250000e+000) (19, 3.51158859259469210000e-001) (0, -4.77866247163374890000e-001) (1, 2.83571625280123360000e-001) (2, 1.69126973910997180000e-001) (3, -2.00225197637521890000e-001) (4, 1.26738923983302500000e-002) (5, -9.21928507924200810000e-002) (6, 6.00752843003927950000e-001) (7, 1.90091227551211050000e-001) (8, 3.09512698539851790000e+000) (9, -5.89087855482163250000e-002) (10, 3.14288699587582250000e+000) (11, -1.50530254005465100000e-001) (12, -3.13236453478152570000e+000) (13, 2.43981004587779870000e-002) (14, 2.24876845348604690000e-001) (15, -1.27104960687651040000e-001) (16, 8.74801104899834540000e-001) (17, 1.06368316523979360000e-001) (18, 2.81000421445514090000e+000) (19, -7.89543252045256800000e-002) (0, -1.11263229406713160000e+000) (1, -4.73498174293378320000e-001) (2, -5.49512530053190230000e-001) (3, 1.00669753009087910000e+000) (4, -9.44614150524585970000e-001) (5, 4.19821385797757800000e-001) (6, 3.14950320691191180000e+000) (7, 1.18424235349285170000e+000) (8, 1.76262881504417380000e+000) (9, 1.98886343406607930000e-001) (10, 3.11561038414124440000e+000) (11, 8.23360924455015740000e-001) (12, -3.45279197033237060000e-001) (13, -1.84109907874369010000e-001) (14, 1.28547628058430810000e+000) (15, 4.64062554842930740000e-001) (16, -5.20827634074227940000e-001) (17, -3.20956080183901620000e-002) (18, 3.19296634232518790000e+000) (19, -7.31499459580046030000e-001) (0, 1.92779819968107310000e+000) (1, 1.53286096158374250000e+000) (2, 8.77547137541388360000e-001) (3, 3.89336071302699320000e-001) (4, 3.09591433892587540000e+000) (5, -3.80726068847435540000e-001) (6, -1.53949116589482830000e+000) (7, 4.33366833677558740000e-001) (8, -4.13361567380617360000e-001) (9, 2.66627865963406930000e-002) (10, -3.11554324546836890000e+000) (11, 3.01226715519181950000e-002) (12, 6.45284326235949670000e-001) (13, 8.30831877228080940000e-001) (14, -9.83749893295253970000e-001) (15, 5.77702884145289590000e-001) (16, 5.98722319789278150000e-001) (17, 4.17478846231485960000e-001) (18, -3.01588504085834600000e+000) (19, 5.96285069499852940000e-001) (20, 2.50818112933192560000e-001) (21, 4.08306076907904100000e-001) (22, -1.23705302433867200000e-001) (23, 3.12639553112977140000e-001) (24, 3.41555024581643330000e-001) (25, 6.43894439603019950000e-001) (26, -9.07059234593203400000e-002) (27, 1.02838588235291430000e-001) (20, -2.54296243365118250000e-001) (21, 3.84783127338634840000e-001) (22, -3.37439902513566590000e-002) (23, 4.38386013208181710000e-001) (24, -1.07311259220826920000e-001) (25, 3.73561504025668680000e-001) (26, -1.19227652399952210000e-001) (27, 3.46990158025475730000e-001) (20, -3.27027840622300560000e-001) (21, 4.07276393603987260000e-001) (22, -1.00450907556099710000e-001) (23, -2.27877501649885430000e-001) (24, -1.55803691584045450000e-001) (25, -7.17139485676026480000e-001) (26, 4.41327907783269360000e-001) (27, 5.20927228386814490000e-001) 

FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 6 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (6, 5, 5.00000000000000000000e-001) (6, 5, 5.00000000000000000000e-001) (6, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 3.02104251163832330000e+000) (1, -8.22685342382862420000e-002) (2, -3.78476597244853720000e-001) (3, 2.72012864552268420000e-001) (4, -3.11200320635311820000e+000) (5, 6.31349336922796490000e-001) (6, -1.51505341802782360000e-001) (7, 8.52180464450501770000e-001) (8, 3.96769592649653720000e-001) (9, -3.16975790846047930000e-001) (10, 3.18259036560807380000e+000) (11, 7.06794413822887550000e-001) (12, 1.97718462832255530000e-001) (13, -7.58439544982660590000e-002) (14, 3.18051517026465100000e+000) (15, 2.18132003892665850000e-001) (16, -3.05489260979897900000e+000) (17, -1.95714059095413510000e-001) (18, 3.16880252360574400000e+000) (19, 1.77426063537867550000e-001) (0, -3.02870852737755580000e+000) (1, -1.14386199734428050000e-001) (2, -2.79212772167340200000e-001) (3, 1.51843502083355950000e-001) (4, 1.85288985825141750000e-001) (5, 6.98965064788591720000e-002) (6, 1.44115831443861040000e+000) (7, 5.43177601335248730000e-001) (8, 3.03453128137057160000e+000) (9, 2.10059423761540130000e-001) (10, 3.07615115503763100000e+000) (11, 1.78949215377887600000e-001) (12, 3.95030421865266990000e-001) (13, -3.27179846322660450000e-001) (14, 4.10355909221182810000e-003) (15, 9.35555157754966170000e-002) (16, 3.01694094907720520000e+000) (17, 5.77280947971358230000e-002) (18, 3.20402591280248840000e+000) (19, 4.16026062755973230000e-002) (0, 3.67860930768272700000e-001) (1, 1.27116287752334080000e-001) (2, 1.33519975642521220000e-001) (3, 6.25669175921447110000e-001) (4, -3.28163106104346510000e-001) (5, 1.30907276261736790000e-001) (6, 1.58388453238316460000e-001) (7, 1.73991993700624070000e-001) (8, -2.14765147313841180000e+000) (9, 2.44299432614498100000e-001) (10, 2.11872680215295590000e-001) (11, 1.87621543101872680000e-001) (12, -1.93650117923000830000e-001) (13, 1.08605424490015480000e-001) (14, 2.35817653610851440000e-001) (15, 1.18921076997892600000e-001) (16, 1.50956310640995520000e-001) (17, 3.55038064933274000000e-001) (18, -6.58170306500908410000e-002) (19, 1.10374344143484700000e-001) (0, 1.62590395949118970000e+000) (1, -1.78910884967295040000e-002) (2, 4.74174425151729140000e-001) (3, 1.61481583399655680000e+000) (4, -3.59628651655312660000e-001) (5, 4.04150490976112280000e-001) (6, 5.61299270549651120000e-002) (7, 7.34690618285695070000e-002) (8, -3.12797000346604200000e+000) (9, 5.02390810136658230000e-002) (10, 4.61395731546774930000e-001) (11, 8.40144707153721250000e-001) (12, 4.01109753118765790000e-001) (13, -4.18947435760962040000e-002) (14, 5.75011233714555870000e-001) (15, -1.22487128862248630000e-002) (16, -8.00274264369663090000e-001) (17, 1.38216967395440150000e-001) (18, 1.30779257514517870000e+000) (19, 1.06367011537537150000e-001) (0, -5.03105593455986930000e-001) (1, -2.27766364533814330000e-001) (2, 5.28843755020737880000e-001) (3, 1.39574813345239220000e-001) (4, 2.03203428307470730000e-001) (5, 5.89275130243977490000e-002) (6, 3.23193569269393690000e-001) (7, -1.32614545778903330000e-001) (8, 1.23363725491181300000e-001) (9, -7.66727585836102510000e-002) (10, 1.38961080789748000000e-001) (11, 1.13718773606476130000e-001) (12, 6.63848376096701110000e-001) (13, -7.51773972332026310000e-002) (14, 3.50522068358867510000e-001) (15, -1.45953765475352690000e-001) (16, -1.81167298734337350000e-001) (17, 7.31844379194611090000e-002) (18, -3.04095296584286800000e-001) (19, -1.34405913740776290000e-001) (20, -1.31911926906625120000e-001) (21, 8.53517702059877430000e-001) (22, -1.72531852837802450000e-001) (23, -1.54490151089043130000e-002) (24, -2.82490486443017270000e-001) (25, 4.26791600745372090000e-001) (20, 5.35936040282671170000e-001) (21, -5.01443574270229670000e-001) (22, -1.22818717331394020000e-001) (23, 5.27863516315335350000e-001) (24, -3.34098577162598110000e-001) (25, 6.70718349451578490000e-001) (20, -6.88269562873509380000e-001) (21, -2.77597873080169870000e-001) (22, -2.75022189448039190000e-001) (23, 6.38812454472960510000e-002) (24, -1.71248017879246150000e-001) (25, 9.86802840199816340000e-001) 

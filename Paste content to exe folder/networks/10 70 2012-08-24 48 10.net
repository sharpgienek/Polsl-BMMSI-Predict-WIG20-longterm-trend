FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 7 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (7, 5, 5.00000000000000000000e-001) (7, 5, 5.00000000000000000000e-001) (7, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 3.02104251163832330000e+000) (1, -4.70114782204119060000e-001) (2, -7.11537338271958090000e-002) (3, 8.49283687902190050000e-003) (4, 1.22048033328617980000e+000) (5, 1.22951938782830530000e-002) (6, -1.30646714555380910000e+000) (7, -3.87118797266070570000e-001) (8, -6.14185382579390930000e-001) (9, -1.90904771303718410000e-001) (10, -3.04749514875124830000e+000) (11, -4.94491004911820490000e-002) (12, 1.62993299504593040000e-001) (13, 4.00287647961667440000e-001) (14, 2.31011010832777340000e-001) (15, -1.79749656321667420000e-001) (16, -9.47784269344149610000e-001) (17, 6.02450342101020360000e-002) (18, -3.27212879687905020000e-001) (19, 5.86510306566864130000e-002) (0, -1.77393202092375500000e-001) (1, 3.93139001728286920000e-001) (2, 7.26641345782389260000e-001) (3, 2.21481886410384130000e-001) (4, 1.94263011770167800000e-001) (5, 2.31510583599009060000e-001) (6, 4.58598206852221860000e-001) (7, 1.51797065804736650000e-001) (8, 1.48387129806511250000e-001) (9, 2.34368880423700520000e-001) (10, 2.16886997846892110000e-001) (11, 3.03330653028586470000e-001) (12, 4.00704470386440360000e-001) (13, 1.39380198452350170000e-001) (14, -5.84691809148943090000e-002) (15, 4.36701769670510390000e-001) (16, 1.72074471456653950000e+000) (17, 2.59811936636085430000e-001) (18, 1.33392753265630550000e-001) (19, 2.43686448114546930000e-001) (0, -3.14335086307023470000e+000) (1, 4.47637293555999740000e-001) (2, 1.37534628708930740000e+000) (3, -3.69391927501465030000e-003) (4, 1.59691889384200310000e-003) (5, -1.33108104218724130000e-001) (6, -3.92772947792045360000e-001) (7, 1.42000923080637750000e-001) (8, 1.90907479549787640000e+000) (9, 5.94599626815854880000e-001) (10, -1.56871442067513090000e+000) (11, -8.94575137294094940000e-002) (12, 1.19167906169359520000e-001) (13, 7.72133750987399890000e-002) (14, 2.62084866313643030000e-001) (15, 5.11488147738039080000e-001) (16, 1.17159624199020260000e+000) (17, 3.07944261572062950000e-001) (18, -1.63980206821879440000e+000) (19, 6.03718402192212960000e-001) (0, 1.08014389522545830000e+000) (1, 8.23627990163286470000e-003) (2, 1.12340354767571770000e-001) (3, 3.10828616528210500000e+000) (4, -8.61105861897077050000e-001) (5, 6.75371813768413710000e-001) (6, 6.75219225875514970000e-001) (7, 2.30316145988310070000e-001) (8, -1.51983784528947120000e+000) (9, -3.37607166952166970000e-001) (10, 1.27650411248186240000e+000) (11, 1.89101231046104570000e+000) (12, 1.80591529561751780000e-001) (13, 3.71094375990237560000e-002) (14, 3.11708133142878820000e+000) (15, -7.44567261728166090000e-002) (16, -3.03426883019478580000e+000) (17, -7.76450141895415610000e-003) (18, 3.06402957282575540000e+000) (19, -2.28964782174748140000e-003) (0, 1.63216176844913300000e-001) (1, 5.66787320970054460000e-001) (2, 1.45760118577737830000e+000) (3, -6.82651795856480670000e-002) (4, 1.33625718982858690000e+000) (5, -8.05432121833611790000e-001) (6, -1.21747178289954980000e+000) (7, -9.25365829873816550000e-001) (8, -7.73550115384485370000e-001) (9, 3.23896860452968500000e-001) (10, -3.11513430991540030000e+000) (11, -2.07191611380052210000e-001) (12, -6.29075013015957810000e-001) (13, 1.37441029976458680000e-001) (14, -1.02820029275267900000e-002) (15, 2.54615853561226070000e-001) (16, -3.79119376509345960000e-001) (17, 7.23740931562432690000e-002) (18, -3.11186892900736730000e+000) (19, 7.46077789913549780000e-001) (0, 4.83749364418475270000e-001) (1, 1.50875919244537500000e-001) (2, 1.33470137396730750000e-001) (3, -5.73472353325371750000e-002) (4, -2.95024252748254670000e-001) (5, 1.47481506860581870000e-001) (6, 1.89552634038695660000e-001) (7, -4.11366354657217270000e-001) (8, -1.05364974943729320000e+000) (9, -9.48655297352957030000e-002) (10, -9.95641936072909520000e-001) (11, 6.30881950561485440000e-002) (12, -3.09511132108262920000e-001) (13, 4.89327692266231660000e-002) (14, 3.87437877915039030000e-001) (15, -5.07133032027129230000e-001) (16, -2.45423847058935500000e-001) (17, 1.83784242167041920000e-001) (18, -1.04077133127663980000e+000) (19, 2.99878139487303300000e-001) (20, -3.12401222426139820000e-001) (21, 2.89804612864261120000e-001) (22, 3.04599534959722640000e-001) (23, -9.42343944233248190000e-003) (24, -6.17456867003139840000e-001) (25, -2.95568672031120530000e-001) (26, 1.87460861339220440000e-001) (20, 1.73315388544669490000e-001) (21, 2.06790907389744320000e-001) (22, 2.76330941027747020000e-001) (23, 4.52788216879669470000e-001) (24, -1.29464063053089720000e-001) (25, -2.10395770365101490000e-001) (26, 6.38940552830590920000e-001) (20, 6.89214711858092240000e-001) (21, 5.09303081420330320000e-002) (22, 3.09778150210227480000e-001) (23, -3.61060197016726980000e-001) (24, 4.50826019895173850000e-001) (25, 2.03213940975817830000e-001) (26, 7.12463608659448470000e-001) 

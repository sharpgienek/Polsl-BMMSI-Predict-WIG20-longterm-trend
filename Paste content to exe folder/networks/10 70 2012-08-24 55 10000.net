FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 8 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (8, 5, 5.00000000000000000000e-001) (8, 5, 5.00000000000000000000e-001) (8, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 5.25411302787943480000e+001) (1, 7.49085947827817230000e-002) (2, 4.24224457853411040000e+001) (3, -1.85115947214611780000e-001) (4, -1.25229213168182910000e+001) (5, 2.25563619626496650000e+000) (6, -5.37774684341710550000e+001) (7, -1.58616969552866440000e+000) (8, -7.39386773568501620000e+000) (9, 5.45561009773814830000e+000) (10, 1.01801064894279120000e+001) (11, 5.70625687874089860000e-002) (12, -2.94974039908036370000e+000) (13, 6.96980515023379540000e-001) (14, 2.03697690226133720000e+001) (15, 4.87196758725873470000e+000) (16, -2.52830658322553920000e+001) (17, 3.05793833621891720000e+000) (18, -4.45099888804130050000e+001) (19, -4.90536640873802720000e-001) (0, 3.23594292509897530000e+000) (1, -1.30317005595010780000e+000) (2, 5.06239714085132350000e+001) (3, -6.75254694674115030000e-001) (4, 5.90521744513055960000e+001) (5, 1.83314675196841770000e+000) (6, 1.56830259717827770000e+002) (7, 1.21639595857626310000e+000) (8, 7.51232685598361060000e+001) (9, -2.37633420504325730000e-001) (10, -3.16069297299382110000e+000) (11, 1.22198317776078350000e+000) (12, 6.65369942989427590000e+001) (13, 4.32623729973603590000e+000) (14, 1.04794566636046400000e+002) (15, 3.05694795623077460000e+000) (16, 6.06799174581837800000e+001) (17, -9.94812576909955060000e-001) (18, 7.89280507116211960000e+001) (19, 1.73182748390260690000e+000) (0, 2.71964539534134670000e+001) (1, -5.52470049245387600000e-002) (2, 5.79323614272899690000e+000) (3, -5.50774687892901760000e+000) (4, 4.24170044448738110000e+001) (5, -1.82365433827978560000e+000) (6, 1.81635937366754340000e+001) (7, 1.03167443441748060000e+000) (8, 1.96147921446449200000e+001) (9, 2.13728331989610650000e+000) (10, 2.00758886304923010000e+001) (11, -2.66435630105903920000e+000) (12, 3.23116011078239670000e-001) (13, 1.19243956628522140000e+000) (14, -3.05475029090051890000e+001) (15, 1.07027175758425790000e+001) (16, -4.05095887361614330000e+001) (17, 1.32876517183671620000e+000) (18, -1.79624405070168080000e+001) (19, -3.30149026267196620000e+000) (0, 6.75792427047237250000e+000) (1, 2.71225917643268370000e-001) (2, -2.55743726289812480000e+000) (3, 2.76989691703942440000e-001) (4, 2.13138028772701780000e+000) (5, -1.21552133340336360000e-001) (6, 1.62711247996035200000e+001) (7, -1.21785085715875010000e-001) (8, -4.05476147064278610000e+000) (9, -7.15065601351511930000e-002) (10, -9.77236931725199650000e+000) (11, 5.68897628561232940000e-001) (12, -1.05083624775894130000e+001) (13, 3.14973120187631180000e-001) (14, -1.03787448865791030000e+001) (15, -7.62370298886862810000e-001) (16, 4.88351377539354380000e+000) (17, 2.30543009772072850000e-003) (18, -1.00467716468438200000e+000) (19, -5.76413470432373790000e-001) (0, -1.06073201607464070000e+002) (1, 3.78661257557864510000e+000) (2, -8.40669092446493590000e+000) (3, -3.11915464911967400000e+000) (4, 2.31020444757101440000e+002) (5, -5.20645707888555260000e+000) (6, -3.87562217682486220000e+001) (7, 5.13493318218607440000e-001) (8, 2.44573673293954980000e+002) (9, 7.63279567058625650000e+000) (10, 2.20032175045552910000e+001) (11, 1.20696683784147260000e+000) (12, -6.17417896453898010000e+001) (13, -5.03679111127269690000e+000) (14, 2.75741592169768010000e+002) (15, -6.21446538633583320000e+000) (16, -4.33014217370065960000e+000) (17, 1.92468346364489800000e+001) (18, -1.25164836120699870000e+002) (19, -1.40189754789036920000e+000) (0, 9.34141376646559110000e+001) (1, 6.55898983671366340000e-001) (2, -1.89412905417787950000e+001) (3, 2.07968164738222860000e+000) (4, -7.67881429131119830000e+001) (5, 1.15994029711200450000e+000) (6, 2.50009653512918250000e+001) (7, -7.44331871567853410000e-001) (8, 9.91081467731780210000e+001) (9, 7.27060563618674880000e-001) (10, 1.38026216156852600000e+002) (11, 4.41546732390663440000e+000) (12, -2.83347116759059180000e+001) (13, 2.79891923847441990000e+000) (14, -3.51752271574004140000e+001) (15, 9.55616356530260340000e-001) (16, 2.06953206616870170000e+001) (17, 1.05869433811662650000e+001) (18, 2.67073120516026530000e+001) (19, 5.17297459849774600000e-001) (0, 1.97288017427603890000e+001) (1, 6.07888938060213050000e-001) (2, 2.91972717265581640000e+001) (3, -7.01438156634554110000e-001) (4, 3.05465016430048380000e+001) (5, -4.42627946431682020000e-001) (6, 1.75204269912265360000e+001) (7, -7.12582951474737270000e-001) (8, -9.50376139351216590000e+000) (9, 2.20368141255817380000e-001) (10, 5.70405405811450810000e+000) (11, 1.92085378135595760000e-001) (12, 5.45679668396469790000e-002) (13, -3.86311333396826660000e-001) (14, -3.46183223012535350000e+001) (15, 3.09703792068255430000e-001) (16, -8.75206230295389890000e+000) (17, 2.02262899816205770000e-001) (18, -6.77237140916033910000e+000) (19, 6.38559393623458070000e-002) (20, -3.44611270070446160000e-002) (21, 3.24708252664208490000e+000) (22, -1.28493430339431980000e-003) (23, -6.33140808372303480000e-004) (24, 3.18633157690186990000e+000) (25, -6.18709849418545570000e-002) (26, 8.63186651294086950000e-002) (27, 2.17655460273356300000e-004) (20, -2.96162754650321200000e+000) (21, -1.99731191673869860000e+000) (22, -2.86199957384688600000e+000) (23, -5.99437078850474240000e+000) (24, -3.00903559991914940000e+000) (25, 2.81677995530083350000e+000) (26, 2.91036306431638090000e+000) (27, 2.38167871841073660000e+000) (20, 3.44463705690261610000e+000) (21, -1.47185540197938010000e+000) (22, 3.20602792560696330000e+000) (23, 7.02702775620374090000e+000) (24, 1.39238702122978620000e-001) (25, -2.37206468030825060000e+000) (26, -4.24813644301872010000e+000) (27, 2.82453830394933590000e+000) 

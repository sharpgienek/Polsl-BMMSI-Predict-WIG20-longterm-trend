FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 4 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (4, 5, 5.00000000000000000000e-001) (4, 5, 5.00000000000000000000e-001) (4, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 2.17225073836916830000e+002) (1, -2.98773515368417850000e+000) (2, 4.56201475813368230000e+001) (3, 3.60921871365719720000e+001) (4, -3.70053625783943800000e+002) (5, 3.43183105865080690000e+001) (6, 1.11118200265133200000e+002) (7, 1.63969546753958840000e+002) (8, 1.81354149480031400000e+002) (9, 5.44180417293850520000e+001) (10, 6.78441355057301170000e+002) (11, -3.66213800638008140000e-002) (12, 6.32599912562872530000e+002) (13, 1.28382430482350760000e+000) (14, 8.28825805416783080000e+001) (15, 4.72613143967965000000e+001) (16, 3.54585825533255390000e+002) (17, 8.90355558027730810000e-001) (18, 6.32538107276451030000e+001) (19, 3.38306727247752330000e+001) (0, 1.04345259963821440000e+001) (1, -3.20240460409928980000e-001) (2, -1.87115134463241830000e+001) (3, -2.76331534823475220000e-001) (4, -3.22313458014355310000e+001) (5, 3.01844725641037090000e-001) (6, -5.35311098784672480000e+001) (7, -3.09178888366501650000e-001) (8, -5.68853104739444650000e+001) (9, 8.08653530270323630000e-002) (10, -4.49223732115590050000e+001) (11, -5.67674908335184860000e-001) (12, -4.32532193939094180000e+001) (13, 7.89825062698507750000e-002) (14, -2.01886025424251300000e+001) (15, -6.12805876132615280000e-001) (16, -3.79056832611966290000e+001) (17, -3.39172711105828030000e-001) (18, -2.91468721122816130000e+001) (19, 3.81085247322559040000e+000) (0, -1.06138283604430300000e+002) (1, -2.23460418264637360000e+000) (2, 1.21945803479939570000e+002) (3, -1.75703212728286150000e+000) (4, 8.89845635262443580000e+001) (5, -2.14411205413148580000e+000) (6, 1.21485006327664820000e+002) (7, 1.01136673522592590000e+000) (8, 1.53760324494011570000e+002) (9, 4.71370976159115780000e+000) (10, -7.20066061375767960000e+001) (11, -3.37945996081180010000e+000) (12, 5.03562256534563700000e+001) (13, 3.72899629837347650000e-001) (14, 2.30453072416097410000e+001) (15, 3.69407385600977100000e+000) (16, 2.28584893265383510000e+001) (17, 3.26464062333584960000e+000) (18, -9.80546662329608630000e-001) (19, -9.09832427705464130000e-001) (20, -1.97355585659499680000e-001) (21, -2.38158634112629960000e+000) (22, 9.41877067083152220000e-002) (23, 2.33596543701914470000e+000) (20, 1.68815210433577810000e+000) (21, 2.01264692986842330000e-001) (22, -1.47899331977800030000e+000) (23, -3.50614523982721860000e-002) (20, -8.32221841794388300000e-001) (21, 1.84883558061325170000e+000) (22, 9.42623253248751180000e-001) (23, 2.34446666435880050000e-001) 

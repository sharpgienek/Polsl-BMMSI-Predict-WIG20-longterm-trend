FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 5 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 1.13895422043317910000e+001) (1, -7.68952355259082680000e-001) (2, 9.07819188984756660000e-001) (3, -2.23685896136855700000e+000) (4, -1.58649695790264320000e+001) (5, -6.48875343596728050000e-001) (6, -6.74657086042291710000e+000) (7, -1.36155152054987760000e+000) (8, -2.88759982200680130000e+001) (9, -1.09147014330162430000e+000) (10, 1.13407982523783200000e+001) (11, -3.06880341870518940000e-001) (12, -5.91219549022294720000e+000) (13, 3.40061880215848660000e-001) (14, -1.50790604646082810000e+001) (15, 2.90523939494542030000e-001) (16, -1.66959466065546280000e+001) (17, -1.52343228325679230000e-001) (18, -3.10934458836737090000e+001) (19, 1.52110448808052130000e+000) (0, -4.55667839056586740000e+000) (1, -1.52721807132932970000e+000) (2, -2.12493204137648770000e+001) (3, -9.63385256282353810000e+000) (4, 4.94242017934217510000e+000) (5, -4.37397811495262970000e-001) (6, 7.29471030941138420000e+001) (7, -1.05827217385161210000e+000) (8, 1.53513917500018420000e+001) (9, -2.93792121155864240000e+000) (10, 8.32597803295367100000e+001) (11, -6.41988247309290740000e+000) (12, 2.76745304999755650000e+000) (13, 7.01223145381574820000e-001) (14, -9.62609498768009940000e-001) (15, 5.03708575298419530000e+000) (16, -1.47714983045767670000e+001) (17, 2.45639486680527540000e-002) (18, -5.33586232301082290000e+001) (19, 1.07829449111977340000e+000) (0, 5.96666577647013770000e+001) (1, -3.12756686563033570000e+000) (2, 9.77576543580102140000e+001) (3, -7.65878896717732020000e-001) (4, -3.33691679311329490000e+001) (5, -3.53792864573471720000e+000) (6, 1.09315418895148450000e+002) (7, -4.87895718468409350000e-001) (8, 7.46764673376768400000e+001) (9, -4.53268828782733290000e+000) (10, 9.15869953738943540000e+001) (11, 7.50866570287330240000e+000) (12, 5.95488550459739730000e+001) (13, -9.48430677003084700000e-001) (14, -9.77825388291862790000e+001) (15, -8.76589089506330940000e+000) (16, -1.13040283384154260000e+002) (17, -5.80853159551466100000e-001) (18, 7.42213106360850220000e+001) (19, -1.31552058575658750000e+000) (0, -8.63874519401943890000e+000) (1, 5.21927052224219760000e-001) (2, -1.90352010640188030000e+001) (3, 7.01658351583978050000e-001) (4, 7.14620516549442360000e+000) (5, 9.33403376358306610000e-001) (6, 3.65982873927734430000e+000) (7, 8.40234889862171210000e-001) (8, 3.06078614131890880000e+001) (9, 4.97094210751252920000e-002) (10, 1.84579342412896170000e+001) (11, -1.29870481402480830000e+000) (12, 1.44143452046789470000e+001) (13, -7.32843883967650770000e-001) (14, 1.58493805776294000000e+001) (15, 9.98999114547557080000e-001) (16, 1.92838381708574150000e+001) (17, 1.25779699102104290000e-001) (18, 1.23911445761917080000e+001) (19, -7.52698534450980780000e-001) (20, -1.91930800724617860000e+000) (21, 1.19242376483886380000e+000) (22, 5.75182529171220920000e-001) (23, -8.68988940076028000000e-001) (24, 5.70256815565804100000e-001) (20, 3.47692189682344210000e+000) (21, -1.66843027488788920000e+000) (22, 1.62463108159707390000e+000) (23, 3.94490394805222700000e+000) (24, 2.52653573021871570000e+000) (20, -3.81410046480848700000e-001) (21, 1.03415677334860480000e-001) (22, -2.22454594170367990000e+000) (23, -2.59217960587980700000e+000) (24, -6.24876459637654300000e-002) 

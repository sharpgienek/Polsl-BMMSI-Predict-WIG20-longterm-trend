FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 5 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -1.24609258255150690000e+001) (1, -3.07848355506549880000e-001) (2, -1.43951348534392290000e+001) (3, -1.78850429046486780000e+000) (4, 2.69355426624646910000e+001) (5, -7.10204765405175190000e-001) (6, 8.30039808195513370000e+000) (7, -7.15366479036343760000e-001) (8, -1.33248763739230220000e+001) (9, -7.86883862047877410000e-002) (10, -2.03232841627221990000e+001) (11, -1.56140563407932080000e+000) (12, -1.42542303863428260000e+001) (13, -4.28751170298906690000e-001) (14, -3.63957080743736730000e+001) (15, 9.67443572769045580000e-001) (16, -2.95721258324057810000e+001) (17, 1.75907056760642310000e-001) (18, -3.67754501773823820000e+001) (19, 1.31405305211058400000e+000) (0, 5.31507798650951460000e+001) (1, -9.12316633924586640000e+000) (2, -6.11717141256018810000e+001) (3, -2.94437220699512000000e+001) (4, 7.80542853619146970000e+002) (5, -8.13385557867825780000e+000) (6, 4.32744403179054470000e+002) (7, -1.42789076066724490000e+001) (8, -8.03754582843334900000e+001) (9, -8.41942068004370010000e+000) (10, 2.14665327392618930000e+002) (11, -3.85295120234060990000e+001) (12, 3.40646882235923210000e+002) (13, -1.19385452033072530000e+001) (14, -4.51159679679039070000e+002) (15, 2.75620784850918880000e+001) (16, -1.75270963162043960000e+002) (17, 1.70174967257588750000e+001) (18, -5.45106479639991790000e+002) (19, 2.15340158366016790000e+001) (0, 1.50000000000000000000e+003) (1, -3.44778439092909110000e+001) (2, 1.30311624781713910000e+003) (3, 4.78529561972219920000e+001) (4, -6.55883666496671140000e+002) (5, 4.26049475555533450000e+001) (6, -1.16235417044954990000e+003) (7, 5.96256453721656500000e+001) (8, -1.24256152480464990000e+003) (9, -6.96083395461220110000e+001) (10, 1.50000000000000000000e+003) (11, 3.60641794337832340000e+000) (12, 4.76233324200330800000e+002) (13, -3.18082592562533260000e+001) (14, -5.19852572758217550000e+002) (15, -1.03830995496759140000e+002) (16, -1.50000000000000000000e+003) (17, -4.16236803840957850000e+001) (18, -8.85231886635316130000e+001) (19, -2.51646863317553700000e+001) (0, 7.77148246911455520000e+001) (1, -3.90739778106154790000e+000) (2, -7.03467678762015540000e+001) (3, 1.77511696815366670000e+000) (4, -2.50152963721311660000e+002) (5, 2.44225502043009040000e+000) (6, -3.46794058032615740000e+002) (7, -1.23716065157580580000e+000) (8, -3.68506900843088940000e+002) (9, 1.14997674945616120000e+000) (10, -2.46529521555559710000e+002) (11, -3.58239421094691620000e+000) (12, -2.33954739384023980000e+002) (13, -1.61843191142398490000e+000) (14, -4.48737604238032120000e+001) (15, -8.11267615668517190000e+000) (16, -1.38890119864970050000e+002) (17, -8.78342611952838580000e-001) (18, -2.16098193680436250000e+002) (19, 1.35550128541598140000e+001) (20, -3.15955045338955640000e+000) (21, 2.38224111055207780000e+000) (22, -5.76899360273890330000e-001) (23, -4.91008041750925660000e-001) (24, 4.19983108960468750000e-001) (20, 3.13649857573797950000e+000) (21, -2.21975001205937650000e+000) (22, 2.74895029855286530000e+000) (23, -7.80659144179928100000e-001) (24, 2.92495073403860230000e+000) (20, 1.19780592002319340000e-001) (21, -2.45709706899225920000e-002) (22, -1.75579878837337990000e+000) (23, 1.73594383103362750000e+000) (24, 2.67173436321610880000e-002) 

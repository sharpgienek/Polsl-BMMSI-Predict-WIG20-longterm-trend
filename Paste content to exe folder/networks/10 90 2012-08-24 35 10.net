FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 4 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (4, 5, 5.00000000000000000000e-001) (4, 5, 5.00000000000000000000e-001) (4, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -5.49415037047828010000e-001) (1, 1.20884030046358450000e-001) (2, -3.18713138130074030000e+000) (3, 5.33384184050038960000e-001) (4, 2.41888258812855780000e-001) (5, 4.72726660023607800000e-001) (6, -4.47798327119914850000e-002) (7, 3.84036473155146050000e-001) (8, 1.20080338728729530000e-001) (9, 1.77542966437517380000e-001) (10, 2.39600183555367500000e-001) (11, 5.60850004771811680000e-001) (12, 8.40329749103319590000e-002) (13, 3.55771091874584060000e-001) (14, 5.29822637171561080000e-001) (15, 4.57193212526516400000e-001) (16, 6.54672167944744810000e-002) (17, 1.48150950536169560000e-001) (18, 1.47050863243349330000e+000) (19, 6.17954497810234930000e-001) (0, -9.33520353115975700000e-001) (1, -1.41411848639489880000e-001) (2, -1.04218371407983230000e-001) (3, -2.33665294079850310000e+000) (4, 1.31807515802319420000e+000) (5, 6.21445504418348670000e-002) (6, -1.54018974143532010000e+000) (7, -1.85288246244478150000e-001) (8, 2.03576939269621880000e+000) (9, 5.49840334901980250000e-001) (10, -4.15341832169188520000e-001) (11, -2.10359561473191900000e+000) (12, 3.13953616574776630000e+000) (13, 6.53340572250782490000e-002) (14, -1.22015367008932380000e+000) (15, 7.17800148504551090000e-001) (16, 1.52380944777698950000e+000) (17, 1.87164210900363280000e-001) (18, -3.96042226416667200000e-001) (19, 2.23000388682656260000e-001) (0, -4.79570623157226160000e-002) (1, -3.78555388845302930000e-001) (2, 1.19587265671557620000e+000) (3, 1.35834367299700890000e-002) (4, 3.08954226851842420000e+000) (5, -3.79995818554267010000e-001) (6, 5.65555649635060780000e-001) (7, -3.86331267867422560000e-001) (8, -6.50808708870975750000e-003) (9, 2.11610170316055530000e-001) (10, -3.12153079438571530000e+000) (11, -3.88587551427930640000e-002) (12, -5.37716281930737810000e-002) (13, 2.42803947880490870000e-001) (14, -2.95895740549368290000e-001) (15, -4.68069550335417920000e-001) (16, 1.70781283215006300000e-001) (17, 1.18110413063425570000e-001) (18, -3.19842288537524680000e+000) (19, -5.87457137247682230000e-003) (20, 3.73647149514001410000e-001) (21, 1.46736582108267790000e-001) (22, -8.84649077107499540000e-002) (23, 3.06856377034366360000e-001) (20, 4.92386084144681050000e-001) (21, -5.90146872348753740000e-001) (22, -8.85747709936370450000e-002) (23, 4.89993505984028850000e-001) (20, 3.98581151106253640000e-001) (21, 3.73987891187790020000e-001) (22, 8.78582678344593800000e-002) (23, 4.69278173644096350000e-001) 

FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 5 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 2.31548843129281900000e+000) (1, -5.84340210759844440000e-001) (2, 8.96552056594547400000e-001) (3, -2.51723186547555770000e+000) (4, -7.29460436636266340000e+000) (5, -1.29610189351626960000e+000) (6, 1.12576522038387330000e+001) (7, 2.07621427020008080000e-001) (8, 8.01747409201477980000e+000) (9, -5.06636945999147880000e-001) (10, -1.47717153826537690000e+001) (11, -7.63216272625986570000e-001) (12, -1.45548904971182920000e+001) (13, -1.30268829493949530000e+000) (14, -2.85948771499619770000e+001) (15, 6.55955723018315730000e-001) (16, -4.91786046690589780000e-001) (17, -9.30284771920077310000e-003) (18, -1.52837773307672080000e+000) (19, -7.22003586538581320000e-001) (0, -5.44456244833447940000e+001) (1, 1.46414952457803170000e-001) (2, -1.89021014217161540000e+001) (3, -8.64949475234022210000e-001) (4, 2.42450583102900590000e+001) (5, -9.69370325834551410000e-001) (6, 1.70443481611376310000e+001) (7, 2.27393569782831370000e-001) (8, 1.26069270924611970000e+001) (9, 8.27832543896490080000e-001) (10, -6.11303460265516700000e+001) (11, -2.49463027231658700000e+000) (12, -1.89761976408043330000e+001) (13, 9.74367485074843500000e-002) (14, -2.06674369276418870000e+001) (15, 2.84101489388566500000e+000) (16, 1.62186230331495020000e+001) (17, 1.71820306469440110000e+000) (18, -1.07659628921261700000e+001) (19, 1.66147842712701580000e-001) (0, -1.11869763022632360000e+001) (1, 3.50005089048118570000e-002) (2, 9.06188651578667150000e+000) (3, 1.02444682725183880000e+000) (4, 5.03125742000649370000e+001) (5, 1.07357865166989930000e+000) (6, -3.86664986879371430000e+001) (7, -1.47990524896389440000e+000) (8, -2.91678577451744160000e+001) (9, 1.51096405982627210000e+000) (10, -6.47063226402370080000e+000) (11, -2.52427167179254090000e+000) (12, -1.18067978743061670000e+001) (13, 3.25910556520754510000e-001) (14, -4.93851506170867170000e+000) (15, 1.72542987052187180000e+000) (16, -6.96347614227387560000e-001) (17, -1.41458858997955800000e-002) (18, -1.29649761498963520000e+000) (19, 9.65759505747616710000e-001) (0, 3.25634836059378690000e+000) (1, -1.84496455143036280000e-001) (2, 3.07529122091552590000e+000) (3, 7.37431514834634010000e-002) (4, -1.08370151369775180000e+001) (5, 1.16827097208534510000e-001) (6, -1.39149625475190340000e+001) (7, -2.21142714476522260000e-001) (8, -2.38289067779851500000e+001) (9, 2.91658723910260930000e-001) (10, -1.29824221486586090000e+001) (11, 1.21085120147080930000e+000) (12, -1.09683566738162920000e+001) (13, 5.37829539478360760000e-001) (14, 1.30259906408749790000e-001) (15, -9.64713888010639950000e-001) (16, -1.21839626963320240000e+001) (17, -2.27411861110387780000e-001) (18, -1.39117001277839410000e+001) (19, 4.97173259582151810000e-001) (20, -1.54288932487229950000e+000) (21, 7.92107774486486840000e-001) (22, -9.85274845117617980000e-001) (23, -1.39108472354969130000e+000) (24, 2.51468594520620310000e-001) (20, 7.94087411072874840000e-001) (21, -1.98743892464170300000e+000) (22, 7.11657969560348440000e-001) (23, -5.80686922166637980000e-001) (24, 1.75084821024500380000e+000) (20, 8.88688040334305040000e-001) (21, 8.61389478152322700000e-001) (22, 4.51082760552063370000e-001) (23, 1.83030695980169390000e+000) (24, 5.29189502081221290000e-001) 

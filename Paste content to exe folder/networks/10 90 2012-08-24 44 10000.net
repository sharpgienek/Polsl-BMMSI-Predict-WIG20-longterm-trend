FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 5 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -1.50000000000000000000e+003) (1, -4.45068661219702690000e+001) (2, -1.15438711559043140000e+003) (3, 3.24142009444377810000e+001) (4, 2.85605167819029530000e+002) (5, 1.03008818666452140000e+002) (6, 4.03939504471633700000e+001) (7, 5.93828028376904060000e+002) (8, 4.59434302239254750000e+002) (9, 1.32217232073201670000e+001) (10, -1.46476197524286790000e+003) (11, 1.61650795644662480000e+002) (12, -1.05550805720775110000e+003) (13, 2.34069792517406090000e+002) (14, 1.80015454006828040000e+002) (15, 6.24735838226378970000e+000) (16, -1.90521553606895880000e+002) (17, 1.17764818270004450000e+001) (18, -6.02804029598557580000e+002) (19, 5.74448744134922860000e+001) (0, -8.19788701296837700000e+001) (1, 3.60835205888692380000e+000) (2, -1.17963190820356300000e+002) (3, 3.15559030619491710000e+000) (4, 4.45994147593804600000e+000) (5, -6.03030494527133750000e-001) (6, -5.70290796724753960000e+000) (7, -6.55119802370650820000e-001) (8, -1.66092091964752410000e+000) (9, 2.05614519053884150000e+000) (10, -1.12065881792503530000e+002) (11, 6.59033979906641780000e-002) (12, -5.53401289198631470000e+001) (13, 7.79586754803262890000e-001) (14, -7.16658602275164030000e+000) (15, 5.85480489614708950000e+000) (16, -9.92302747897630160000e+000) (17, 2.58242047731774040000e+000) (18, -6.02413526113567240000e+001) (19, -1.26251878577706320000e+000) (0, -9.01838125138417130000e+002) (1, -5.17177232374108510000e+001) (2, 1.50000000000000000000e+003) (3, -3.03911231391671950000e+002) (4, 1.02170852840445640000e+003) (5, -4.56052144310605740000e+001) (6, 1.28645606837797710000e+003) (7, -1.42640738479443460000e+002) (8, -1.50000000000000000000e+003) (9, 6.42372923543876480000e+001) (10, -1.49966608924909060000e+003) (11, -2.29619242170715160000e+001) (12, -3.07996272011683400000e+002) (13, -8.64544572142330200000e+000) (14, -1.50000000000000000000e+003) (15, -7.08452124650754630000e+001) (16, -1.50000000000000000000e+003) (17, -1.59562774551547280000e+001) (18, -5.06571694635126330000e+002) (19, -4.40970404175623760000e+001) (0, -1.15346065693877900000e+002) (1, 3.93479075453056960000e+000) (2, -1.03613337956223570000e+002) (3, 4.21140878035081290000e+000) (4, 6.54355016146922710000e+001) (5, -1.85488459544009670000e-001) (6, -5.36730716044790700000e+001) (7, -6.95186660835532870000e+000) (8, 2.32147720691629220000e+001) (9, 5.68187272232868600000e+000) (10, -2.96554066336981920000e+002) (11, -3.29720336689621530000e+000) (12, -1.64007593630973100000e+002) (13, -6.22657809243141620000e+000) (14, -7.75365053714437740000e+000) (15, 9.77115668342057120000e+000) (16, 7.36687686676117720000e+001) (17, 1.99270796246130240000e+000) (18, -9.76282699329698290000e+001) (19, 3.62442510195442000000e+000) (20, -1.63760177314377890000e+000) (21, 1.94163195281935200000e+000) (22, -1.54923196292739390000e-002) (23, -1.83731160069922720000e+000) (24, 1.55246235482563310000e+000) (20, 7.61414355954285190000e-001) (21, -1.72843839915744390000e+000) (22, -8.76445977624790820000e-001) (23, 9.04138363445170620000e-001) (24, 7.37520392115263660000e-001) (20, 3.95667526391443480000e-001) (21, 6.24828658883095560000e-002) (22, 1.74928464314734900000e+000) (23, 3.58131281971144740000e-001) (24, 1.66875387193851910000e+000) 

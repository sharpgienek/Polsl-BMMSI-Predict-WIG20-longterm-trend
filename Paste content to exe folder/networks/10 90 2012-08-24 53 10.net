FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 6 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (6, 5, 5.00000000000000000000e-001) (6, 5, 5.00000000000000000000e-001) (6, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 3.20350711396663270000e+000) (1, 2.45869262753134140000e-001) (2, -3.18713138130074030000e+000) (3, 3.21876932943008560000e-001) (4, -3.06532351815753670000e+000) (5, -4.80410464768522840000e-004) (6, 1.14997942314220490000e+000) (7, 1.13777253387765430000e-001) (8, -4.06144617858057170000e-002) (9, -1.03634847693724090000e-001) (10, 2.69858453506048330000e+000) (11, 6.98122332370915480000e-001) (12, -1.18576158521398200000e+000) (13, -6.26929252901215210000e-002) (14, 1.15938921055830370000e+000) (15, -1.60138579618599240000e-002) (16, -5.96222442361275420000e-001) (17, -3.00380272597202510000e-001) (18, 3.18331058046255590000e+000) (19, 2.69313172847767070000e-001) (0, -3.04062869357080510000e+000) (1, -5.18323023876554880000e-002) (2, -1.02808212210318080000e+000) (3, -3.35678145998389190000e-001) (4, 3.17703006279084390000e+000) (5, 1.38121284686652130000e-001) (6, -1.75637507709698280000e+000) (7, 9.39013423620699980000e-002) (8, 3.21391971177804250000e+000) (9, 4.21323395203388460000e-002) (10, 5.09803470978358320000e-001) (11, -8.22409043549595650000e-001) (12, 3.13953616574776630000e+000) (13, -3.17402596338248240000e-001) (14, 1.68094376787638890000e-001) (15, -9.30616079898168300000e-002) (16, 1.21170854727542250000e+000) (17, 2.97645656854287570000e-001) (18, 1.50469017445108100000e-001) (19, -2.92579654862404390000e-001) (0, 8.45594198390324570000e-001) (1, -5.18995214947978320000e-002) (2, -7.29115463533892560000e-002) (3, 3.27031153273873940000e-001) (4, -1.27343491849909050000e+000) (5, 1.36947447449920320000e-001) (6, 1.35706139841725020000e+000) (7, 4.62719856562249340000e-001) (8, -1.77292103824263610000e-001) (9, -3.05978659338238420000e-001) (10, 3.10855471997360680000e+000) (11, 5.83175260667378570000e-001) (12, -2.22689404381714500000e-001) (13, -1.56318809213059820000e-001) (14, 2.07810587730130880000e+000) (15, -5.00229528746613750000e-001) (16, -7.66415390371475210000e-001) (17, -7.25556010591881520000e-002) (18, 3.03166262898407530000e+000) (19, -5.89381845665714500000e-001) (0, -1.40846089050482530000e+000) (1, -1.63904366137087860000e-002) (2, -3.09076907517733000000e+000) (3, 3.93671172516401550000e-001) (4, -1.24004502780760320000e+000) (5, 2.59459393865635770000e-001) (6, -3.61625765200039580000e-002) (7, 3.42546654122705130000e-001) (8, 1.34201890866444320000e+000) (9, -7.48736489695561030000e-002) (10, 3.15524661520062020000e+000) (11, 1.41418911075808030000e-001) (12, 1.28641588049214350000e+000) (13, -4.81707677122771850000e-002) (14, 1.42843248417085580000e+000) (15, 4.61684665525269040000e-001) (16, 2.20903906528126110000e-002) (17, -1.02760612275729230000e-001) (18, 3.01853396667906800000e+000) (19, 1.36008800652635260000e-001) (0, -1.63689341373751020000e-001) (1, -6.70298761819200510000e-003) (2, 6.06667439928009760000e-002) (3, 1.30123013465512780000e-002) (4, 2.21876453973526950000e-001) (5, 1.42881847107440510000e-001) (6, 3.07034837730386480000e-001) (7, -9.45047397811427100000e-004) (8, 1.90547403879139400000e+000) (9, 5.51720170411936770000e-002) (10, 1.16743714968884980000e+000) (11, -5.15671423507965910000e-001) (12, -4.60417841333408410000e-002) (13, -1.44099401113339740000e+000) (14, 8.36827252027505960000e-001) (15, -4.95032678299381910000e-002) (16, -5.12764412883400800000e-001) (17, -1.43736852279812340000e+000) (18, 7.44217366334764870000e-001) (19, -5.79787118960241930000e-001) (20, 1.53985039026275690000e-001) (21, 4.60439145946316690000e-001) (22, -5.57166468287747520000e-002) (23, 7.42993288756097490000e-001) (24, -1.44567119150982470000e-001) (25, 3.68236250955287170000e-001) (20, 6.05086882087637170000e-001) (21, -1.83509971727614710000e-001) (22, 1.22172618283223970000e-001) (23, 3.81694747385540270000e-001) (24, -1.98079072966560020000e-001) (25, 5.18275285684636320000e-001) (20, -4.91878158612819440000e-002) (21, -2.74735269061717640000e-001) (22, -6.83444740947980910000e-001) (23, -4.50100800067662680000e-001) (24, -1.00379666392317570000e-001) (25, 5.29372098123552350000e-001) 

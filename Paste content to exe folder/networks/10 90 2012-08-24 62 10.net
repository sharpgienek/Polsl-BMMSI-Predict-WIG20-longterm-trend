FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=20 7 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (20, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (7, 5, 5.00000000000000000000e-001) (7, 5, 5.00000000000000000000e-001) (7, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 3.20350711396663270000e+000) (1, -2.23745961938355030000e-001) (2, -4.62240868697976650000e-001) (3, 2.44103049140156130000e-001) (4, -3.06532351815753670000e+000) (5, 2.58802612555231070000e-001) (6, 1.14997942314220490000e+000) (7, 7.19420413247117900000e-003) (8, -3.06022708253471890000e+000) (9, -2.94820798013931020000e-001) (10, 1.76357308434535700000e+000) (11, 1.13833388283878810000e+000) (12, -3.09627444635311640000e+000) (13, -1.80888156467264280000e-001) (14, 3.13459842153356320000e+000) (15, -7.60173776413576640000e-001) (16, -7.48507675260879470000e-001) (17, -8.82591991128375160000e-001) (18, 1.21208266323460560000e+000) (19, 1.78891273875395750000e-002) (0, -9.33520353115975700000e-001) (1, -2.58896700926500690000e-001) (2, -1.02808212210318080000e+000) (3, 3.51799449649610070000e-001) (4, -7.89853273897832000000e-002) (5, 3.86807355589320940000e-001) (6, -3.70043174922339010000e-001) (7, 5.90765113227707860000e-001) (8, 1.22681137787972410000e+000) (9, 1.85825119127345980000e-001) (10, 1.16076523431743480000e+000) (11, 1.68063793967664830000e-001) (12, 1.00193417791472040000e+000) (13, -1.52762609200932680000e-001) (14, 1.21571518630812790000e+000) (15, 6.02278016998329060000e-001) (16, 3.44284622615287420000e-001) (17, -9.90096400456163820000e-002) (18, 1.11286484297866340000e+000) (19, -1.27939667725088830000e-001) (0, 3.15534427145207540000e+000) (1, -2.28691192906660090000e-002) (2, -3.13034412679754400000e-001) (3, 3.32526173042032360000e-001) (4, -3.14054324584089790000e+000) (5, 1.85386739778286570000e-001) (6, 5.82496885542491020000e-001) (7, 4.72452647256922540000e-001) (8, 4.67124818307403270000e-001) (9, -2.99897097182252380000e-001) (10, 3.10855471997360680000e+000) (11, 4.21918264217828300000e-001) (12, 3.03532286572809620000e-001) (13, -1.21215068021759810000e-001) (14, 2.53794757107928600000e+000) (15, -1.12383280780780930000e-001) (16, -1.06844531683466690000e-001) (17, -1.73332588367769820000e-001) (18, 3.03166262898407530000e+000) (19, -1.30931865041551350000e-001) (0, 5.49022352979085300000e-001) (1, 4.51609600400776520000e-001) (2, 1.84261099033507010000e-001) (3, 2.47628731527833710000e-001) (4, 3.71582025971308740000e-001) (5, 6.17238438952707360000e-001) (6, 1.25094419929546450000e-001) (7, 8.86465903610683230000e-001) (8, -2.68958297323025230000e-001) (9, 2.81290208440511610000e-001) (10, -4.07968785184321560000e-001) (11, 6.68758527136606550000e-001) (12, -8.23220811139816930000e-001) (13, 3.71445267551262560000e-001) (14, 1.61428648986593650000e-001) (15, 3.89454260617470740000e-001) (16, 2.79758225995808300000e-001) (17, 4.17632410296392140000e-001) (18, -2.66505587658069590000e-001) (19, 1.18913514805905020000e+000) (0, 6.43361599917837370000e-001) (1, 8.14250301745111930000e-002) (2, 1.23116719774378970000e+000) (3, 5.31691628558864790000e-003) (4, -2.39375251425550220000e-001) (5, -3.80094028909580330000e-001) (6, 3.07034837730386480000e-001) (7, -1.82209921554671030000e-001) (8, -4.33945990759815490000e-001) (9, -9.07738367255341020000e-003) (10, 7.37704184897380880000e-001) (11, -2.69037370003844810000e-001) (12, -2.15631557829554510000e-001) (13, -1.21385966461806400000e-001) (14, 1.78781690461601640000e-001) (15, 2.46745120269510150000e-001) (16, 1.06942564190312740000e-001) (17, -1.51836653482331580000e-001) (18, 7.28540973989845740000e-001) (19, -6.22395227475074230000e-002) (0, 3.04703738507255210000e+000) (1, 4.34311777547954300000e-001) (2, 3.03442752160340050000e+000) (3, 6.43266164497492630000e-001) (4, -1.33217515374938820000e+000) (5, -2.37530018930750150000e-001) (6, 5.65811394202585750000e-001) (7, -5.66300513377543390000e-001) (8, -3.06767337170817720000e+000) (9, 2.31427856362821180000e-001) (10, -3.59902874594116960000e-001) (11, 2.14727064866702840000e-001) (12, -1.34153184334193880000e+000) (13, 4.78175962616865850000e-001) (14, 2.59723374651141400000e-002) (15, 4.34810483455044860000e-001) (16, -3.08257436192782560000e-002) (17, -2.27432069442812940000e-002) (18, -4.29135053360131800000e-001) (19, 5.11849058821759020000e-001) (20, -1.67347468590019640000e-001) (21, 6.54060949267030510000e-001) (22, 6.53654151332649510000e-002) (23, 3.06236738312672450000e-001) (24, -1.47252666066000240000e-001) (25, -1.51600945034756910000e-001) (26, 2.60307782550152740000e-001) (20, 6.14846403717440370000e-001) (21, 6.03078004859063100000e-002) (22, 1.64053160744451920000e-001) (23, 4.04497167979074030000e-001) (24, 4.35089701251000440000e-001) (25, 4.37497234760174800000e-002) (26, 6.31566262592531680000e-001) (20, 1.65220737311379840000e-001) (21, -1.22255269942933180000e-001) (22, -9.85146199088825610000e-001) (23, 4.58350613115052430000e-001) (24, -1.49419414145162300000e-001) (25, -1.88962502090961740000e-002) (26, 3.73829127480584170000e-001) 

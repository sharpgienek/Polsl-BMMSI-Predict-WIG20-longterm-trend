FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=22 5 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 2.13616756540238310000e+001) (1, 2.40259554900219690000e-002) (2, -2.73129156962690670000e+000) (3, 2.60064801911370820000e-001) (4, 1.34986521492662490000e-001) (5, 4.84498222882550290000e-001) (6, 3.78520675261293600000e+000) (7, 4.36685406499515930000e-001) (8, -1.64486497444611660000e+000) (9, 5.84614337711698020000e-001) (10, -1.46579051503527320000e+000) (11, 2.97100592712290500000e-001) (12, 4.10855724405788600000e-001) (13, -3.26682672509098380000e-001) (14, -1.25035869033513800000e+000) (15, 6.03761289340755900000e-001) (16, 2.41077830229139960000e+000) (17, -7.23422690034784880000e-001) (18, -2.13326783680984650000e-001) (19, 4.35683607896412420000e-001) (20, 4.13079854952118450000e+000) (21, 6.80610075389386670000e-001) (0, 6.76429666698502960000e+001) (1, -6.96260218026272960000e+000) (2, -3.01306760794161850000e+002) (3, -1.45974057343078990000e+003) (4, 2.88581878266989020000e+000) (5, 6.87954111002207810000e+002) (6, 4.92383595801574260000e-001) (7, -8.90200158943882690000e+000) (8, -1.33908531371271500000e+001) (9, 2.16406701789756330000e+001) (10, 3.43852012139967630000e-001) (11, -1.12326721512052700000e+000) (12, 4.93905357838454680000e+000) (13, 4.32319952970531660000e-001) (14, -9.01901530544407250000e-001) (15, -3.60771206686498200000e-001) (16, 8.90155481122686790000e+000) (17, 1.13284621268667050000e-001) (18, -5.16252569602169920000e-001) (19, 2.70719519022320970000e+001) (20, -1.48444799691009510000e+003) (21, -7.48250613038404570000e-001) (0, 6.62103996229035820000e+000) (1, -5.33082072295185850000e+000) (2, 1.76144296028587390000e+000) (3, -3.78231832718017640000e+002) (4, 3.29994605854294240000e-001) (5, 1.44818977973629280000e+000) (6, 1.66944510001534540000e+000) (7, -2.67404401763155720000e-001) (8, 2.15888695110805880000e+000) (9, -9.11350914377974060000e+000) (10, 3.71569631028030380000e-002) (11, -6.51315090955784660000e-001) (12, 1.28225657541886460000e+000) (13, 2.85972668674580650000e-001) (14, -5.12366519879102870000e-001) (15, 2.83429423247229690000e-001) (16, 9.91336426291910120000e+000) (17, -1.16023053517002840000e-001) (18, -9.55335851425204050000e-001) (19, 4.03379139725795920000e-001) (20, 1.61955140272141220000e+002) (21, -9.51307021679661300000e-001) (0, -7.63666934672417030000e-002) (1, -9.27496133790309480000e-001) (2, 4.88965725071590860000e-001) (3, -8.47388553113946560000e-001) (4, 3.20352299315691090000e-001) (5, 6.50972915049153000000e-001) (6, 4.01811913744628440000e-001) (7, 6.70560685085814230000e-002) (8, 3.83914291272031600000e+001) (9, -1.01562957965506770000e-001) (10, 1.46551115639406500000e+000) (11, -9.68884083150630570000e-001) (12, 5.18887477207110700000e-001) (13, 2.93573585687998180000e-001) (14, -1.04093473943368150000e-001) (15, -1.03705184530803330000e+000) (16, 2.57042038820189500000e-001) (17, 1.37448934715801170000e+000) (18, 6.97732865493463920000e-001) (19, 3.75016959633645790000e-001) (20, 7.31707454762624550000e-001) (21, -6.92641421694982110000e-001) (22, -2.10669059179534470000e+000) (23, 3.28192087655756030000e-001) (24, 3.39283407791138090000e-002) (25, 5.89155764711397080000e-001) (26, 1.76638364517503390000e+000) (22, 1.74672416062438840000e+000) (23, 1.14827782569916280000e+000) (24, 1.13833666771476440000e+000) (25, -2.74094567383485810000e-001) (26, 1.31835622474819210000e+000) (22, 2.46090290546892190000e-001) (23, -1.08869336552805350000e+000) (24, -1.71556945975976730000e+000) (25, -2.00506468854136520000e+000) (26, 2.35687067419457600000e+000) 

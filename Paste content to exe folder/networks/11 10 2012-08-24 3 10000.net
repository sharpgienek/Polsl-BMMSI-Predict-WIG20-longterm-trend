FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=22 5 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 5.99411871095373190000e-001) (1, 3.60235217109431850000e-001) (2, 2.48244522203700470000e-001) (3, -1.11640612333182370000e+000) (4, 1.12362812337366160000e+000) (5, 4.84979264390272610000e-001) (6, 1.44068136183047390000e+000) (7, 2.16342623300405960000e-001) (8, 5.66965680128765470000e+000) (9, 1.05725503659233060000e-001) (10, 1.16532832341759970000e+000) (11, -8.41715203964104370000e-001) (12, 1.23104487914863640000e+000) (13, 5.58112513444978590000e-001) (14, 7.16131162588590080000e-002) (15, -6.12610406867294290000e-001) (16, 3.15614815602967780000e-001) (17, 1.58700576826028030000e+000) (18, 1.87213220181193750000e+000) (19, 2.78207418152687180000e-001) (20, 3.13768230053924670000e+000) (21, -1.40329573674148070000e+000) (0, -1.84148292474547170000e-001) (1, 1.27122886738983440000e+001) (2, 7.02946029426248180000e-001) (3, 1.24364080641090190000e+000) (4, 2.92232640053448590000e-002) (5, 5.49737897295790150000e-001) (6, 5.20661525289003220000e-001) (7, 8.40571862535779760000e-001) (8, -2.85976591580070940000e+000) (9, 2.88545600698413110000e-002) (10, 8.40408385521461350000e-001) (11, 1.53656709214397220000e+000) (12, -5.63350121040851250000e-001) (13, 7.00910140946014000000e-001) (14, 2.78004579627483440000e+000) (15, 1.56534752051107720000e+000) (16, 1.07369638787129640000e-001) (17, 1.88618011873229730000e+000) (18, 1.29112810326035900000e+000) (19, 5.74481140907644460000e-001) (20, 5.99435088004259150000e+000) (21, 9.94446532158631880000e-001) (0, 1.37892951868582840000e+000) (1, 9.23279263352702050000e-001) (2, -5.95617367043335120000e+000) (3, -1.41098995516711080000e+000) (4, 2.35616806738760340000e-001) (5, -8.49592451291500080000e-004) (6, 8.43422725380266950000e+000) (7, 1.53914557776349190000e-001) (8, 3.41362034055299950000e+000) (9, 1.37907162304871120000e+000) (10, 4.64763670447838560000e-001) (11, -2.73049441358887710000e+000) (12, 2.57365628817040550000e+000) (13, -1.30461504515969220000e-001) (14, 5.53144573298276490000e-001) (15, 2.15507548723812680000e+000) (16, 1.23743996626166530000e+000) (17, -2.44820539321808760000e+000) (18, -3.47091609635797980000e+001) (19, -9.14349004045086790000e-001) (20, -1.41876392304742660000e+001) (21, -4.61049181647338670000e-002) (0, -7.63233577108831620000e+000) (1, 1.85136074293593440000e-001) (2, 1.41516198239289740000e+000) (3, 2.86689626250702070000e+000) (4, 1.33378715314089650000e+000) (5, -1.82168340755219820000e-001) (6, -4.76497522986487710000e+000) (7, 2.66990754307393440000e-001) (8, 8.48771906468878790000e+000) (9, -5.14012531057966850000e-001) (10, 4.01738802817683990000e+000) (11, 6.92363227367337800000e-001) (12, 1.85416916607315240000e-001) (13, 9.91861183756184820000e-001) (14, 2.02853418371633550000e+000) (15, -1.12579897130927720000e-002) (16, -4.02716870950883650000e-001) (17, 1.03872185033716760000e+000) (18, 6.46545511198703290000e-001) (19, 2.43911992982042500000e-001) (20, 1.51307943024845400000e+000) (21, -3.00022945118615660000e-001) (22, 3.26577720100854660000e+000) (23, 9.22428885143629200000e-001) (24, -6.43147129595054710000e-002) (25, 1.73800394730688490000e+000) (26, 3.87163397842523960000e-001) (22, 2.42128297764699630000e-001) (23, 4.86791109936609010000e-001) (24, 1.37477956124014700000e+000) (25, -2.25465109580846960000e+000) (26, 2.88388456045213460000e+000) (22, -4.56716056905666610000e+000) (23, 2.38146698235745010000e+000) (24, -5.43659011853289580000e-001) (25, 7.25565529437226100000e-002) (26, 1.54155415866198300000e+000) 

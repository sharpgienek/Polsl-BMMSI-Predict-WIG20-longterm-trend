FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=22 6 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (6, 5, 5.00000000000000000000e-001) (6, 5, 5.00000000000000000000e-001) (6, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 2.66175912327234370000e-001) (1, 9.34366997107969470000e-002) (2, 4.29567364235221020000e-001) (3, 4.45819558519299830000e-001) (4, -8.90805803084332440000e-003) (5, 1.03697776513787320000e-001) (6, 1.49350480023958070000e-001) (7, 4.06857305215333700000e-001) (8, -1.96448643202738750000e-001) (9, 2.19433043244021460000e-001) (10, -8.43089008878004230000e-002) (11, 4.45752419846424390000e-001) (12, -3.03065975509046390000e-003) (13, -5.30303731414420440000e-002) (14, 6.82951223682596090000e-001) (15, 3.11732553631081210000e-001) (16, -1.69753370294072170000e-001) (17, 1.19486504786469290000e-001) (18, 2.28774246344181860000e-001) (19, 3.51094800793094020000e-001) (20, 1.76613967957530900000e-001) (21, 1.98331720805104530000e-001) (0, 4.93596326762548170000e-001) (1, 7.18027575735194600000e-001) (2, -4.34335684018534170000e-001) (3, 2.22461904981049420000e-001) (4, -1.68399940201575810000e-002) (5, -6.92693984616426390000e-001) (6, 2.63030966053410750000e-001) (7, -1.47035173717830040000e-001) (8, -4.32730194340070480000e-001) (9, 3.79253063339950790000e-001) (10, -1.21744736883668600000e+000) (11, 6.22940904396417690000e-001) (12, -4.16281219485586660000e-001) (13, -4.05263086878469430000e-002) (14, 3.03209597859990780000e+000) (15, 2.78077447769145890000e+000) (16, 2.62604938408514290000e-001) (17, -2.06282000772551540000e-001) (18, -1.56605486360438810000e-001) (19, 4.94731580685714770000e-001) (20, -1.03292831358164470000e+000) (21, 3.24348967229268930000e-001) (0, -4.38507424455767260000e-002) (1, 1.83919957145984500000e-001) (2, 4.45675665168772330000e-001) (3, 9.49230319707496720000e-001) (4, 1.44974786416274260000e-001) (5, 1.31322528394087590000e-001) (6, -5.12477049326746870000e-001) (7, 4.21536260512192170000e-001) (8, -7.27960098423182430000e-001) (9, -2.29556311619055320000e-001) (10, 9.19648442776474370000e-002) (11, 1.06461728431752150000e+000) (12, -7.42327774418527490000e-001) (13, 2.95089447315420810000e-001) (14, -6.13564185599357880000e-003) (15, 7.73324513462802510000e-002) (16, -6.09454237282307410000e-001) (17, 6.19383442551413750000e-001) (18, 6.36400044367480880000e-001) (19, -3.77406246532785090000e-001) (20, 2.60778096729375910000e-001) (21, 3.33889490435227280000e-002) (0, -1.88326619436836320000e-002) (1, 2.75363106941108100000e-001) (2, 2.09947222748208820000e-001) (3, 1.86296378490324430000e+000) (4, 1.16620967391626740000e-001) (5, -4.59462887066071970000e-001) (6, 3.03948935413136000000e-001) (7, 3.13339161838459830000e-001) (8, -1.59946431131974780000e+000) (9, 4.40518604777437460000e-002) (10, -1.09532699778680020000e-001) (11, 1.06382219529586220000e+000) (12, -5.52252715143575430000e-001) (13, 2.19792805797295290000e-001) (14, 1.58638093899994030000e-001) (15, 4.66430387686289800000e-001) (16, 2.58003322279195760000e-001) (17, 8.09142915910509240000e-001) (18, 9.31441095232893420000e-001) (19, 1.69304729381509450000e-001) (20, -9.54274749591207950000e-002) (21, 3.21016390707437820000e-001) (0, 2.09420914439139020000e-002) (1, 1.52651083113139220000e+000) (2, -1.03793481007286800000e-001) (3, 6.79570251918194310000e-001) (4, -7.13758301215483450000e-001) (5, -4.02634409122703650000e-001) (6, -1.92893445443481410000e-001) (7, 5.26181136963026930000e-001) (8, -2.01605899233377170000e+000) (9, -1.76077138180229520000e-002) (10, -2.00111148234541150000e+000) (11, 1.97902500708503660000e+000) (12, 3.40611144547202650000e-001) (13, -2.31879784051591880000e-001) (14, 1.25309712971119610000e+000) (15, 1.27043905490045980000e+000) (16, 1.35311178391610220000e-001) (17, -1.04431855421708270000e-001) (18, -2.66020885867359840000e-001) (19, 1.37761006646732850000e-001) (20, 4.61308167830103420000e-001) (21, 9.64057635225263890000e-001) (22, 1.84745745031714150000e-003) (23, -3.49488275413206150000e-001) (24, 5.96240744774428450000e-001) (25, 3.50760215500479370000e-001) (26, -7.35138351322156990000e-001) (27, 1.08270717222486460000e-001) (22, 8.02654942838182390000e-002) (23, 3.43973887481724140000e-001) (24, 2.89364908334081160000e-002) (25, -2.66951530819475310000e-001) (26, -9.08351764350748940000e-001) (27, 8.79667137106177060000e-001) (22, 3.94599159067822390000e-001) (23, 2.96486188179586720000e-001) (24, 1.08425123457090200000e-001) (25, 6.75721415312988790000e-001) (26, 1.94251700874492970000e+000) (27, 1.54142716102798040000e+000) 

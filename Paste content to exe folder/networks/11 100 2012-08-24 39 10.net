FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=22 5 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 1.60853531632484480000e+000) (1, 4.35458348073883060000e-001) (2, 8.30127889566108610000e-001) (3, -2.39542037606622130000e-001) (4, 3.18133304137058830000e+000) (5, -5.52933737028609820000e-001) (6, 7.75617390706963890000e-001) (7, -1.81678926237803800000e-001) (8, 5.47919083950056950000e-002) (9, -2.49855196784959430000e-001) (10, 8.30469686446201780000e-001) (11, -1.23209028060625200000e-001) (12, -2.33640297746229850000e-001) (13, -5.59922262523371960000e-001) (14, 7.27198200532336860000e-001) (15, -1.01309613671798500000e-001) (16, 5.88117929174831300000e-001) (17, -5.29758727171946440000e-002) (18, 5.80425052154603050000e-001) (19, 1.41747950457983250000e-001) (20, -6.05887839380187420000e-001) (21, -1.36398725522792000000e-001) (0, 6.01854495833301640000e-001) (1, -1.49439365158495930000e-001) (2, 1.68666031748899800000e+000) (3, 1.02092097607393850000e-002) (4, -3.04252688695846540000e+000) (5, 6.67094624902990760000e-001) (6, 6.98566696797069400000e-002) (7, 1.03213733587413770000e-001) (8, 5.10899904634222630000e-001) (9, 4.26783450210013570000e-001) (10, 2.69528099343127820000e-001) (11, -2.09650547696337450000e-001) (12, 9.40527474951426970000e-001) (13, 2.31174448869270770000e-001) (14, 7.97051462377478400000e-002) (15, -7.94574754018457510000e-002) (16, 3.05060183825066660000e+000) (17, 1.45126198909376060000e-001) (18, -9.08056548574632470000e-001) (19, -1.05467308063438610000e-001) (20, 2.62296275504897090000e-001) (21, 1.09006911345703330000e-001) (0, -3.04807498274426350000e+000) (1, 3.40063470814145670000e-001) (2, -4.45475239333603450000e-001) (3, -3.43952042464920620000e-001) (4, -9.58117521451252060000e-001) (5, -1.31565573691735770000e-001) (6, -3.08126590120759670000e+000) (7, -2.42197481927662940000e-001) (8, -3.03551394740083950000e+000) (9, 2.59274827253081870000e-001) (10, 5.65843402691105000000e-001) (11, 5.78037537743952830000e-002) (12, -3.54544423063926750000e-001) (13, -1.20639932576076350000e+000) (14, 1.08951889536515670000e+000) (15, 8.86487427025374750000e-002) (16, 4.29629522351541170000e-001) (17, 8.11493856085314880000e-001) (18, -1.07134994501354660000e+000) (19, 3.75395605045426960000e-001) (20, 3.18444583438572250000e+000) (21, 8.61637486556135340000e-002) (0, 3.11954104826231580000e+000) (1, 4.23690466600043000000e-001) (2, 3.06266848882109870000e+000) (3, 3.25228551070346210000e-001) (4, 1.13461777299030730000e+000) (5, -3.68831566483891480000e-002) (6, 3.02340457222039570000e+000) (7, 8.49924945054711390000e-002) (8, 3.96582801675505970000e-001) (9, -7.15075576746813100000e-001) (10, -1.41328581517889070000e-001) (11, 2.33864131174520010000e-001) (12, -3.11353518879782150000e+000) (13, 4.99935332757997230000e-001) (14, -1.56550749713467050000e-001) (15, 7.69435669556957640000e-001) (16, -3.04971668418985150000e-001) (17, -7.10933622362859290000e-002) (18, 2.67447564798248290000e+000) (19, -7.65679796153406760000e-003) (20, -3.15498416402483440000e+000) (21, 2.20831291303072770000e-001) (22, -3.69707481557961790000e-001) (23, 2.33599846180839070000e-001) (24, 3.63821976422041670000e-001) (25, -3.87820790747283260000e-001) (26, 5.28437610881230760000e-001) (22, -1.80130415627569960000e-001) (23, 3.36022943410187120000e-001) (24, -4.71823477545895820000e-001) (25, 1.63858536880769960000e-001) (26, 6.88661651278401620000e-001) (22, -1.77975874579839790000e-001) (23, -1.07041610710900640000e+000) (24, 1.00535818606254450000e-001) (25, 2.07945824942204290000e-001) (26, 7.32003501218684070000e-001) 

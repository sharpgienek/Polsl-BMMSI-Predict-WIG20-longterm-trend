FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=22 5 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 1.90534439421324380000e+000) (1, 1.35237592071669520000e+000) (2, -1.60113352278708840000e+000) (3, 9.26866749042877470000e-001) (4, 1.95670456162711840000e+001) (5, -2.86324344679166030000e+000) (6, 6.32480022563855560000e+000) (7, -1.04413883537054190000e+000) (8, 6.44803079350369960000e-001) (9, -1.27357242737698000000e+000) (10, -1.52410835568894300000e+000) (11, -5.18582884984228470000e-002) (12, -2.80559070453223820000e+001) (13, -9.09928152351510810000e-001) (14, -2.58229025815543840000e+001) (15, 5.96574273463113760000e-001) (16, 2.72462149237818840000e+000) (17, 8.32754823799501280000e-001) (18, -2.25730949909668010000e+001) (19, 6.40884053221448120000e-002) (20, -1.22891903654952570000e+001) (21, -1.65114059398270110000e-002) (0, 3.46889888326695720000e+000) (1, -9.30620468723657000000e-001) (2, 2.31695844100191730000e+000) (3, -2.16256940366604640000e-001) (4, 8.05284881969148550000e-002) (5, 1.70727492778202470000e+000) (6, 2.52666770056470540000e+001) (7, 8.87474761327940730000e-002) (8, 4.64095026066324370000e+001) (9, 3.60009676688380520000e-001) (10, 6.29107321240124940000e+000) (11, 5.45539446220584700000e-001) (12, 7.14144080922325750000e+000) (13, 2.33608394395921030000e+000) (14, -5.23137334291013720000e-001) (15, -1.06956452414456770000e+000) (16, 1.43875146178002180000e+001) (17, -3.01755186929798520000e+000) (18, 3.97360748315289380000e+001) (19, -6.16714531292782750000e-001) (20, -2.83091867552775870000e+000) (21, 9.85381666055826500000e-002) (0, 3.18916150272138040000e+000) (1, -1.18174158821162250000e-001) (2, 3.78896305354828570000e+000) (3, -7.17395420211648370000e-001) (4, 2.14350437883842560000e+000) (5, 2.13723127079995940000e-001) (6, 3.60065364423844250000e+000) (7, 2.27923174973695350000e-001) (8, 3.80907438696341270000e+000) (9, 7.36907732386798960000e-002) (10, -1.53358854231281150000e+000) (11, 3.63702217396192210000e-002) (12, 2.27047031509743210000e+000) (13, 1.06213199854194420000e+000) (14, -2.32358647578135270000e+000) (15, 1.51219946386235040000e-001) (16, -5.01819251844931460000e+000) (17, -4.31392297100569730000e-002) (18, 2.38232194880506750000e+000) (19, -2.74464719109279330000e-001) (20, -1.31938030111092390000e+001) (21, 2.12691012003631080000e-001) (0, -2.97203010196523460000e+000) (1, -1.21362075467951320000e-001) (2, -2.43341776201835410000e+000) (3, 1.42337757397565330000e+000) (4, 2.40608181382511970000e+000) (5, 6.31451787542282260000e-001) (6, 2.21731572888028140000e-001) (7, 1.35628286355440660000e-001) (8, -3.62125508264701420000e+000) (9, -7.38011976194602600000e-002) (10, 3.59487081990734580000e+000) (11, 3.42666358150593490000e-001) (12, 1.19721071936783070000e+000) (13, -1.04268065180243650000e+000) (14, 3.10356000859462800000e+000) (15, -1.30536672859598200000e+000) (16, -1.81663505442312000000e+000) (17, -2.26779789015198490000e+000) (18, -1.17675175748767720000e-001) (19, 3.97605806268067300000e-001) (20, 1.46970030965443020000e+001) (21, -3.66023240664670120000e-001) (22, -1.26473381004238680000e+000) (23, -6.29750590072595040000e-001) (24, -9.85478688625367690000e-001) (25, -3.39778966477715050000e-001) (26, 4.06867338088141180000e-001) (22, 3.98167286257804730000e-001) (23, 2.13937751421774310000e+000) (24, -2.16015259938491870000e+000) (25, -1.24407803143671700000e+000) (26, 1.24942774610715700000e+000) (22, 8.79535730009722180000e-001) (23, -1.35107928766466760000e+000) (24, 2.89317477190095750000e+000) (25, 1.51772101722232080000e+000) (26, 7.57591549380372430000e-001) 

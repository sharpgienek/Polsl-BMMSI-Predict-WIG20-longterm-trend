FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=22 5 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 7.56006765503097710000e+000) (1, -7.24986854566207370000e-001) (2, -5.43140353395895840000e+000) (3, 9.72685203278631950000e-001) (4, 3.24449924266291420000e+001) (5, 7.38586853396701940000e-001) (6, 3.32206365927360140000e+001) (7, -2.26156265310905540000e+000) (8, 1.42261658716960430000e+001) (9, 1.10912250281357080000e-001) (10, -1.73938255133663770000e+001) (11, 5.58178132121593840000e-001) (12, -1.07808211580457570000e+000) (13, 2.88749550376492920000e+000) (14, -1.10127530026882800000e+001) (15, -1.08284489262092710000e+000) (16, -7.00224515056737330000e-001) (17, -4.67268832840983080000e+000) (18, 4.69468502345896890000e+001) (19, -1.22352463192114150000e+000) (20, -3.21777516807602450000e+001) (21, 1.69231542868082910000e+000) (0, 2.57279772304926780000e+002) (1, -1.93910500339695670000e+001) (2, -2.13137783064412280000e+002) (3, 1.62119585793664310000e+001) (4, 3.43372337316389600000e+001) (5, 1.91902957965417760000e+001) (6, -2.89529509841370740000e+000) (7, -4.10692362357810130000e+000) (8, -1.03767491107342880000e+001) (9, 1.47348501863559210000e+001) (10, -1.16210997378419270000e+000) (11, 3.96556737037405400000e+001) (12, -3.25392731415496140000e+002) (13, 1.28485465785660100000e+002) (14, 1.29044653399566390000e+001) (15, -9.78857769734631770000e+000) (16, -1.81187139057590370000e+002) (17, -4.02322576821364710000e+001) (18, 9.12569337781851000000e+002) (19, 1.01303874231948440000e+000) (20, 6.74524846758489800000e+000) (21, 7.47427977496962650000e+001) (0, -2.63138378319161140000e+001) (1, 5.62346581570038250000e-001) (2, -2.66811877458359450000e+001) (3, -1.73877865540230750000e+000) (4, 5.85251265611553780000e+000) (5, 3.34213774102800680000e-001) (6, 1.48722784254596710000e+001) (7, -1.46342461630987080000e+000) (8, -1.20391595580393690000e+001) (9, 2.81726843820471150000e+000) (10, -4.65573791275012000000e+001) (11, -2.12603182264776700000e+000) (12, -3.43887056472886950000e+001) (13, 2.43122937040949380000e+000) (14, -1.13553329779598990000e+001) (15, -1.61902382561630340000e-001) (16, -2.64139025414797930000e+001) (17, 2.57304838802648290000e+000) (18, -9.43311498635910440000e+000) (19, -1.89701804856372520000e-002) (20, 7.67496413919975410000e+000) (21, 2.30735419672974370000e+000) (0, 1.10257904646125700000e+001) (1, -2.38703088896218270000e+000) (2, -1.96384587898708650000e+002) (3, 9.48125568371875540000e+000) (4, 2.27305957481078200000e+001) (5, -1.12892482868569670000e+001) (6, 3.05640237619533830000e+001) (7, -1.41701680184358260000e+001) (8, -5.51590525677088910000e+000) (9, -3.66158646623710170000e+001) (10, -1.94973448233343650000e+002) (11, 2.22999359318940050000e+001) (12, 1.13032929533878070000e+001) (13, -3.35198641848996410000e+000) (14, -1.50974984994645690000e+002) (15, 3.61537366232760290000e+000) (16, 1.58534143168168100000e+002) (17, -1.46128984596837340000e+001) (18, 1.60973405427240810000e+000) (19, 3.96880655643945970000e+000) (20, -4.83192242718404540000e+002) (21, -4.07132336831201690000e+000) (22, -1.72364074864849700000e+000) (23, 1.82290243434937690000e+000) (24, -6.83704047639982350000e-002) (25, -1.43336732480722210000e-001) (26, -8.99430788564111430000e-003) (22, 1.51187479193466760000e+000) (23, -7.13430074945466840000e-001) (24, -1.91519974381417060000e+000) (25, -1.79277299305607560000e+000) (26, 2.04043583506200980000e+000) (22, -3.99196418844750410000e-002) (23, -6.51593463212319920000e-001) (24, 2.26650869900830010000e+000) (25, 2.17551884952708010000e+000) (26, 5.43786493040368520000e-001) 

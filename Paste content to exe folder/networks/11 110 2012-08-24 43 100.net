FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=22 5 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -8.53947284468668140000e+000) (1, 1.32949926769214990000e-001) (2, 1.39097656384591860000e+000) (3, -9.72597548143557030000e-001) (4, 5.77462530866560680000e+001) (5, 9.32986753980456610000e-001) (6, 7.28493271797955090000e+000) (7, 4.53507183472230140000e+000) (8, -3.40275236202010940000e+001) (9, 8.30761708169253210000e-001) (10, 8.58378064859675580000e+000) (11, 1.81339986027399050000e+000) (12, -2.84867543934321330000e+001) (13, -1.53520012997653700000e+000) (14, -1.71435945553840520000e+001) (15, 4.69000942112453860000e-001) (16, 2.26058441030869110000e-001) (17, 4.38288596740694380000e+000) (18, -3.54410444510946120000e+001) (19, 1.12166363170049870000e+000) (20, -1.62796721616216790000e+001) (21, 4.38326528627207390000e-001) (0, -1.95900629037451570000e+001) (1, -5.09384260538059520000e-001) (2, 6.39517452726586730000e+000) (3, -2.82752740742402200000e+000) (4, -2.52385458639896750000e+001) (5, -3.30580792180820540000e-001) (6, -3.85294771112942770000e+001) (7, -8.78883490676690470000e-001) (8, 9.56427321114273310000e-001) (9, 1.09249104068065160000e+000) (10, 1.00066641355627140000e+001) (11, -3.33284393741888840000e-001) (12, 7.78567769894401920000e+001) (13, -1.99774225951252980000e+000) (14, 3.95303717612537750000e+001) (15, -1.42256276364187140000e+000) (16, 2.84324810746758200000e+001) (17, -2.23601305409185740000e-001) (18, 1.58595144868953730000e+001) (19, 1.83124747799624680000e-001) (20, 1.68793573768140740000e+001) (21, -4.90098323326907110000e-001) (0, 4.50248856077769770000e+000) (1, -2.04820690483048960000e-001) (2, -2.01324932602586730000e+000) (3, 4.88031975913714440000e-001) (4, -6.47066217150774570000e+000) (5, -8.59268012431944060000e-002) (6, 6.83243762598200630000e+000) (7, 1.85818244642052820000e-002) (8, 3.19093503918156520000e+000) (9, -2.12269385766847420000e-001) (10, -9.22738326267656170000e-001) (11, 4.68357675069545720000e-003) (12, -1.10408813443575920000e+001) (13, 4.11321748702242970000e-001) (14, -6.77312444212222030000e-001) (15, 4.00242385806878320000e-001) (16, -1.44637348695181630000e+000) (17, 2.08814493141053270000e-001) (18, -1.21204087434086660000e+000) (19, 1.83406498104452280000e-001) (20, 7.43509162805560650000e+000) (21, 1.00928341253440570000e+000) (0, 8.56881269705142760000e+000) (1, -4.24126269235314930000e-001) (2, 7.98198413134568250000e+000) (3, -2.48691918761648640000e-001) (4, 7.70289199014341900000e+000) (5, 1.70430493403728670000e+000) (6, 1.26587118817698400000e+001) (7, 4.84186807917886820000e-001) (8, 1.46876595737979710000e+001) (9, 1.59097859008936230000e+000) (10, 2.92997966577332460000e+001) (11, 3.98015813493359850000e-001) (12, -3.28609791006598540000e+001) (13, 5.31643965870516880000e-001) (14, -2.02767481273556350000e+001) (15, -2.44828799508692960000e-002) (16, -8.06760574140220580000e+000) (17, 1.72998691159927850000e+000) (18, -1.60079771801877650000e+001) (19, 1.35057221905166360000e-001) (20, 1.02407258918638300000e+001) (21, 7.59936862012084880000e-001) (22, 1.98396055718536140000e-001) (23, 1.63042073599460300000e+000) (24, 2.16979713952599700000e+000) (25, 1.43140856627469560000e-001) (26, 9.27232634281134620000e-002) (22, -1.14275117110317280000e+000) (23, -8.20036463432059050000e-001) (24, -1.31071002517964420000e+000) (25, 8.69993575003048080000e-001) (26, 1.34950719369396330000e+000) (22, 8.17288799959381860000e-001) (23, -7.05214063500977150000e-001) (24, -7.30254125599901880000e-001) (25, -9.58616361077695210000e-001) (26, 9.39687254483777320000e-001) 

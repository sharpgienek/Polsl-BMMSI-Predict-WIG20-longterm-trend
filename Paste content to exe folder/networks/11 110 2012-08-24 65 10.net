FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=22 7 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (7, 5, 5.00000000000000000000e-001) (7, 5, 5.00000000000000000000e-001) (7, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -3.07499148705160060000e+000) (1, 2.66650599929674380000e-001) (2, 9.34772755004313890000e-002) (3, -7.21202615559506100000e-003) (4, -9.36944425432623350000e-001) (5, -8.60951495334974920000e-002) (6, -1.01505721956534470000e+000) (7, 1.28529893711612480000e+000) (8, -3.75052235869516970000e-001) (9, 1.35544233448443690000e-001) (10, 1.09001938365386450000e+000) (11, 2.81254718992810130000e-001) (12, -6.70473318415239770000e-002) (13, -1.35837999050843150000e+000) (14, 1.19660507860149010000e+000) (15, -3.21102073585166630000e-002) (16, 5.73167621550244370000e-001) (17, 7.19753202618660830000e-001) (18, 2.44411115640145780000e-001) (19, 3.97378758510211590000e-001) (20, 1.18037583031278050000e+000) (21, 2.64824054685474150000e-001) (0, 6.62150048138640980000e-001) (1, 3.84083709274515410000e-001) (2, -8.51218166437238910000e-001) (3, -2.47031411227022990000e-001) (4, 3.05204837147534660000e+000) (5, -6.76273482925731360000e-002) (6, -7.82821355757012620000e-001) (7, 2.10739062653349210000e-001) (8, 6.65983056008257250000e-001) (9, -1.50538580179905650000e-001) (10, 2.47966953228991150000e-001) (11, -2.57637179322795010000e-001) (12, 2.17870141496338130000e-001) (13, -3.97659222812980380000e-001) (14, -8.74313056339095060000e-001) (15, 6.91088615238832500000e-002) (16, 1.85973168364786200000e-001) (17, 1.67122702303138860000e-001) (18, 4.24021544093986350000e-001) (19, -2.27560836256213320000e-001) (20, 5.72448511632659420000e-002) (21, -4.03368861286240550000e-001) (0, -3.16401126376872370000e+000) (1, -7.60285015015634000000e-002) (2, -3.18525149846022780000e+000) (3, -1.00780497404760990000e-001) (4, -3.14985721082343680000e+000) (5, -3.64240714593588100000e-002) (6, -3.17294681077687370000e+000) (7, -1.18488877955806550000e-001) (8, -4.70988893758660820000e-001) (9, 3.34233902584993470000e-001) (10, 1.41807214151388110000e+000) (11, -1.27996073982179660000e-001) (12, 3.12030398772680950000e+000) (13, -2.11951619393645490000e-002) (14, 1.18779160190765910000e+000) (15, 8.05916690881836870000e-002) (16, 1.23075826219082110000e+000) (17, 9.88136738798676810000e-001) (18, 6.05013281589520830000e-001) (19, 1.96424247974096330000e-001) (20, 3.02621829296544840000e+000) (21, -5.66398717470144310000e-002) (0, -7.27589584604905190000e-002) (1, -3.13192984755594480000e-001) (2, 2.94700499331830730000e-001) (3, -1.64118055410185400000e-001) (4, -3.14144656616685850000e+000) (5, 1.55824874845148780000e+000) (6, 7.57393654953915310000e-001) (7, -1.55530503111044710000e-002) (8, 3.06635501231353210000e+000) (9, 9.29817560052450860000e-001) (10, 8.62996905505188260000e-001) (11, -6.63694154156823200000e-001) (12, 5.39919944544269060000e-001) (13, 7.79235965077405450000e-001) (14, 8.37130205900994410000e-001) (15, 1.36005678859376840000e-001) (16, 7.17249736244308990000e-001) (17, 9.23112653703883620000e-001) (18, 3.68534728947671470000e-001) (19, 6.31755605956254430000e-003) (20, 3.07071292253472010000e+000) (21, 4.24622912068646390000e-001) (0, 3.02171389836707770000e+000) (1, 3.33347122452305970000e-001) (2, 1.91441537174933220000e+000) (3, 2.22273381234187990000e-001) (4, 3.18066775815754980000e+000) (5, 4.71161636808613060000e-001) (6, 3.14784915415288950000e+000) (7, -4.01007787246891870000e-002) (8, -1.44889196046983600000e+000) (9, -8.18604152632419200000e-002) (10, -3.18790652779666580000e+000) (11, 8.35626101370856990000e-002) (12, -3.08971927047418670000e+000) (13, 2.53213941720875860000e-001) (14, -3.07377688742412670000e+000) (15, 2.60424141021425300000e-001) (16, -6.62592365722586170000e-001) (17, 6.46294267228925130000e-003) (18, -2.20458141047797040000e-001) (19, -4.48902277096308730000e-002) (20, -3.06387698493285670000e+000) (21, 5.22431656990468340000e-001) (0, 3.36364740290280300000e-001) (1, 4.51756733050371010000e-001) (2, 1.84999886688320150000e+000) (3, 1.10499697928005390000e-001) (4, 4.94255617276828370000e-001) (5, -2.79751916574175590000e-001) (6, 1.41014635108587120000e+000) (7, 2.04869653021365760000e-001) (8, 2.77112417944979650000e-002) (9, 3.39781735715060860000e-003) (10, 5.84980911527535060000e-002) (11, 5.15701379602274000000e-002) (12, -1.54555157853310270000e+000) (13, -1.99690318546449030000e-001) (14, -1.12954160646568940000e-002) (15, 1.74355383715837340000e-001) (16, -1.52466534775312360000e+000) (17, -2.68010599620511210000e-002) (18, 4.47224846523168880000e-002) (19, -3.01032884618950110000e-001) (20, -3.19118411573613070000e+000) (21, 1.32761832956699630000e-001) (22, 4.27572850757013620000e-001) (23, -1.52183701110181120000e-001) (24, 2.56714309040645130000e-001) (25, 4.53000097229659120000e-001) (26, -2.14343476614532540000e-001) (27, -4.06427295083380160000e-001) (28, 3.55240086007296450000e-001) (22, -2.26521628918497970000e-001) (23, -8.78681852387792480000e-002) (24, -3.32786281323650460000e-001) (25, 4.07433028883114670000e-001) (26, 5.16054783522946870000e-001) (27, 4.77690564806783850000e-001) (28, 4.83193454908274980000e-001) (22, 2.05357934278641710000e-001) (23, -7.64315607168589570000e-002) (24, 8.44151288976693200000e-003) (25, -4.67168337941121280000e-001) (26, 5.95600603393270590000e-001) (27, 2.25475518607191840000e-001) (28, 5.33741471612122710000e-001) 

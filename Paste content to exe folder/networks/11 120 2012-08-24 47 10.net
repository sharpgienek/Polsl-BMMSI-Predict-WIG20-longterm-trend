FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=22 5 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -5.58458377483932680000e-001) (1, -8.33395371838304700000e-001) (2, -2.64276436983490400000e-001) (3, -3.74403203273288080000e-001) (4, -3.16446902744741990000e+000) (5, 1.19100542196692190000e+000) (6, -3.13414065785486700000e+000) (7, -9.42144012131455880000e-003) (8, 3.14797122446720850000e+000) (9, 9.91092674223091440000e-001) (10, 3.13509280630655510000e+000) (11, -6.78606632045008690000e-003) (12, 4.29424778792299570000e-001) (13, 6.51601103851156440000e-001) (14, 8.85653155842792690000e-001) (15, -1.00049867919234110000e-001) (16, 8.07655692997722220000e-001) (17, 6.54788173432950370000e-001) (18, -5.64374590821527190000e-001) (19, 3.17730673899749900000e-001) (20, 1.04854936515666130000e+000) (21, 9.97569040171638750000e-002) (0, 3.16773440835545280000e+000) (1, -1.44527893891492680000e-001) (2, 7.52216511452013740000e-001) (3, 1.32745133839324680000e-001) (4, 3.18935883217635390000e-001) (5, 1.28347593806147520000e-001) (6, 1.51344002890604320000e+000) (7, -7.98928818590338040000e-001) (8, 1.81443978432207990000e+000) (9, -9.92627230162988210000e-002) (10, 2.82761155728055440000e-001) (11, -1.85230963509362440000e-001) (12, 6.83282318331915330000e-001) (13, 1.01555428082534530000e+000) (14, 5.60025131607966740000e-001) (15, 9.36765866824547930000e-002) (16, 7.41545326154145480000e-002) (17, -5.88401266853992190000e-001) (18, -3.45699241920442990000e-002) (19, -4.91312642360383970000e-001) (20, -1.93745379491991330000e-001) (21, -2.36903327560590680000e-001) (0, -1.26839462158889750000e-002) (1, -5.63733634193745170000e-002) (2, 9.53106605984602170000e-001) (3, 1.74242211158731290000e-001) (4, -1.01942733681796450000e+000) (5, -1.16875297961826860000e-001) (6, -3.92983461694330630000e-002) (7, 1.55484513318487280000e-001) (8, 5.93486575786734920000e-001) (9, -2.40581860541431850000e-002) (10, 3.99133373820325710000e-001) (11, 7.32213284989203100000e-002) (12, -6.84454238399894610000e-001) (13, 9.25624530331907310000e-002) (14, 6.54198246613284180000e-001) (15, 2.15837670762926950000e-001) (16, 8.74735648563143150000e-002) (17, -1.39068817289727730000e-001) (18, 2.72625803575841040000e-001) (19, -5.93213615101781110000e-002) (20, 6.60704594366486320000e-001) (21, 1.63468777486993070000e-001) (0, -4.35131346860734680000e-001) (1, 2.35742171459789150000e-001) (2, 8.43477862245581860000e-001) (3, 2.84294746830797700000e-002) (4, 4.55656312277134210000e-001) (5, 6.48537138961749800000e-001) (6, 4.64181033832314850000e-001) (7, 3.66674904480182580000e-001) (8, 2.08721344692068770000e-001) (9, 5.20234485758994670000e-001) (10, -3.34907479411591320000e-001) (11, 2.06246023972655290000e-001) (12, -1.50421857010469280000e+000) (13, 6.32826689508895870000e-001) (14, -3.39497030096135580000e-001) (15, 4.92548938471447930000e-001) (16, -1.13893271471495060000e-001) (17, 4.16340441242102950000e-001) (18, -1.84510398580108010000e-001) (19, 2.49523909946403060000e-001) (20, 2.39337066159454330000e-002) (21, 7.53646567947440890000e-001) (22, 5.14169531734904650000e-001) (23, -4.76368720101074840000e-001) (24, 1.82587523603793520000e-001) (25, 2.37725253857627940000e-001) (26, 3.55905369220334940000e-001) (22, 3.87558174096255680000e-002) (23, 5.29424074832129280000e-001) (24, 2.05986179605995190000e-001) (25, 3.42445977174067720000e-001) (26, 6.45518899519192170000e-001) (22, -5.43165526346540810000e-001) (23, -1.56398154917367170000e-001) (24, -4.28320605956490910000e-001) (25, 4.64448352115216570000e-001) (26, 3.83540147784590500000e-001) 

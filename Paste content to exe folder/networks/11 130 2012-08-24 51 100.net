FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=22 5 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -2.43812074093737860000e+001) (1, 4.53159138904740620000e-002) (2, -3.81817817241172360000e+001) (3, 3.08340206391391190000e+000) (4, 4.89271307714637780000e+000) (5, 1.48349819901365720000e-002) (6, 2.60728806487774020000e-001) (7, -8.82116499450525500000e-001) (8, -2.76949894054187100000e+001) (9, 2.11022275817974360000e-001) (10, -7.15745388149072960000e-001) (11, 1.35853526412660710000e+000) (12, -2.24762186010277530000e+001) (13, 5.01093630928349130000e-001) (14, -2.10834136307285500000e+001) (15, 6.06771734957414540000e-001) (16, 6.47792149088696510000e+000) (17, 9.55034314298842220000e-001) (18, 2.18168617301739940000e+001) (19, 1.25206774801552780000e+000) (20, 1.46382008906029310000e+001) (21, 1.82275683190621040000e+000) (0, 1.16651109365123490000e+001) (1, 3.51183550927011700000e-001) (2, 2.64906555351770160000e+001) (3, -1.01364979210401970000e+000) (4, -3.13323628639013380000e+001) (5, 7.13808277437875230000e-001) (6, -1.93594862731643720000e+001) (7, -1.50750317286068400000e+000) (8, 1.82280306479744280000e+001) (9, -2.70939351696500500000e-002) (10, 1.20754063391973500000e+001) (11, -5.79630009026786190000e-001) (12, -3.27283946793375380000e+000) (13, 8.14158296408127200000e-001) (14, -1.09027525776403490000e+001) (15, 4.70857171218602670000e+000) (16, 1.04578642295443310000e+001) (17, 6.91316429989321700000e+000) (18, -8.53217196900396990000e-001) (19, -5.49333713435558810000e-001) (20, 1.38862116735654500000e+001) (21, 2.54863969180552630000e+000) (0, -1.43530465095708660000e-002) (1, -9.60201764933051730000e-002) (2, 4.05841146629658490000e+000) (3, -1.00822977712447940000e-001) (4, -9.72001299178799270000e+000) (5, -2.12968480383787210000e-001) (6, 4.71503887858403110000e+000) (7, -2.00490983936443670000e+000) (8, -3.15886929285188730000e+000) (9, -9.77589592401852810000e-002) (10, -1.37741937319193690000e+000) (11, 2.98778199955340860000e-002) (12, -1.06995846114185110000e+000) (13, 6.98046541440698550000e-001) (14, -1.32710674212594860000e+001) (15, 4.89385930979353840000e-001) (16, 4.23479385944814890000e+000) (17, 2.98612393573854280000e-002) (18, 5.92122117046549600000e+000) (19, -3.36765238928651290000e-001) (20, 4.81648161315408750000e-002) (21, 6.26499174708987860000e-001) (0, -7.73607688992592110000e+000) (1, 8.79474423750756350000e-001) (2, -1.39795483440110980000e+001) (3, -1.14648718514981710000e+000) (4, 5.73563251986288450000e+001) (5, 6.92009170363701990000e-002) (6, -5.59569249831386030000e-002) (7, 3.84381457743417170000e+000) (8, -3.49316092458727850000e+001) (9, -3.60589629193749040000e-001) (10, 1.82770906024029230000e+000) (11, -9.73308169067563340000e-001) (12, -7.97837502615137200000e-001) (13, -2.94351498457847600000e+000) (14, 1.26450951532713520000e+001) (15, -2.41291990980904760000e-001) (16, -7.80383199849970490000e+000) (17, 7.33886929813958310000e-001) (18, -1.46485463535464150000e+001) (19, 1.82958598396183090000e-001) (20, 2.79890191396825250000e+000) (21, 7.12397609169937440000e-002) (22, 1.16799528042338370000e+000) (23, 1.21845256148017160000e+000) (24, -2.21430983782241420000e+000) (25, -1.30368149589921710000e+000) (26, -4.44398391639515220000e-001) (22, -1.68798555515625810000e+000) (23, -5.79640874039189510000e-001) (24, 9.99619931896988610000e-001) (25, 2.48411763313963370000e-002) (26, 2.21784438151457450000e+000) (22, 2.55279439902708680000e-001) (23, -5.70888041756265170000e-001) (24, 1.17110288453609690000e+000) (25, 1.19936517442021340000e+000) (26, 6.75884751281139780000e-001) 

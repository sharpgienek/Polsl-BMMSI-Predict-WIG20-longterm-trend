FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=22 5 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 3.17003817008728320000e-001) (1, -1.64466814602581720000e-001) (2, 1.05782240702969870000e+000) (3, 2.45633937591168560000e-002) (4, -4.55790530448121400000e-001) (5, 2.28476112427233040000e-001) (6, 7.06695404618297380000e-001) (7, -2.21800836634714200000e-001) (8, 2.89288401480383090000e-001) (9, 1.94637977856845060000e-001) (10, -3.30398825717752250000e-001) (11, -9.05047814069552540000e-003) (12, -8.05887562700111640000e-002) (13, -9.44413072343505130000e-002) (14, 2.83587639894465230000e-001) (15, -2.86982685568835440000e-001) (16, 3.23620566565641140000e-001) (17, 1.00235300630737090000e-001) (18, -1.42821797087735370000e+000) (19, -6.16200590021683120000e-002) (20, -1.69236218528461270000e-001) (21, 3.67765287067317450000e-002) (0, -6.10298830720451820000e-002) (1, 1.00659769570168480000e-001) (2, -9.38909757493159080000e-001) (3, 1.41529677218820200000e-001) (4, 3.15107791396662670000e+000) (5, -2.01193301570978990000e+000) (6, -1.50517954458732060000e-001) (7, -1.33402213582850400000e-001) (8, -1.21957749582155240000e+000) (9, -7.59432968392429020000e-001) (10, -3.07217762202454560000e-001) (11, -3.54663712546411940000e-001) (12, 2.13430910540757830000e-001) (13, -5.24342283878552640000e-001) (14, -3.39273426742621180000e-001) (15, 7.50616831329695140000e-003) (16, 2.54644891753032450000e-001) (17, 6.85108078942119200000e-002) (18, 2.21656818281183200000e+000) (19, 8.39882644823998490000e-003) (20, -3.22803049710504500000e-001) (21, 1.92686835135203900000e-001) (0, -3.73875246024363510000e-001) (1, 2.16474187405208850000e-001) (2, 1.54229745121105230000e+000) (3, -1.57759050830753260000e-001) (4, 6.53856449733191010000e-001) (5, 4.63885221632738880000e-001) (6, 8.75038592582133120000e-001) (7, -9.77853369876391380000e-003) (8, -5.91901446700459300000e-001) (9, 2.02786171337637600000e-001) (10, -3.11683108728443430000e+000) (11, 1.58057438487856470000e-001) (12, -1.85380454412164830000e-002) (13, 5.35358175507832490000e-001) (14, -2.25998751217533040000e-001) (15, 1.78665077228205290000e-001) (16, -6.75373127631784360000e-001) (17, -7.92508144986502900000e-002) (18, -4.83437080938563250000e-002) (19, -1.35106388222199920000e-001) (20, -1.11796032125660800000e-001) (21, 1.65420448124594940000e-001) (0, 1.83724033700353080000e+000) (1, -4.41989492726334810000e-001) (2, 3.12759158549165320000e+000) (3, 1.75036800490332050000e-001) (4, -1.10485824629410920000e+000) (5, 1.22494980558635950000e+000) (6, 1.34791321515115470000e-001) (7, -1.74310585085991900000e-003) (8, 1.14927231422430200000e+000) (9, 3.03951017706565680000e-001) (10, 7.31392678003970560000e-001) (11, 2.03730013002498820000e-002) (12, -3.20672366674893810000e+000) (13, 7.42923816340552490000e-001) (14, 4.74156740024206770000e-001) (15, 6.57572252568714990000e-001) (16, 8.68543746925776630000e-001) (17, 2.51850245857814960000e-001) (18, -7.30811677621217850000e-002) (19, 4.61514004055730560000e-002) (20, 1.16902939459683040000e+000) (21, 2.93567863071860780000e-001) (22, 9.97355279296568000000e-002) (23, -1.56311249187284980000e-001) (24, -6.61782293824745290000e-002) (25, 1.26157647464002040000e-001) (26, 6.36884744812402310000e-001) (22, -7.31279596300718480000e-003) (23, -1.14303512598030350000e-002) (24, 3.34263593562001800000e-001) (25, 2.30465625447297900000e-001) (26, 5.97041475083444520000e-001) (22, 8.27930346695336800000e-002) (23, 4.56447357869563310000e-001) (24, 4.67877737558927810000e-001) (25, -4.32532362636336820000e-001) (26, 6.62527431360774300000e-001) 

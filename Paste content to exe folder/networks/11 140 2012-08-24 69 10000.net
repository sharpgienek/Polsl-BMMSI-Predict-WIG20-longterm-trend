FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=22 6 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (6, 5, 5.00000000000000000000e-001) (6, 5, 5.00000000000000000000e-001) (6, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 1.50000000000000000000e+003) (1, -6.33150694275810720000e+001) (2, 6.86820795729347420000e+002) (3, 1.55382328997925380000e+002) (4, -1.38932595414215780000e+003) (5, 4.63833944766238630000e+001) (6, 6.90584889765980170000e+002) (7, 5.84155814892980400000e+002) (8, -1.12609690794621260000e+002) (9, 8.86520371099349290000e+002) (10, 7.70620556465743450000e+002) (11, 2.40413191724846510000e+002) (12, -1.29495664131332520000e+003) (13, 2.89808564625278960000e+002) (14, -1.50000000000000000000e+003) (15, 1.59652378094221380000e+002) (16, -3.31663181760062000000e+001) (17, -4.46887364327903710000e+001) (18, -6.86072590386504880000e+002) (19, 7.07284697340488440000e+001) (20, 7.33267708458996820000e+002) (21, -3.05134305256086660000e+001) (0, 2.84872831681580460000e+002) (1, 8.27826769456714470000e+000) (2, 5.83698255303380620000e+001) (3, 5.66194275372804010000e+001) (4, -4.88042957011688030000e+002) (5, 2.28484890135022340000e+002) (6, 5.76518101588315860000e+002) (7, 4.78383924718945510000e+001) (8, 1.30783155990096500000e+003) (9, 3.95246743193406260000e+001) (10, 1.50000000000000000000e+003) (11, -5.77071714305795740000e-001) (12, -1.07865999058785130000e+003) (13, 6.84864113184230660000e+001) (14, 3.25620292241200390000e+002) (15, 9.60994857343463770000e+000) (16, 9.81165186420854870000e+002) (17, -1.26775500241341720000e+001) (18, 8.91138265964323300000e+002) (19, 8.39233105131588530000e+000) (20, 4.43983676508240530000e+002) (21, 2.28874149598536110000e+001) (0, 1.28949613505293560000e+003) (1, -4.00193647178617910000e+001) (2, 9.07107463198082540000e+002) (3, -1.15390749446913170000e+001) (4, -7.32456712276864550000e+002) (5, -8.74395195412455450000e+001) (6, 8.82912363759894450000e+002) (7, -3.91518003974801730000e+002) (8, 1.50000000000000000000e+003) (9, -3.65695189479992870000e+000) (10, -1.50000000000000000000e+003) (11, 1.60050414889602810000e+001) (12, 7.16188946939228000000e+002) (13, -9.92201549861615460000e+000) (14, -1.04463480153055440000e+002) (15, 2.39598330354740450000e+001) (16, -5.03534074839894460000e+002) (17, 8.29743074593279320000e+000) (18, -3.50535404885421140000e+002) (19, -3.32741739521096650000e-001) (20, -5.88466538127795050000e+002) (21, -1.24585247270550600000e+002) (0, -1.87819282697630370000e+002) (1, -4.15544165293878100000e+000) (2, -9.15837014538145700000e+001) (3, -7.96926525768539280000e-001) (4, 8.44416472789699950000e+001) (5, 8.46257215484782680000e+000) (6, 1.88629405741742420000e+001) (7, 1.00569163537384850000e+000) (8, -1.55632219517663730000e+001) (9, 9.16779407584298230000e+000) (10, 6.60929212013344980000e-001) (11, 4.64238252034503150000e+000) (12, 2.73246118704823400000e+001) (13, 3.83765528323160070000e+000) (14, 3.16596305833649830000e+001) (15, 4.79875331088558000000e+000) (16, 3.15112259176351870000e+001) (17, 2.13858536613376860000e+000) (18, -3.52109511624046550000e+001) (19, 1.30067742508380000000e+000) (20, 2.15673829568715480000e+001) (21, -2.38421749549904490000e+000) (0, -1.79669216623488380000e+002) (1, -1.65611448291412630000e+000) (2, -1.43597977444481560000e+002) (3, -4.08348405907336880000e+000) (4, 1.43641904715272120000e+002) (5, 6.88567519809089590000e-001) (6, 7.90164596311186070000e+001) (7, -3.74441508790534750000e+000) (8, -3.89983292182252440000e+001) (9, -2.34827829371199610000e+000) (10, -5.89643782644598740000e+001) (11, -1.67613244615779530000e+000) (12, -1.86919431727730160000e+001) (13, -1.58051092273831470000e+000) (14, 7.14490014942900160000e+001) (15, -7.31082554716798730000e-002) (16, 8.57423328584882680000e+001) (17, 6.31231935485212390000e-001) (18, -4.19664266311053850000e+001) (19, -2.13005046250291440000e+000) (20, -4.22963774515139410000e+001) (21, 8.17595875872378740000e+000) (22, -1.72334428260573320000e+000) (23, -3.79757946770131470000e-002) (24, -1.48058745355365260000e-001) (25, 2.11587183805516330000e+000) (26, -1.98886609593174010000e+000) (27, 1.71748628437642540000e+000) (22, 3.74659714236559140000e-001) (23, 1.10605870084539020000e+000) (24, 1.15525806808871150000e+000) (25, -1.20341659394462910000e+000) (26, 4.23617304329509390000e-001) (27, 1.16936165404009060000e+000) (22, 7.00815820312252650000e-001) (23, -8.62722908294431880000e-001) (24, -7.21282583123373340000e-001) (25, -3.13490192622986610000e-001) (26, 7.27850679387613160000e-001) (27, 3.38698619221923850000e-001) 

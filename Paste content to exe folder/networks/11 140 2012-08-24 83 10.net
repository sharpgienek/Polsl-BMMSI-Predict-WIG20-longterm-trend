FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=22 7 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (7, 5, 5.00000000000000000000e-001) (7, 5, 5.00000000000000000000e-001) (7, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 2.18507794941656230000e-001) (1, -5.46327808752827480000e-002) (2, 6.10615291676649960000e-003) (3, 8.59995795934137690000e-002) (4, -3.09753787410631800000e+000) (5, 1.26009462177267870000e+000) (6, 5.29445325209627270000e-002) (7, 1.37601784315144780000e-002) (8, 1.88926138866397860000e-001) (9, 4.75461035064353170000e-001) (10, 1.42768785525350990000e+000) (11, 8.97105171487110470000e-002) (12, 1.76674298224538590000e-001) (13, 5.86324003269076850000e-001) (14, 1.34782945562602710000e+000) (15, 2.31611582087466250000e-002) (16, 3.45658318936264250000e-001) (17, 1.45829098777443930000e-001) (18, -2.98410772884473430000e-001) (19, 3.71409362872382600000e-002) (20, 2.04355826596318120000e-001) (21, 9.82127145410286590000e-002) (0, -1.69905417137350690000e-001) (1, -1.29947623687623000000e-001) (2, -3.04601809794798850000e+000) (3, 2.99469623644306180000e-002) (4, -4.70299196482758530000e-001) (5, 5.17275272585210330000e-003) (6, -3.08754641887930870000e+000) (7, 1.93938629628039180000e-001) (8, -5.99016425718357300000e-002) (9, 3.63039887781086260000e-001) (10, 3.18879764109119450000e+000) (11, 3.61293215985490220000e-002) (12, 1.20539751202513080000e+000) (13, -9.64078262142748830000e-001) (14, 3.74671865723773420000e-001) (15, 1.66171164209239470000e-001) (16, 5.62358859951428700000e-002) (17, 4.08408456849464750000e-001) (18, 1.56725472697356970000e+000) (19, 2.88980003929858790000e-001) (20, 3.16360232821575500000e+000) (21, -1.95159412830628930000e-001) (0, -1.00142971051898000000e+000) (1, 3.58775085340367200000e-002) (2, -4.58609655079554310000e-001) (3, 9.32871773582574370000e-002) (4, -9.27050626457069260000e-001) (5, 2.89529954657043400000e-001) (6, -1.92604785037726090000e+000) (7, 6.48710852114554040000e-002) (8, -1.89518104578760980000e-002) (9, 2.10009188991501390000e-001) (10, 8.98640380932028250000e-001) (11, 3.95083836601014590000e-001) (12, 4.91358714743542260000e-001) (13, 1.70847836446282870000e-002) (14, -1.08513898930522240000e-001) (15, 4.59488134435713400000e-001) (16, -1.02423491389201070000e-001) (17, 2.97951615015285550000e-002) (18, 9.61964356485004000000e-001) (19, 4.23684038763844640000e-002) (20, 2.08919200222360990000e-001) (21, 4.46243505332103050000e-001) (0, 3.01539065608535410000e+000) (1, -2.52379450371621020000e-001) (2, 3.12759158549165320000e+000) (3, 8.90169560321153050000e-002) (4, -3.21196658674893860000e+000) (5, 3.25913987920617540000e-001) (6, 4.57927380918539410000e-001) (7, -6.34412433662047470000e-001) (8, 3.13638064812262040000e+000) (9, 2.24252004722546100000e-002) (10, 2.67451915938316410000e-001) (11, -9.26358787059728130000e-002) (12, -8.71615302392645400000e-001) (13, 1.05993588776338820000e+000) (14, 1.26911851695314270000e+000) (15, 1.37914142467833150000e-001) (16, 1.07042947112136400000e+000) (17, -4.11301015706540870000e-001) (18, -3.42368840957709640000e-001) (19, -8.69046232064509260000e-002) (20, 1.29718282372749920000e+000) (21, 7.22340701948715750000e-002) (0, 2.66760308928632380000e+000) (1, -7.46036667559033810000e-002) (2, 6.54309689190656910000e-002) (3, -1.12463774741937940000e-001) (4, 6.54601549787037660000e-001) (5, 5.13374482685473030000e-001) (6, 6.64113624267059550000e-001) (7, -1.54198109912956900000e-002) (8, 4.35794286426397560000e-001) (9, 3.70326384850765300000e-001) (10, 4.93995658382535730000e-001) (11, 2.71906972297164760000e-001) (12, 4.35903548702309520000e-001) (13, -3.60599650096824630000e-002) (14, 1.50186877548324720000e-001) (15, 1.63264392553264190000e-001) (16, 4.10037450105090790000e-001) (17, 1.45216296581201610000e-001) (18, -1.27523498431335500000e+000) (19, 1.71467517675500280000e-001) (20, 4.88656836042235920000e-001) (21, -1.36035552436934800000e-001) (0, 1.19052597694840450000e+000) (1, 6.52355573450566160000e-001) (2, 3.09128177049746980000e+000) (3, 2.14792838470591720000e-001) (4, 3.15882937892588260000e+000) (5, -6.79859211683007230000e-001) (6, 3.01962649599222300000e+000) (7, 1.01819932831170680000e-001) (8, -1.26305283826626050000e+000) (9, -7.03455403440867770000e-001) (10, -1.22905625572842190000e+000) (11, 1.86728937429084580000e-001) (12, -1.30182237009397150000e+000) (13, -3.22687726826523910000e-001) (14, 6.51316593605825310000e-002) (15, 1.32130840529539380000e-001) (16, -1.81097626509755780000e-001) (17, -4.91648335724761190000e-001) (18, 6.46370633165800460000e-001) (19, -7.88680228718510460000e-002) (20, -3.09292971974077610000e+000) (21, 2.62192741520655300000e-001) (22, 4.41549901746537910000e-001) (23, 2.33531762823464470000e-001) (24, 4.70578222491593590000e-001) (25, -1.88141436327239870000e-001) (26, -2.00688340504831560000e-001) (27, -3.89686406626025550000e-001) (28, 4.02072362095776740000e-001) (22, 4.24820165169120410000e-001) (23, -3.06592687812762890000e-001) (24, 3.82217625471793110000e-001) (25, 2.70445409082137540000e-001) (26, -1.52720810493184670000e-001) (27, 3.54973343433287070000e-001) (28, 3.71829441723247110000e-001) (22, -4.11194809884238290000e-003) (23, -1.78859274692648560000e-001) (24, 2.56236709097153070000e-001) (25, -2.92783810850856310000e-001) (26, -1.59020244366350820000e-001) (27, 2.91369871349960840000e-001) (28, 3.57155855373918420000e-001) 

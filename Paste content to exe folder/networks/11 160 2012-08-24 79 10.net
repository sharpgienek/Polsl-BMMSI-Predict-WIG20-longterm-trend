FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=22 6 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (6, 5, 5.00000000000000000000e-001) (6, 5, 5.00000000000000000000e-001) (6, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 7.70527403434526080000e-003) (1, 4.33665905947640260000e-001) (2, -9.43304288808642700000e-001) (3, -2.50488906306586380000e-001) (4, 3.06716067638803750000e+000) (5, -1.85544238057515740000e+000) (6, -1.00739120382611210000e+000) (7, 3.38926888444111170000e-001) (8, -1.16170395980291950000e+000) (9, 4.70818284914992220000e-004) (10, -6.64811017395859900000e-002) (11, 5.96082149260498960000e-002) (12, 2.60285877691164460000e-001) (13, -1.34890441900486510000e+000) (14, -4.47525381480700980000e-001) (15, -3.03485733268175630000e-001) (16, 3.60074598625877150000e-001) (17, 2.36951125152400740000e-001) (18, 1.65475409991153840000e-001) (19, -1.94073584830117550000e-002) (20, -1.76236290031672540000e-001) (21, -3.11212784164567660000e-001) (0, 7.01123654731989340000e-002) (1, 2.36566992515908680000e-001) (2, 8.08767312518522690000e-002) (3, -1.66461735044868250000e-001) (4, 3.33386832846033760000e-002) (5, 1.35164992730518050000e-001) (6, 1.02863377242319560000e-001) (7, -1.46027164427869420000e-001) (8, 1.53139205844266020000e-001) (9, 3.11748789359156880000e-002) (10, 2.09822492077860130000e-001) (11, 9.71651984388323740000e-003) (12, 1.48897750864531800000e-001) (13, -3.08541013723910020000e-002) (14, 1.57497051845025350000e-001) (15, 1.15917014977118160000e-001) (16, 2.35969829855947780000e-001) (17, 2.48629023417704310000e-002) (18, 7.07965002830054920000e-002) (19, -4.92378907088308060000e-002) (20, 7.32319780910555270000e-002) (21, -1.20735835144144820000e-002) (0, 1.59010018629836860000e+000) (1, -1.37211777058957070000e-001) (2, -3.26413934444325090000e-001) (3, -1.80845810912279830000e-001) (4, 3.02386843941480790000e+000) (5, -4.82601315145236410000e-001) (6, -1.69484092612362830000e-001) (7, -8.10794969305257680000e-002) (8, 1.74498846772288660000e-001) (9, -1.06015424288795540000e-001) (10, -1.59343638183995000000e-001) (11, -2.40934923135802360000e-001) (12, 2.77763267977686690000e-001) (13, -1.32202544567318530000e-001) (14, -4.67577028473445070000e-001) (15, 1.91606748447166740000e-001) (16, 5.65504448510599780000e-001) (17, -2.24838668246899850000e-001) (18, 7.31147716462391180000e-001) (19, -1.08463415870952010000e-002) (20, 3.40557960731634800000e-001) (21, -1.66458540423843140000e-001) (0, 2.11522540786420700000e+000) (1, -4.47181707947404470000e-001) (2, -1.43012975315900740000e+000) (3, -2.06526338717434090000e-001) (4, -1.29642075868535620000e+000) (5, 1.70796705280969130000e-001) (6, -3.19895389124253440000e+000) (7, 9.50072271052422030000e-002) (8, 3.16221683014823450000e+000) (9, 5.36997843142122400000e-001) (10, 3.15470340230190070000e+000) (11, -1.57423545557646680000e-001) (12, 3.07084719988047090000e+000) (13, 3.82492393707700660000e-001) (14, 3.19593875447885530000e+000) (15, 4.55139234029878050000e-001) (16, 7.62183552616159220000e-001) (17, 8.41109552328425100000e-002) (18, -3.03466555871632250000e+000) (19, 9.62329762760544130000e-002) (20, 3.05740115475823430000e+000) (21, 2.50396730156373650000e-001) (0, 9.63618916167707780000e-001) (1, 3.42479891607101410000e-001) (2, 3.20811526833217450000e+000) (3, 9.04839274257379370000e-002) (4, 3.21604905825157220000e-001) (5, 6.63963005741931570000e-002) (6, 1.34237291257596820000e+000) (7, 1.02331445964381570000e-001) (8, -1.29740342471562410000e+000) (9, -3.22189376266140860000e-001) (10, -1.26548203752120840000e+000) (11, -2.88795507143062570000e-002) (12, -8.51047267997192320000e-001) (13, 3.50678781006622920000e-001) (14, -5.34972049008959120000e-001) (15, 1.37735841739771990000e-001) (16, 3.40683581127416700000e-001) (17, -3.30569503344139480000e-001) (18, -3.86777089559855900000e-001) (19, -1.61475062881934860000e-001) (20, -1.16506089344669170000e+000) (21, 2.75074421016199060000e-001) (22, 2.69641994169807720000e-001) (23, -1.43036455186373280000e-001) (24, -3.33469915043855310000e-001) (25, 6.86757715525826320000e-001) (26, -5.37431330988702440000e-001) (27, 7.40103116110492070000e-001) (22, -3.52392403346610090000e-001) (23, -5.07442408393103950000e-002) (24, -2.54048312081816720000e-001) (25, 2.26001114832891090000e-003) (26, 2.91079901596188070000e-001) (27, 4.98402238513504900000e-001) (22, 3.51750687135265750000e-001) (23, 1.22072570935084220000e-001) (24, -1.54776411343515860000e-001) (25, -2.61694790937825160000e-001) (26, 3.22752883968789820000e-001) (27, 5.58228899561241550000e-001) 

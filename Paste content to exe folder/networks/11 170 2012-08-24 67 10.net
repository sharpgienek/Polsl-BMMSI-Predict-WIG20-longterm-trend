FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=22 5 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -1.02499374315091040000e+000) (1, -2.65980487990253260000e-001) (2, -2.53492694835420700000e-001) (3, 3.98082474935038180000e-001) (4, -2.21688096384744520000e-002) (5, 2.44121291000212820000e-001) (6, -3.09433963187116050000e+000) (7, 1.48294254937266170000e-001) (8, 6.18541507800707380000e-001) (9, 6.44455092360553430000e-001) (10, 6.19780521491045120000e-001) (11, 1.07917168598903050000e-001) (12, 5.75542239581843470000e-001) (13, -1.73452844395279170000e-002) (14, 4.85094129370230550000e-001) (15, 7.16256641518674960000e-002) (16, 8.10112710345843590000e-002) (17, 2.70761297777792290000e-001) (18, -2.49807608028899120000e-002) (19, 1.50509831142155830000e-001) (20, 1.84289219255650010000e+000) (21, 8.83492972135690450000e-002) (0, -1.10642074631739230000e+000) (1, 6.47103573523525460000e-002) (2, 1.50234931248428020000e-001) (3, 9.95718567402386070000e-002) (4, -3.16136844146371750000e+000) (5, 3.42731473288709730000e-001) (6, -7.32290349144666950000e-001) (7, 6.43547261270857710000e-001) (8, 5.19035891083583280000e-001) (9, 7.15580065933098860000e-001) (10, 3.66704235027958680000e-001) (11, 2.22789632013826480000e-001) (12, 5.47697890068724270000e-001) (13, 2.14447779970883820000e-001) (14, 4.52672253887106910000e-001) (15, 1.70862702684145720000e-001) (16, 3.48540711183500550000e-001) (17, 3.76488236332008950000e-001) (18, -3.90740720392611650000e-001) (19, 2.16085975523855610000e-001) (20, 5.88304690943893820000e-001) (21, 2.57463576355278390000e-001) (0, -1.40380014854539990000e-001) (1, -1.55555681657295500000e-001) (2, 3.04272219946137580000e+000) (3, 3.07942666129689850000e-001) (4, -3.78781483971962100000e-001) (5, 1.09377997202351280000e+000) (6, 4.68464359034232500000e-001) (7, -5.54005195466931170000e-002) (8, 2.50935802410957140000e-001) (9, 1.50197137740284700000e-001) (10, 3.38566466782359580000e-001) (11, 1.28873681945055190000e-002) (12, -1.03994125313927070000e+000) (13, 7.61640606046409490000e-001) (14, 7.65987220466111900000e-001) (15, 2.66042167441812250000e-001) (16, 8.05106724679223620000e-001) (17, 2.48881640976712570000e-002) (18, 2.44585264861608140000e-001) (19, -4.62722600292665020000e-002) (20, 3.97392712047752560000e-001) (21, 5.42573897102620830000e-001) (0, 1.33907643368059550000e-001) (1, -4.07143797734549060000e-001) (2, -2.39126673498895460000e-002) (3, -5.38001803075921490000e-003) (4, -9.16973722010036680000e-001) (5, 5.78544818895995850000e-001) (6, -7.02270400774751110000e-001) (7, 5.72039641840327300000e-003) (8, 1.21166245187273750000e+000) (9, 3.98570756192314140000e-001) (10, 6.01543216531788220000e-001) (11, 1.65634897111405330000e-001) (12, 9.14360934422277390000e-001) (13, 6.11554465705745680000e-001) (14, 4.88506105472403970000e-001) (15, 3.95146683875666480000e-001) (16, 3.48865515808213010000e-001) (17, 3.50066116797663700000e-001) (18, -7.02923476956357710000e-001) (19, 2.85962673614663710000e-001) (20, 1.13747421834537170000e+000) (21, 2.18321730068894410000e-001) (22, 1.74308282947127410000e-001) (23, 1.31815606532687200000e-001) (24, 7.35909962124032260000e-002) (25, 3.04265039653382420000e-001) (26, 5.33633323356841100000e-001) (22, -2.76048619272904140000e-001) (23, -2.32841831519697810000e-001) (24, 6.52986496404146740000e-001) (25, -6.30792098830923460000e-003) (26, 6.40950363412294390000e-001) (22, -5.01521713444019610000e-002) (23, -2.93558566677377730000e-001) (24, -2.74472873033603740000e-001) (25, -5.08026640265805570000e-001) (26, 1.07368693641941880000e+000) 

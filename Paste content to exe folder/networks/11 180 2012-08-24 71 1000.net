FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=22 5 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 1.25360432938990360000e+002) (1, -2.80072807826447080000e+000) (2, 1.42812699025409070000e+002) (3, -6.58601629650807310000e+000) (4, -9.68215628125941090000e+001) (5, -2.10164801722242520000e+000) (6, 1.32670732131404490000e+002) (7, 4.86858488170769380000e+000) (8, 6.34022216374569040000e+001) (9, -7.69909931849293370000e+000) (10, -3.14078605237665690000e+001) (11, -1.44366800885724530000e+000) (12, 3.89959655185419360000e+001) (13, 1.16062437605068570000e+001) (14, -4.08523681821851740000e+001) (15, 1.01316756952720130000e+000) (16, 2.22296319643818610000e+002) (17, -4.06883258070359370000e+001) (18, -7.67869860908711670000e+001) (19, -3.47196227736069170000e-001) (20, -2.15466662223871130000e+001) (21, -2.38187867243327460000e+001) (0, 3.48755953339581520000e+001) (1, -1.08319890969233020000e+000) (2, 4.88843847462519750000e+001) (3, 2.41476040833696450000e+000) (4, -9.24813601570413650000e+001) (5, 4.43297467285684470000e+000) (6, -3.92791649617877140000e+001) (7, 1.66652346494491250000e+000) (8, 4.28406357471846920000e+001) (9, 2.68941183858950070000e+000) (10, 8.81678670349767090000e+001) (11, 1.52914687478235380000e+000) (12, -1.42023872290271990000e+001) (13, 5.30870543128250020000e+000) (14, -8.20262018330602270000e+001) (15, 1.58796237690688740000e+000) (16, 4.38245995673957190000e+000) (17, 5.24645703997457360000e-001) (18, -2.96482426687665740000e+001) (19, -2.94791301885975910000e-001) (20, -2.14885272891901810000e+001) (21, -3.18078796744325820000e-001) (0, -9.62897714546275020000e+000) (1, 3.26031272568435980000e-002) (2, 1.91033411068867700000e+001) (3, -2.44485162633819250000e-001) (4, 2.26757768127425480000e+001) (5, 2.96523623935277090000e-001) (6, -1.99327264182470430000e+001) (7, 2.01948828521560220000e+000) (8, -1.31579650794839690000e+001) (9, 6.80403572115015900000e-001) (10, 3.06282611942653630000e+001) (11, 7.40686652131262390000e-001) (12, 2.49887035250585790000e+001) (13, -1.09315415144773410000e-001) (14, 1.24566405907806900000e+001) (15, -4.06271012305366300000e-002) (16, 5.87184842578469240000e+001) (17, -3.43200855112097030000e-001) (18, 3.11230103382423100000e+001) (19, 2.72706959083691470000e-001) (20, 9.44907078924633130000e+000) (21, 2.57428621038354820000e+000) (0, -9.97446665252647050000e+001) (1, 3.44100639518806650000e+000) (2, 1.48346352935975290000e+001) (3, -4.95524080213026700000e+000) (4, 2.33785190133854340000e+002) (5, -3.57272819715248690000e+000) (6, -7.04599490397685460000e+001) (7, -6.51805584364883310000e-002) (8, -8.49582028338919460000e+001) (9, -2.07751592706911400000e-001) (10, 4.44928072473470240000e+000) (11, -2.98258136698733440000e+000) (12, 4.64351062599015840000e+000) (13, -9.43647159199338680000e+000) (14, 6.53290626034124950000e+001) (15, -4.29196591878148580000e+000) (16, 1.58984607122571490000e+001) (17, 6.11791340333974530000e-001) (18, 5.65440122291986070000e+001) (19, -7.45439405202208080000e-001) (20, -1.66960743467864070000e+001) (21, 3.78004785439182060000e+000) (22, -1.67211454950755930000e+000) (23, -5.95206539456516250000e-001) (24, 1.94528205824050020000e+000) (25, -2.02904393972362530000e+000) (26, -1.26325630508776010000e+000) (22, 9.99994178972250290000e-001) (23, 1.38768160017100790000e+000) (24, -8.85767238840763690000e-001) (25, 1.27212667243056730000e+000) (26, 1.47862218845992980000e+000) (22, 1.55695068450998590000e-001) (23, -1.04430639771894420000e+000) (24, -3.69482038400468950000e-001) (25, 1.76515357614486640000e-001) (26, 1.61767885842469370000e+000) 

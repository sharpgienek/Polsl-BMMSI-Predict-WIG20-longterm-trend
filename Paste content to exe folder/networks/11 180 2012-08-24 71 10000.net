FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=22 5 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -5.12741477139680910000e+001) (1, -1.01615395566322930000e+000) (2, 2.22778611557074980000e+001) (3, 2.84338224904919670000e-001) (4, -1.27270501425163050000e+001) (5, 8.12310915821660370000e+000) (6, -3.29267154304867450000e+001) (7, 2.89939728611175340000e+000) (8, -5.23873216236965860000e+001) (9, 2.42736890350743200000e+000) (10, 1.01955220150260770000e+002) (11, -1.07988058706317140000e+000) (12, -6.42510296567760410000e+001) (13, 5.17550927608413860000e+000) (14, -9.89185969217054580000e+001) (15, 1.88094284397965050000e+000) (16, 1.32262026011476120000e+001) (17, -1.18855169598916820000e-001) (18, 1.52868996145342100000e+001) (19, 1.74099064548907080000e-001) (20, 8.84837369333262200000e+000) (21, 2.61343373889402470000e+000) (0, 1.50000000000000000000e+003) (1, -7.35126211554481300000e+000) (2, 6.29201367066100490000e+002) (3, 3.32129118759120630000e+001) (4, -9.37926356473982570000e+001) (5, -1.55392267954618360000e+000) (6, -4.91575769547765870000e+001) (7, -6.00542612402447860000e+000) (8, 1.12932359799775050000e+003) (9, 2.63416705319919940000e+001) (10, -8.31709405666387060000e+002) (11, 8.53836753830523920000e+001) (12, 7.45764613628082880000e+002) (13, -2.14188832506811800000e+001) (14, 2.74712710518386150000e+000) (15, -1.88626282371769460000e+001) (16, 5.22560468577576330000e+002) (17, -1.14641768357582520000e+001) (18, 4.78266485014412180000e+002) (19, -3.18164642155082160000e+001) (20, -2.30733542658644640000e+002) (21, -4.70262654983793880000e+001) (0, -5.40877359599585860000e+001) (1, -1.74654690212042140000e+001) (2, 2.01736923025877700000e+002) (3, 1.86810891638632020000e+001) (4, -1.50000000000000000000e+003) (5, 3.30223195091061640000e+000) (6, -2.48729313431358320000e+002) (7, 1.04016980655433780000e+001) (8, 3.48173450602853620000e+002) (9, 1.22617814804980320000e+001) (10, 1.60748601593083720000e+002) (11, 1.92068082407637100000e+001) (12, 2.72227998734779080000e+002) (13, 6.79578140729082350000e+001) (14, 3.34261825977008870000e+001) (15, 2.74657333616933920000e+001) (16, -6.26077571935922490000e+002) (17, 6.56258516145306550000e+000) (18, -8.73066567445948410000e+002) (19, 6.92663766892836640000e+000) (20, 1.71152301739049700000e+002) (21, -3.32513215222237420000e+000) (0, -9.84969371849957010000e+001) (1, 5.00731000664611690000e+000) (2, -9.78842645724328410000e+001) (3, 1.01616754654908290000e+001) (4, 3.07700152190894190000e+002) (5, 6.70375083080907520000e+000) (6, -8.96341413817584400000e+001) (7, 6.64127726431855690000e+000) (8, 1.91800978765274070000e+002) (9, 7.90952436167748820000e+000) (10, 4.42050338527792090000e+001) (11, 6.00166056868631070000e+000) (12, 1.30743784811042700000e+002) (13, -2.33340347600765870000e+000) (14, 1.42697747511072180000e+002) (15, 2.70251169854076200000e+000) (16, 2.10702656110971280000e+002) (17, 6.03259900190951640000e+000) (18, 2.77200587823668910000e+002) (19, 2.94039221872198330000e-001) (20, -1.55664479093604150000e+000) (21, 3.10394982299712070000e+000) (22, -1.67585517119843040000e+000) (23, -1.61486110086234660000e+000) (24, 1.51542734003131610000e+000) (25, 1.44059892478503860000e+000) (26, 1.94987163086887790000e-001) (22, 1.57376200162192230000e+000) (23, 1.44259512009559800000e+000) (24, -2.26725523117230210000e-001) (25, -6.59415123072478290000e-001) (26, 8.03651970586166890000e-001) (22, -5.58894574597537320000e-002) (23, -9.80228627626673030000e-003) (24, -1.02430537736839630000e+000) (25, -3.74206829451824710000e-001) (26, 1.37952295013184620000e+000) 

FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=22 6 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (6, 5, 5.00000000000000000000e-001) (6, 5, 5.00000000000000000000e-001) (6, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -1.23176621670630350000e+000) (1, -6.66906630716539880000e-001) (2, 1.38508531555618260000e+000) (3, -1.09194307033161670000e-001) (4, -3.12028567717966170000e+000) (5, 4.25997006761213830000e-001) (6, -3.11438968099805450000e+000) (7, 1.42035352253670530000e-001) (8, 3.03019778521224750000e+000) (9, 8.16823365575589210000e-001) (10, 1.17720810565620270000e+000) (11, 1.60107862288596910000e-001) (12, 2.28260157615670260000e-001) (13, 4.39272153443403920000e-001) (14, 1.16589829103454830000e+000) (15, 4.42732846854347270000e-001) (16, 1.42193718202379360000e-001) (17, 1.98206007387553510000e-001) (18, -3.13302981799456410000e+000) (19, 1.97905775553546080000e-001) (20, 3.08852298139386060000e+000) (21, -5.12475288259374090000e-002) (0, 1.13878554300176840000e+000) (1, -3.15876668220091830000e-002) (2, -1.50900177191632620000e-001) (3, -1.64039021023442140000e-002) (4, 7.12749393214485490000e-001) (5, -8.74565225968432920000e-002) (6, 9.46407551279353720000e-001) (7, 3.30207741202832860000e-002) (8, 2.31608808899237380000e-001) (9, -5.10047326960617610000e-002) (10, 4.17857833725956360000e-001) (11, -5.01396275095282450000e-002) (12, -6.83004459738374980000e-001) (13, 3.15193092541597160000e-002) (14, 7.19716467823498960000e-001) (15, -6.77665808971903010000e-002) (16, 2.93358291681557750000e-001) (17, 2.44975390918504910000e-002) (18, 3.12630679755605710000e-001) (19, 6.52090292676199560000e-003) (20, 1.83323896070361120000e-001) (21, -5.00476699586286380000e-001) (0, -1.33240661602465370000e+000) (1, 1.55381920979875690000e-002) (2, 2.78822687038063320000e+000) (3, 2.83220352077365680000e-001) (4, -3.13418948598059450000e+000) (5, 5.25480451110912790000e-001) (6, -2.02442985671207280000e-001) (7, 2.53868330715305120000e-001) (8, 1.00811217158550860000e+000) (9, 4.73395779354867870000e-001) (10, 8.25289385412052430000e-001) (11, 2.94416284500365670000e-001) (12, 1.62784098238053120000e-001) (13, 8.88571616532685790000e-001) (14, -5.43521765719843340000e-001) (15, 2.79792260844950740000e-001) (16, -1.06334696706275000000e-001) (17, 3.56757594023073550000e-001) (18, -1.41799458205313830000e+000) (19, 2.15733778806112390000e-001) (20, 3.73816456860430520000e-001) (21, 8.92117759163652440000e-001) (0, -1.33887071048978480000e+000) (1, -3.68339632487584000000e-002) (2, -3.03238284383855740000e+000) (3, -1.23552714540968460000e-001) (4, -7.17232975873419610000e-001) (5, -4.28842340618451610000e-003) (6, -4.11411168048117060000e-001) (7, -2.48640828890277640000e-001) (8, 4.21290829990494780000e-001) (9, 2.99404145047296290000e-001) (10, 3.12412478856499390000e+000) (11, -5.35709265603428020000e-001) (12, 3.06473758064880550000e+000) (13, -2.13643027491810800000e-001) (14, 5.23380841704626180000e-001) (15, -1.06896220152142350000e-001) (16, 4.28260280635421540000e-001) (17, -8.66734617331231110000e-002) (18, -1.76731172256801520000e-001) (19, -1.49460725341096080000e-003) (20, 6.89473547450969340000e-001) (21, -4.14484266664274240000e-001) (0, -3.14965579480481050000e+000) (1, -3.83711324948250220000e-001) (2, -3.03542239466510020000e+000) (3, 1.30641500348351370000e-001) (4, -9.78381193628204300000e-001) (5, 1.75158431769623780000e-002) (6, -1.06894515982146250000e+000) (7, 2.85891620103326460000e-002) (8, -1.58342472552819550000e-001) (9, 5.66916916522525850000e-001) (10, 1.22471176847343740000e+000) (11, 2.65663654608605530000e-003) (12, 3.12326419284904500000e+000) (13, -1.05138351385757870000e+000) (14, 6.60085303894532900000e-001) (15, 2.43382765598582930000e-001) (16, -1.29068345391236370000e+000) (17, 6.13993703123798970000e-001) (18, 3.10531375312843760000e+000) (19, 9.89179469910644540000e-001) (20, 3.39608686687433160000e-002) (21, -2.13536050569482830000e-001) (22, 2.96320923661172460000e-001) (23, -4.15686973992038800000e-002) (24, 4.27430202498052410000e-001) (25, 5.41606254713649160000e-002) (26, 4.78524886252893520000e-001) (27, 3.93248458359248100000e-001) (22, 2.38870364044835220000e-001) (23, -9.70390422708839600000e-002) (24, 4.50654079797240150000e-001) (25, -1.59166728743533600000e-001) (26, -3.62946948041541540000e-001) (27, 3.65259791415391110000e-001) (22, -6.31803962984905820000e-001) (23, -1.10985575681828500000e-001) (24, 3.28931665874059920000e-001) (25, -1.17540751560758230000e-001) (26, 2.42046355178400510000e-002) (27, 3.31244898330404770000e-001) 

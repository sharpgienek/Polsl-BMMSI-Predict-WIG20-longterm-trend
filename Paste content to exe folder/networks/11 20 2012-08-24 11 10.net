FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=22 7 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (7, 5, 5.00000000000000000000e-001) (7, 5, 5.00000000000000000000e-001) (7, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -3.10126712220876310000e+000) (1, -9.89875194030813420000e-002) (2, -9.30123165535000070000e-002) (3, -1.14203065792488250000e-001) (4, -3.13553836295381940000e+000) (5, -7.86331162152618330000e-001) (6, 3.09955203429258130000e+000) (7, -9.90627822433690520000e-001) (8, -3.15065066786651030000e+000) (9, -7.73458971103878850000e-001) (10, 5.84993214245340230000e-001) (11, 1.17245683770615150000e+000) (12, -3.91021238781567860000e-001) (13, -2.25214127163327620000e-001) (14, 2.03464884540374860000e+000) (15, 6.40421539160644700000e-001) (16, 5.87825781333407130000e-001) (17, -3.79273042893126630000e-001) (18, 3.51562346166841120000e-001) (19, -9.26330185356936560000e-001) (20, -9.93955534486209990000e-001) (21, 2.54081711459608230000e-001) (0, -4.82046601324942200000e-001) (1, -1.16071676857166570000e-001) (2, -8.23680630736936260000e-002) (3, -2.34479881746586350000e-001) (4, -9.25884854955322910000e-001) (5, 4.39130386331310240000e-001) (6, 1.07964340693675040000e+000) (7, 5.98724999197308700000e-002) (8, -2.97583977764771860000e-001) (9, 8.61437903446757080000e-001) (10, 2.43868324204785850000e-001) (11, 8.51207227816115210000e-002) (12, -1.32098311676085570000e-003) (13, -3.36168803178864490000e-001) (14, 3.15852816411744560000e-001) (15, 4.45941828881536720000e-001) (16, 2.49700887729777110000e-001) (17, 1.52830482560559590000e-001) (18, -2.56117206593278970000e-001) (19, -7.05791600773217340000e-003) (20, -1.17090324057607060000e+000) (21, 1.14634681210147800000e-001) (0, -3.16805179117268220000e+000) (1, -1.15630351141666050000e-001) (2, -1.15245829651561230000e-001) (3, -2.22974136895574160000e-001) (4, -3.08627078409467530000e+000) (5, -1.04552597001936440000e+000) (6, 3.11508548178967270000e+000) (7, -2.24032115225367260000e+000) (8, -5.46917615256689140000e-001) (9, -1.27466300774721160000e+000) (10, 5.21579479100877760000e-001) (11, 1.01760779207241960000e-001) (12, -1.04612709102648580000e-001) (13, 4.21838508296819790000e-001) (14, 5.50096332143955370000e-001) (15, 2.15646278951143130000e-001) (16, 4.21224571973448790000e-002) (17, 1.50784586340496460000e-001) (18, 5.21031798179321550000e-001) (19, -1.99593809822175590000e-001) (20, -5.07118162528143230000e-001) (21, -7.67753700939317190000e-002) (0, 5.02824327922395950000e-001) (1, 2.70921796380188820000e-001) (2, 3.36296553213723590000e-001) (3, 2.38553391397433420000e-001) (4, 1.59508120512564490000e+000) (5, 5.76963733324034170000e-001) (6, -3.17804934991540740000e+000) (7, 1.79854279228958410000e-001) (8, 5.54154895093531000000e-001) (9, 3.89408777453834340000e-001) (10, 2.37194655227290640000e-001) (11, 7.05751413180649020000e-003) (12, 3.27378831692445420000e-001) (13, 2.99845545554779320000e-001) (14, -2.07145669614883730000e-001) (15, 7.70587358780309570000e-002) (16, 1.79858639219564080000e-001) (17, 2.86540664626842190000e-001) (18, 1.33235494527629890000e-001) (19, 5.53556750553367950000e-001) (20, 6.21394591200236990000e-001) (21, 3.22130264766543550000e-001) (0, -4.25243043467619540000e-001) (1, 1.78215253416111410000e-001) (2, 5.16206801001713790000e-001) (3, 3.96186438452163000000e-001) (4, 2.35090228823507250000e-002) (5, 1.15231186505084480000e+000) (6, 6.01522070599997740000e-001) (7, 5.36710783228022680000e-001) (8, -3.09476687797127690000e+000) (9, 5.73623797532838870000e-001) (10, -4.72450263011435280000e-001) (11, 8.29658867153720060000e-001) (12, -2.82287434931199340000e-001) (13, 2.55734788137831180000e-001) (14, 1.80326896532133270000e+000) (15, 1.32386923147788150000e+000) (16, -7.00422842603063420000e-002) (17, 2.82648839104071070000e-001) (18, 6.65994165172921320000e-001) (19, -5.93092856708681730000e-001) (20, 2.47763596612801170000e-001) (21, 7.42209013109646380000e-001) (0, -3.19443118609701580000e+000) (1, -1.19699060213895870000e-003) (2, -6.55867726739449990000e-002) (3, 2.97650356250981740000e-001) (4, -3.05442263908885090000e+000) (5, -7.54617294492544890000e-001) (6, 3.18452518009002980000e+000) (7, -2.19129161050745050000e+000) (8, -3.20166385222041590000e+000) (9, -7.36019882106046850000e-001) (10, 1.07410157424713490000e+000) (11, 1.07791337926609420000e+000) (12, -1.43581962269258410000e+000) (13, 2.99727725642438270000e-001) (14, 1.92131266207428290000e+000) (15, -4.20396851989155660000e-001) (16, 5.77468848236879380000e-001) (17, 4.19044298704268570000e-001) (18, -1.05100948730133950000e-001) (19, -1.02746864329516140000e+000) (20, -7.60702499661346510000e-001) (21, 8.62063323729353790000e-001) (22, -3.25675685514569320000e-001) (23, -3.27186238897065500000e-001) (24, 4.60717903407417920000e-001) (25, 3.35980800225925470000e-001) (26, 1.15217008662054340000e-001) (27, 2.13524861185026700000e-002) (28, -1.49880183760949780000e-001) (22, -6.94483235338422160000e-001) (23, 1.55485616285850110000e-001) (24, -9.48042433940973540000e-001) (25, 3.85199977259398030000e-001) (26, 4.34229519005620120000e-001) (27, -2.37483543144343220000e-001) (28, 4.28614284546946610000e-001) (22, 1.19441728725213770000e+000) (23, 4.82329347567863110000e-001) (24, 2.96428125474103300000e-001) (25, 1.00187918210445330000e-001) (26, 4.57123180049886340000e-001) (27, 4.34466929712281900000e-001) (28, 7.19857938557839420000e-001) 

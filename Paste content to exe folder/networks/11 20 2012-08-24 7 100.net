FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=22 5 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -8.92334166896880720000e-001) (1, -4.20436926364929630000e-001) (2, -3.57592621341094150000e-001) (3, 4.01411095735684060000e-001) (4, -1.14036400291793110000e+001) (5, 2.56477124080896960000e-001) (6, 3.70955018731239220000e+001) (7, -2.57630252325454110000e-001) (8, -1.56675250493589060000e+001) (9, -1.45698387424784580000e-001) (10, -4.18944891834563790000e-001) (11, 1.82993263378478390000e+000) (12, 2.39141835913215850000e+000) (13, -3.40328123760544330000e-001) (14, 4.30775726556950200000e+000) (15, -7.18888541586105020000e-002) (16, 7.60990431852261030000e+000) (17, -1.94094694644835530000e-001) (18, -2.10345058701169730000e-001) (19, -2.17924309744552190000e+000) (20, -9.68653490338047260000e+000) (21, 1.27198710860149330000e+000) (0, -1.00943393242375100000e-001) (1, -3.79470045181774020000e-001) (2, -1.18060460338328460000e+001) (3, -2.39454845898910800000e-002) (4, -3.86721402092032340000e+000) (5, 1.12972811819768170000e+000) (6, 2.75024030630802230000e+001) (7, 2.31705168121602730000e+000) (8, 1.42731321828815830000e+000) (9, 1.57707119797017440000e+000) (10, 6.68730684023567610000e+000) (11, -1.36154476639801510000e+000) (12, 4.60201812277605080000e+000) (13, -6.97713478766673780000e-001) (14, 3.08569731070422460000e-002) (15, -9.80110823232004490000e-001) (16, -5.94804675952226880000e+000) (17, -1.46865928984910490000e-001) (18, 1.07228608217715470000e+000) (19, 2.35978139726664480000e+000) (20, -1.39524333365951260000e+001) (21, -3.18105868658345450000e-001) (0, -9.16594047694614920000e+000) (1, 8.51474107375055020000e-001) (2, -3.18375492800481550000e-001) (3, -1.20920993622213470000e-001) (4, -2.63364560295906990000e-001) (5, 1.42853542663183420000e+000) (6, 1.03896264692488290000e+000) (7, 8.81289580673919600000e-001) (8, -5.17287450079200100000e+000) (9, 1.90367507940979030000e+000) (10, 3.13925226593310700000e+000) (11, 1.15833912618020080000e-001) (12, -3.93786348410535890000e-001) (13, -6.99455677905389650000e-001) (14, 3.14241108854806490000e-001) (15, 4.92329605182355690000e-001) (16, 6.17924270645477680000e+000) (17, 8.46522613445764520000e-001) (18, -6.18411942008771300000e+000) (19, -3.68462574772454720000e+000) (20, -1.64785191466257070000e+001) (21, 2.29849307923252870000e+000) (0, 3.56305302351868110000e+001) (1, 8.36088756065617170000e-001) (2, 3.26336856276773930000e+000) (3, 1.48178571231879100000e+000) (4, 3.41317569231120910000e+000) (5, 4.83412611826539500000e-001) (6, -7.15123050689387530000e-001) (7, 2.10594590537402170000e+000) (8, 9.95729067502190190000e+000) (9, 2.24737201375075880000e-001) (10, 6.54179179318160120000e+000) (11, -2.41152935213464310000e-001) (12, 1.56148232361731960000e+000) (13, 5.49349599366859450000e-001) (14, -1.26427939252479520000e+001) (15, -3.01826274144566110000e-002) (16, 3.53345644156375930000e-001) (17, -8.81962021520239730000e-001) (18, 3.83877613543130640000e+000) (19, 3.03695869507407500000e+000) (20, 2.64515367759157200000e+001) (21, 9.72166976308656140000e-002) (22, -1.07321517145284040000e+000) (23, -5.77491878050834680000e-001) (24, -1.04664383012366180000e+000) (25, -7.52434956909340370000e-001) (26, 8.68485186868282580000e-001) (22, -5.08119644372588830000e-001) (23, 1.04039790971716230000e+000) (24, 2.06855736768220840000e+000) (25, 1.76051256564804400000e+000) (26, 5.27855531580121770000e-001) (22, 2.40441601087704180000e+000) (23, -4.26403915305456570000e-001) (24, -6.66116569550111560000e-001) (25, -7.49032941629110720000e-001) (26, 2.00128668010016360000e+000) 

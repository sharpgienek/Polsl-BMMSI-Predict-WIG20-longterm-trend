FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=22 6 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (6, 5, 5.00000000000000000000e-001) (6, 5, 5.00000000000000000000e-001) (6, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -3.73808940288421130000e+002) (1, -8.87672881397645890000e-001) (2, 1.08477538000562890000e+002) (3, -2.90650597456128000000e+001) (4, 7.12014267466034990000e+002) (5, 5.89772673738955260000e+000) (6, 4.00294228634618780000e+002) (7, -1.09859809624735460000e+002) (8, 1.45405761968561410000e+003) (9, 3.98740609389746050000e+000) (10, -7.74699678830193530000e+002) (11, -2.35516325781469220000e+001) (12, 1.50000000000000000000e+003) (13, 3.76660238461655300000e+001) (14, 1.49991583126765020000e+003) (15, 2.63014674663233670000e+000) (16, -2.69545656619381130000e+002) (17, -2.40909760726873900000e+001) (18, -1.37428361577513210000e+003) (19, -4.90867827617574550000e+001) (20, -1.50000000000000000000e+003) (21, -1.52396705313859310000e+002) (0, -1.50000000000000000000e+003) (1, 9.57957517294756260000e+002) (2, 1.50000000000000000000e+003) (3, -5.89807312915136210000e+002) (4, 1.50000000000000000000e+003) (5, 1.50000000000000000000e+003) (6, 1.50000000000000000000e+003) (7, -1.50000000000000000000e+003) (8, 1.50000000000000000000e+003) (9, 7.93888383076976200000e+002) (10, -1.50000000000000000000e+003) (11, 7.19664278512797640000e+002) (12, 1.50000000000000000000e+003) (13, 9.56190577247575110000e+002) (14, -1.50000000000000000000e+003) (15, -1.50000000000000000000e+003) (16, 1.50000000000000000000e+003) (17, -6.54592860366034300000e+002) (18, -1.50000000000000000000e+003) (19, -4.03904151925141380000e+002) (20, -2.03572092558290930000e+002) (21, 2.93767232239949370000e+001) (0, -4.31170796295590360000e+001) (1, -7.15438664770062080000e+000) (2, -3.03928870337328140000e+002) (3, 3.94466283746069660000e+001) (4, -9.14676032731594890000e+002) (5, 2.36406701981698400000e+001) (6, -6.41065952274482240000e+002) (7, 1.39405976767832080000e+001) (8, 2.26607061007046010000e+002) (9, 4.03260820426817330000e+001) (10, 1.50000000000000000000e+003) (11, 1.86168474502726620000e+000) (12, -6.10431364254753030000e+001) (13, 2.16760116861930000000e+001) (14, -2.68470888109181090000e+002) (15, -4.46856633837237370000e+000) (16, -2.29568922647959310000e+002) (17, 1.68775017851411380000e+000) (18, -8.46180273364441720000e+001) (19, 4.05934015445198800000e-001) (20, 4.52500665551989700000e+001) (21, 3.66350033421378360000e+001) (0, -3.05607493984249570000e+002) (1, 3.30272337962523040000e+001) (2, -2.54705415294298600000e+002) (3, -7.62850498235490890000e-001) (4, 3.31111080911634020000e+001) (5, 2.70131977431736560000e+001) (6, 2.93904552945485930000e+002) (7, 4.21080891565294450000e+001) (8, 1.50000000000000000000e+003) (9, 3.87478192203910370000e+001) (10, -7.75189286674733350000e+002) (11, 1.93854071835549710000e+001) (12, 2.80217085604572450000e+002) (13, 2.81509237140434010000e+001) (14, 6.67634948800831810000e+002) (15, 7.11527505854961790000e+001) (16, 1.20929373973075780000e+001) (17, 1.24603289392869470000e+001) (18, 1.15515182095380110000e+002) (19, 1.36024180639506160000e+000) (20, 4.44897615426217850000e+001) (21, -1.94953623136389850000e+001) (0, -9.23389427176005740000e+001) (1, -2.11087325349998920000e+000) (2, -1.62098284842516360000e+002) (3, 1.63034097862953490000e+000) (4, -3.46792161442287110000e+001) (5, 9.07755474472958750000e+000) (6, -1.53153195200659550000e+002) (7, -1.51455923310453140000e+000) (8, -1.34236527486103060000e+002) (9, 3.76547603583011940000e+000) (10, 5.74892083838653390000e+001) (11, -2.10101886057070610000e+000) (12, 9.69752405617947350000e+001) (13, -9.76205434344947780000e-001) (14, -2.61657056139228710000e+001) (15, -6.51448240125308600000e+000) (16, 1.15360717518003300000e+002) (17, 2.99438205988978410000e+000) (18, 3.71610827474106560000e+000) (19, -1.28720717641953200000e-002) (20, -1.56014448971413510000e+002) (21, 2.43704655987140000000e+000) (22, -6.83875256289942950000e-001) (23, -2.35370719700724460000e-001) (24, 6.98238083562245480000e-001) (25, 8.14529172495920380000e-001) (26, 4.77142977646085820000e-001) (27, -6.48427410678779850000e-001) (22, 8.35307614907744300000e-001) (23, 2.78567563468259500000e-001) (24, 6.15083109342264960000e-001) (25, -3.37968777600366370000e-001) (26, -7.10790967265207190000e-001) (27, 1.26390280343153830000e+000) (22, -8.09872085027682070000e-002) (23, -6.10983528148890120000e-002) (24, -1.99758380879744620000e+000) (25, -4.16537740280117420000e-001) (26, 2.42534851767005010000e-001) (27, 2.33395810995241780000e+000) 

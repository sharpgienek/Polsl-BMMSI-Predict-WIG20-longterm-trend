FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=22 5 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 1.53899835870775160000e+000) (1, 1.38496241462701950000e-001) (2, -5.86213316923474980000e+000) (3, -1.28432247596043660000e-001) (4, 7.82370620764168920000e+000) (5, 1.39729035627830790000e+000) (6, 5.68972744393759060000e+001) (7, 1.37555382493495020000e+000) (8, -2.04689380346915540000e+001) (9, 5.44343477138934160000e-001) (10, -5.35918214076947490000e+000) (11, 1.42362768695486610000e+000) (12, 1.35650027037356420000e+001) (13, -8.09895631156482000000e-001) (14, 1.08646280833107750000e+001) (15, 9.61559716775368440000e-001) (16, -2.49308481452489380000e+000) (17, -3.86309787217763820000e-001) (18, -1.05682458022053700000e+001) (19, 5.58463510307218370000e-002) (20, -1.43112192938356820000e+001) (21, 3.83324646359270460000e+000) (0, -1.14752858654926880000e+002) (1, -2.74227055320333080000e+002) (2, -7.82908408520617770000e+001) (3, -2.91073823443952410000e+001) (4, 1.45178628152511290000e+002) (5, -1.10831742771062000000e+002) (6, -1.15448411659857360000e+003) (7, 1.47277320234937950000e+003) (8, -1.49867712396499840000e+003) (9, 1.04647035263848930000e+001) (10, -1.45892223066393370000e+002) (11, 1.10822076005340520000e+003) (12, 1.49873442991932070000e+003) (13, 1.49919378401966150000e+003) (14, -1.49709762228336900000e+003) (15, -5.00755290222674460000e+001) (16, 1.50000000000000000000e+003) (17, -3.07164730144783790000e+002) (18, 1.49318602977192880000e+003) (19, 6.00602944029785420000e+001) (20, 1.48896631931798970000e+003) (21, 6.45280501597677360000e+002) (0, -3.29721489595986840000e+002) (1, -1.16258687453103210000e+001) (2, -2.10731798217828780000e+002) (3, -3.86586792998832610000e+002) (4, -1.49961193154030180000e+003) (5, 6.99287440542431330000e+000) (6, 5.15665038427843460000e+002) (7, 1.34649485174029550000e+000) (8, 1.07019191084289730000e+001) (9, -5.91386228305152970000e+000) (10, 1.02584769703968720000e+001) (11, 8.65890335045644120000e+001) (12, -3.96199948163147160000e+000) (13, 2.55031251340244270000e+000) (14, 6.72361624451211810000e+001) (15, -1.95437197940286240000e+000) (16, -3.50197155402587840000e+002) (17, -1.08584097582831230000e+002) (18, 2.75039038672059630000e+002) (19, 8.89122305780129380000e+001) (20, -6.66679760474683720000e+001) (21, -1.00416966431015100000e+001) (0, 2.78784269578247120000e+002) (1, 3.42064830964673660000e+000) (2, -3.00565811627251110000e+000) (3, -2.24384467512457290000e+000) (4, 5.80232130813181190000e+001) (5, 5.91471956313512090000e+001) (6, -6.92842974319719790000e+001) (7, -5.11989868713824380000e+001) (8, 2.02904280057319570000e+002) (9, 3.49131185575716700000e+000) (10, -6.80026883922285120000e+000) (11, -2.64882474338888230000e+000) (12, -7.41743837131035550000e+000) (13, -5.75132090011669420000e+001) (14, 5.00520406927973060000e+001) (15, 4.94697263428280960000e+001) (16, 1.89022555325305060000e+002) (17, -2.94480764014590250000e+000) (18, -2.90530711664192720000e+002) (19, 4.02648263261414030000e+000) (20, 5.80645859009531320000e+000) (21, -1.85817979852425120000e-001) (22, -8.10115570920168500000e-001) (23, -8.69108847097474490000e-003) (24, 4.01176727909193510000e-002) (25, -2.34423185720040830000e-002) (26, 7.88969375732058940000e-001) (22, -4.28131884214805300000e-001) (23, -1.65416724601461680000e+000) (24, -2.85402006173572610000e+000) (25, 2.89191146410388190000e+000) (26, 1.96945137502631430000e+000) (22, 3.00804829360228300000e+000) (23, 1.93547546384009220000e+000) (24, 3.65570941979286520000e+000) (25, -4.27275032158154740000e+000) (26, 4.10940044689216740000e-001) 

FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=22 7 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (7, 5, 5.00000000000000000000e-001) (7, 5, 5.00000000000000000000e-001) (7, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 3.81675929478769550000e-001) (1, 6.17281348178579270000e-001) (2, 9.19692462990642890000e-001) (3, -3.14134890991540330000e+000) (4, 7.60366067107582700000e-001) (5, 1.99302004766554300000e-001) (6, -1.47577008335265240000e+000) (7, 4.75906043549990720000e-001) (8, 3.14900882213891990000e+000) (9, 1.61458233544676560000e-001) (10, 1.47855361615017600000e+000) (11, -5.58535303075072930000e-001) (12, 1.10369462965026480000e+000) (13, 1.59476739757370200000e-001) (14, 3.75645896863435190000e-002) (15, -4.37010012590144830000e-001) (16, -1.74626841536732470000e-001) (17, -1.30069688966102690000e+000) (18, 4.39555237818459510000e-001) (19, -5.63369287522104910000e-001) (20, 3.11304080402482960000e+000) (21, -5.44724857945221070000e-001) (0, -1.49608298709772010000e+000) (1, -5.53127754723538450000e-002) (2, -1.95332481669408840000e-001) (3, -1.56387739492526520000e-001) (4, -1.48685102658998290000e-001) (5, 4.23972962195640510000e-001) (6, 6.59694678888712140000e-001) (7, -3.51639637927830540000e-001) (8, 3.55737404317201130000e-001) (9, -5.00907369065856530000e-001) (10, 1.32022359555520840000e+000) (11, 1.10059170899502070000e-001) (12, 7.20455305250698560000e-001) (13, -4.21372748384959580000e-001) (14, 1.27769600312101560000e+000) (15, -3.61329570483410020000e-001) (16, 2.91636906977867790000e-002) (17, -2.41905965981695860000e-001) (18, 5.12033187533456060000e-001) (19, -2.55962334205066140000e-001) (20, 6.95902490047841550000e-001) (21, -8.80447541933479630000e-001) (0, -3.09095218064880850000e+000) (1, 3.17271583492069670000e-002) (2, -9.89269865665458160000e-001) (3, 2.03182910197003770000e-001) (4, 1.62870459694721210000e-002) (5, -7.36141952420365840000e-001) (6, 1.44792256751506640000e-001) (7, -3.79181766332435940000e-001) (8, -1.25868272101364060000e+000) (9, -4.07895639669852090000e-001) (10, 4.19888721657528250000e-002) (11, 9.26718387043290900000e-001) (12, -2.81808706944182360000e-001) (13, -3.81099633072949920000e-002) (14, 1.17131821299031150000e+000) (15, 9.75982582887980790000e-001) (16, -6.65104733653178020000e-001) (17, -6.50929380483340150000e-001) (18, 2.47883502137031540000e+000) (19, 8.88082167517733060000e-002) (20, -3.06808841077686180000e+000) (21, 1.77236864888502570000e-001) (0, 2.94116453422138100000e-001) (1, -2.13406573857217380000e-002) (2, -4.74350333052298900000e-001) (3, -8.35740256607891830000e-002) (4, -1.26603811458232930000e-001) (5, -3.36518819119481340000e-001) (6, 1.34256323179012980000e-001) (7, -2.65064761473883250000e-002) (8, -9.19244229856369890000e-001) (9, 2.07293942362756890000e-001) (10, 1.78419821372803820000e-001) (11, 4.25824745470619880000e-001) (12, -4.42215057762655960000e-001) (13, -3.07736160057118900000e-001) (14, 2.78468761805605150000e-001) (15, 5.81919788085385140000e-001) (16, 4.67735730674070260000e-001) (17, 2.06195000860052300000e-003) (18, -1.89686729632155580000e-001) (19, 6.74349787027632990000e-002) (20, -1.07505477905312800000e+000) (21, 2.57544187252168880000e-001) (0, 1.05305649247807450000e+000) (1, 6.67002288839266130000e-002) (2, 2.63420679284468990000e-001) (3, 4.36307178933432320000e-002) (4, 1.45006587686280720000e-001) (5, 1.18521318555248590000e+000) (6, 3.03127999932962240000e-001) (7, 9.53312157734823360000e-001) (8, 5.51927737276834970000e-001) (9, 4.41956337507874330000e-001) (10, -1.26590756419724670000e-002) (11, -4.49403694478936470000e-002) (12, 4.13837528021373300000e-001) (13, -2.39838046026256860000e-001) (14, -1.01073593043845240000e+000) (15, -1.03157709882210350000e-002) (16, 8.92175208592278610000e-001) (17, 8.71609919794948670000e-002) (18, -1.59229878386922310000e+000) (19, -1.72766319380048830000e-002) (20, 1.93596183286600000000e+000) (21, 4.17406517145132020000e-002) (0, 4.70157725347725540000e-001) (1, -1.61627983809357640000e-001) (2, -9.56643138062264200000e-002) (3, 8.64789763420067970000e-003) (4, -5.75181128744249870000e-001) (5, -2.06634255073150420000e-001) (6, 3.01529910334961480000e+000) (7, 5.40417212086840680000e-001) (8, -1.15744980934890270000e+000) (9, 3.13143646803199340000e-001) (10, -2.28010858282728320000e-001) (11, 2.46302627998808810000e-001) (12, 1.27606379861688710000e-001) (13, -1.52206087463051670000e-001) (14, 2.04195099904164960000e-001) (15, 2.47529434657714600000e-001) (16, 3.13313810814832170000e-001) (17, 1.33634619415323240000e-002) (18, -1.77040245068708270000e-001) (19, 1.40693495565735130000e-001) (20, -7.63113103510211780000e-001) (21, 1.35658960712120640000e-001) (22, -2.22111996870504870000e-002) (23, -1.58488716743837690000e-001) (24, 4.56164295331937390000e-002) (25, -3.11159295635628890000e-001) (26, -1.73051242287985190000e-001) (27, -4.07365179816855550000e-001) (28, 1.91385744882967830000e-001) (22, 1.39608580302074290000e-001) (23, -2.46278536575413040000e-001) (24, -4.98647509079754140000e-001) (25, 1.10604064904815170000e-001) (26, 8.78387366589948760000e-001) (27, -4.27992672303157130000e-002) (28, 5.35023332809201670000e-001) (22, -6.71466584107500490000e-001) (23, -4.82725345302606410000e-001) (24, 1.18475205108111630000e+000) (25, 4.63467006930461780000e-001) (26, -2.33177721152384440000e-001) (27, 3.07823684527577410000e-001) (28, 8.60800924234718100000e-001) 

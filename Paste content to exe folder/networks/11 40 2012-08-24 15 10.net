FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=22 5 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -3.10198733706324510000e+000) (1, -8.01857614258935650000e-003) (2, -1.78398245823780140000e-001) (3, 1.75992898168466950000e-002) (4, -9.50604156630516230000e-001) (5, -1.65776451594309430000e-002) (6, 3.80945594411243430000e-001) (7, -3.03849731575715380000e-001) (8, 7.00875099424223210000e-001) (9, 4.25381087608012630000e-001) (10, 3.17890994563135630000e+000) (11, 1.26329829595475280000e-001) (12, 2.19575910191947390000e-001) (13, 6.10991311114066020000e-001) (14, 6.53734379418872020000e-001) (15, 6.94224309088950080000e-002) (16, 4.70417921539163510000e-002) (17, 9.79586065559812790000e-002) (18, 3.05080325426929290000e+000) (19, 5.51542220170837820000e-002) (20, -5.94929623789503760000e-001) (21, -1.57397674984723980000e-001) (0, 3.02222659368721750000e+000) (1, 1.18993186897956700000e-001) (2, 9.95509785279528740000e-002) (3, 3.78417416060767680000e-001) (4, 3.13010013045090840000e+000) (5, 3.03724124234845320000e+000) (6, -3.28384754705367320000e-001) (7, 4.16396027193250580000e-001) (8, 4.88890626962508790000e-001) (9, 1.88268090273983200000e-001) (10, -1.19647568883668360000e+000) (11, -2.90925556721338800000e-001) (12, 6.50566654762294250000e-001) (13, 2.79229581417123900000e-001) (14, -3.09792239559642280000e+000) (15, -1.47870761619496920000e-001) (16, 6.13359622957866280000e-001) (17, 9.15657696520970900000e-002) (18, -1.28997178636406980000e+000) (19, -5.57860715977271800000e-002) (20, 6.43779545286158440000e-001) (21, 5.14551015430630090000e-001) (0, 3.02638308788977910000e+000) (1, 2.03609287902916410000e+000) (2, -3.66843007231564930000e-001) (3, 3.44313567136139640000e-001) (4, 3.18167483825068140000e+000) (5, 2.43162432730744670000e+000) (6, -4.97505860289705610000e-002) (7, -7.13876042229647400000e-001) (8, 4.02471996916950070000e-001) (9, -3.67012694282705940000e-002) (10, -3.14346072635312180000e+000) (11, 5.76442772475755590000e-001) (12, 5.88243655786734320000e-001) (13, -2.02924473377361140000e-001) (14, -8.29278171051854970000e-001) (15, 3.82167571606458260000e-001) (16, 3.96221918916596750000e-001) (17, 8.49228904230737050000e-001) (18, -2.70289570085713580000e-001) (19, 1.16552201952181690000e-001) (20, 3.75537104155243990000e-001) (21, 2.29139151174298140000e-001) (0, 8.89821688539875750000e-001) (1, 9.75858632743645120000e-002) (2, 2.62718774977134810000e-001) (3, -2.25880685090432650000e-002) (4, 1.03817552820348920000e-001) (5, 1.35371542739198250000e+000) (6, -1.15644272925577110000e+000) (7, 3.91343349652577720000e-001) (8, 3.27488916952570150000e-001) (9, 8.18006307354232960000e-001) (10, 3.09598147759875090000e+000) (11, -1.35119615251075900000e+000) (12, 2.88694206759307850000e-001) (13, 4.01714934804797920000e-001) (14, -1.54831684962009120000e+000) (15, -5.82138893905577850000e-002) (16, 6.03760626554572970000e-001) (17, 6.23458035168126150000e-001) (18, -3.17118289473496430000e+000) (19, -4.28346849963524220000e-002) (20, 3.01782595885601790000e+000) (21, 8.69151069508061900000e-002) (22, 8.36373619961420010000e-001) (23, -4.36453338926799660000e-001) (24, -4.65518280766151040000e-001) (25, 3.89850229828691270000e-001) (26, 3.47152927683663400000e-001) (22, -5.00528046912837250000e-001) (23, 4.15436330527489960000e-001) (24, 2.91130207421216370000e-001) (25, 5.68537709135018780000e-001) (26, 3.58120945425224600000e-001) (22, -6.48895041775318650000e-002) (23, -2.06169107100331820000e-001) (24, 6.21380231192122600000e-001) (25, -7.41638077142625200000e-001) (26, 1.32263632536636780000e+000) 

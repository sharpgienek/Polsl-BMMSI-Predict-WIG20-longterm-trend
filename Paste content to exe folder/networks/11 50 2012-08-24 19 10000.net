FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=22 5 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -1.11453210595529630000e+001) (1, -2.96964775337811520000e-001) (2, 4.65245500365676360000e+000) (3, -2.52939968869707540000e-001) (4, -5.03796010133764580000e+000) (5, -2.05603685788233160000e+000) (6, 1.46387276208295990000e+001) (7, -3.08407663203595140000e-001) (8, 3.49609380571826800000e+001) (9, -2.49272848106015550000e-001) (10, -8.10740000924174350000e+000) (11, 7.38813642752490510000e-001) (12, -1.65234416521316130000e+001) (13, -1.06094403185059070000e+000) (14, -1.27376516255926260000e+001) (15, 1.35186688551517720000e+000) (16, -3.55261618247351480000e+001) (17, -8.38105648318946630000e-001) (18, -1.37924763213744550000e+001) (19, 2.40014013585584740000e+000) (20, -2.56000778965823310000e+001) (21, 1.56010912687123150000e+000) (0, 5.60192780275929960000e+001) (1, 8.45430485672281120000e+001) (2, 5.67302218744244660000e+001) (3, 4.46613500904224300000e-001) (4, 1.12796954106003170000e+000) (5, 1.30101452988869980000e+001) (6, -8.32253461607041340000e+000) (7, -2.32231703011978870000e-001) (8, -2.84294664228939990000e+001) (9, 2.69965916269378290000e+000) (10, -6.40136399198866230000e+002) (11, 6.35910420811228620000e-001) (12, -1.92917659635591010000e+002) (13, 4.91258676564988170000e-001) (14, -2.83853046166554360000e+000) (15, 5.13556281709679040000e+001) (16, -2.55569502183635960000e+001) (17, 1.16754187182676210000e+000) (18, -2.26475891298903280000e+001) (19, -9.89687528844855780000e-001) (20, -2.31602035109242620000e+000) (21, 2.25419456809128210000e+001) (0, -4.65821319318964560000e+001) (1, 2.24459276836041430000e+000) (2, -7.55830056495354650000e+001) (3, -8.59389608353045680000e-001) (4, 1.38453306309595270000e+000) (5, 6.99006595974683060000e-001) (6, -8.34169053754015980000e+000) (7, 1.06972915712746360000e+000) (8, -7.92597894077072600000e+000) (9, 9.79501765303899760000e-001) (10, 1.41486803253193090000e+000) (11, -5.67628151161840400000e-001) (12, 7.21799646035914400000e+000) (13, -2.57229980935760770000e-001) (14, -1.91343074815717390000e+001) (15, -3.86939652076944470000e-001) (16, -1.89231498700088280000e+001) (17, -2.49951999793695760000e-001) (18, 1.97333156132195990000e+001) (19, 2.94321626757767410000e-001) (20, 3.71293639969971000000e+001) (21, 4.47847507995848380000e+000) (0, 4.70128590249832620000e+000) (1, 7.82416897458590630000e+001) (2, 1.78958528889821080000e+002) (3, 5.73323906382325890000e+000) (4, -1.15908618457186490000e+001) (5, 6.14847300898182430000e+000) (6, 3.80940624936156810000e+001) (7, -5.42531036665680810000e-001) (8, 3.34257617911024880000e+001) (9, 1.89273179722650870000e+001) (10, 3.56137264373118570000e+002) (11, 1.29058431337118010000e+000) (12, 1.00090320245187140000e+002) (13, -5.18423413511060720000e+001) (14, -2.60969539960362400000e+002) (15, 1.45734493388486300000e+002) (16, -1.18453257937951110000e+002) (17, 3.92860508978721710000e-001) (18, 5.40592162745823690000e+001) (19, 3.78222310393892090000e+001) (20, 1.33666924778914940000e+001) (21, 3.02754921393519670000e+000) (22, -7.22552536339184090000e-003) (23, -2.37578475928810960000e+000) (24, 2.88373773412377910000e+000) (25, -3.44475841484475700000e-002) (26, -4.23470813985957100000e-001) (22, -2.84281117418883160000e+000) (23, 2.49207640117440030000e-002) (24, -3.51413315218285850000e+000) (25, 2.77029108167406330000e+000) (26, 3.47517457356118160000e+000) (22, 3.72977550950177510000e+000) (23, 2.99731913645444870000e+000) (24, 1.66445307437223170000e+000) (25, -3.32325877731487920000e+000) (26, 1.03226642086883610000e+000) 

FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=22 5 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -1.25673126124503450000e+002) (1, 5.71992409224383990000e-001) (2, -1.36121260783787280000e+002) (3, 1.84126333348964310000e-001) (4, -6.41865963634481890000e+001) (5, -6.11769905232038180000e+000) (6, 2.70241572807772460000e+002) (7, -6.63040853984099600000e+000) (8, 7.57810361357342120000e+001) (9, 1.67633609088903260000e-001) (10, -3.60803667839234090000e+001) (11, 4.80414305739604860000e+000) (12, -6.25752436181774100000e+001) (13, -1.24328969853562050000e+000) (14, -1.49095977069234320000e+002) (15, 3.66226509648415800000e-001) (16, -8.79249220403568050000e+001) (17, -3.85693515227833790000e+000) (18, 6.11754698567137820000e+001) (19, 6.49757340625133020000e-001) (20, 1.51729253198587400000e+002) (21, 1.26352416323120170000e+001) (0, -3.49924614431541270000e+001) (1, 9.41163188891339760000e-001) (2, 9.30536826832072140000e+001) (3, 1.32302278733160380000e+000) (4, 7.40627755779907200000e+001) (5, 4.56961784924962800000e+000) (6, -8.32220778852560700000e+001) (7, 4.78488152376967250000e+000) (8, 4.25995779707778370000e+001) (9, -3.95713282687886640000e-001) (10, 4.78127998386768600000e+001) (11, -1.61255824374836630000e-001) (12, 6.32339560767926020000e+001) (13, 1.64427913417691120000e+000) (14, 1.11936418594351910000e+002) (15, 7.57710579882298950000e-001) (16, 8.81477570427948510000e+001) (17, 1.55973007883799690000e+000) (18, 3.60395401909253080000e+001) (19, -3.37424996414744800000e-002) (20, 1.20356674005265660000e+002) (21, -7.06907663092519380000e+000) (0, 5.15009240767412900000e+000) (1, 6.64723058515225840000e-001) (2, 2.52097354317608870000e+001) (3, 3.67353339898340070000e-003) (4, 4.01744498008785200000e+001) (5, 2.31387430642331890000e+000) (6, -7.68427767615665120000e+001) (7, 2.84667798241611970000e+000) (8, -6.51496027942258250000e-001) (9, -1.11621490339099600000e-001) (10, 4.04442646131513470000e+001) (11, -1.77946272324315460000e+000) (12, 7.10582022217225530000e+001) (13, 1.73088119336882200000e+000) (14, 7.94807087932407370000e+001) (15, 7.87190171639592930000e-001) (16, 5.22947916014822310000e+001) (17, 1.91082554256906920000e+000) (18, 4.70571089645492790000e+000) (19, 2.34295993145727550000e-001) (20, -2.03671555612832260000e+001) (21, -5.58703166578234890000e+000) (0, 1.91868274944776050000e+001) (1, 3.92263737184793230000e+002) (2, 4.90707763378117760000e+002) (3, 2.25776904245804210000e+001) (4, 1.96508300106060600000e+002) (5, 4.06575058070346170000e+001) (6, 1.51978786233380360000e+002) (7, -4.65713998351803580000e+000) (8, -5.53746527904059600000e+000) (9, -1.02752177892572620000e+001) (10, -2.21914663453619820000e+002) (11, -2.79447194866243360000e+000) (12, 9.38960048084732080000e+001) (13, 1.27756691792331090000e+001) (14, -4.59653210463453210000e+002) (15, 4.80537019834878440000e+000) (16, -1.47500000000000000000e+003) (17, -1.62146774895554660000e+001) (18, -2.48189741338755340000e+002) (19, -3.46032008844287460000e+000) (20, 3.70717731897768700000e+001) (21, 5.01305945790262370000e+001) (22, 3.10930701617001540000e+000) (23, -1.82885754422676460000e-001) (24, 3.31159824386740050000e+000) (25, -1.74858294010336170000e+000) (26, 1.70953299293465520000e+000) (22, -7.02666395194231260000e+000) (23, 2.37115351732715180000e+000) (24, -2.50198612187359170000e+000) (25, -2.72280841094499700000e-002) (26, 6.86750234729794150000e+000) (22, 1.59787235720025810000e-002) (23, -1.93079360490549410000e+000) (24, 8.73005516574141690000e-002) (25, 1.85223040139518160000e+000) (26, -2.05207469068109170000e-002) 

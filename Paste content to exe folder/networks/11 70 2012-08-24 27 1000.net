FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=22 5 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 7.59279063336032680000e+000) (1, 1.19237782118517990000e+001) (2, 9.82927356651301950000e+001) (3, -1.39079572444490350000e-001) (4, -4.35463668798394730000e+000) (5, 1.42756634128209910000e+000) (6, -1.49322767212296980000e+001) (7, -8.91727919787345110000e-001) (8, 9.55292802392181170000e+000) (9, 4.56263558014448560000e+000) (10, -6.69690188424885240000e+001) (11, 1.71098524619483630000e+000) (12, 2.09256038800024410000e+001) (13, -2.61766599301011650000e-001) (14, 5.79418541628100030000e-001) (15, 7.49059860532660780000e+000) (16, -2.01954513220330900000e+001) (17, 7.27749683168507520000e-001) (18, -1.19124503075761330000e+001) (19, -7.64047253334885990000e-001) (20, -1.12948905914802680000e+001) (21, 4.06406302829792660000e+000) (0, -2.89413167080223800000e+000) (1, 2.09968951494387030000e+000) (2, 1.24954557020962430000e+002) (3, 4.70463306138031450000e-002) (4, 1.06218150072896580000e+001) (5, 5.95319694847432220000e-002) (6, 1.63744594035144290000e+001) (7, 3.01991687726244740000e+000) (8, -2.05275588499377020000e+001) (9, 4.67113487577419660000e-001) (10, -4.10148028114255750000e+001) (11, 9.97840140279193320000e-001) (12, 1.78274003430049560000e+002) (13, 7.60979204004816270000e+000) (14, -2.96785869797087380000e-001) (15, -6.79168084946337850000e-001) (16, 2.13315866217172250000e+001) (17, -9.28555556396797850000e+000) (18, -7.97669902706863070000e+001) (19, -5.95443971860452500000e+000) (20, 4.73703021166453980000e+001) (21, -1.08579628812431640000e+000) (0, -1.28419099292309420000e+001) (1, 7.89303861650057750000e+000) (2, 2.93506688944703030000e+001) (3, -2.27490566030409930000e-001) (4, -6.62659630876699790000e+000) (5, 2.15914149479200420000e+000) (6, -2.29779309298785210000e+001) (7, 1.96064780970541100000e+000) (8, -1.71136535751467140000e+001) (9, 2.83480186101594930000e+000) (10, -6.03905716314921380000e+000) (11, 1.25915559319351680000e+000) (12, 9.39209507090239980000e+001) (13, -3.69825087784506510000e-001) (14, -8.18895101350031670000e+000) (15, 1.39865816901825910000e+000) (16, -6.29628731632886040000e+000) (17, -1.65740717923088090000e+000) (18, -2.38330862373068090000e+001) (19, -3.34854633135054370000e+000) (20, 2.49591650448418470000e-001) (21, 3.71400454283961690000e+000) (0, 7.88931407160046570000e+000) (1, 1.40278395986487300000e+000) (2, 4.44391759889922180000e+001) (3, 1.88543183265988210000e-001) (4, 1.47125021074622600000e+001) (5, 1.98246330161494950000e+000) (6, 1.67221534457045320000e+001) (7, 1.46868177882353400000e+000) (8, 6.02910337616456860000e+001) (9, 1.71492950585414340000e+000) (10, 7.80116745683400980000e+001) (11, -8.08101261095418130000e-001) (12, 1.48699807738625030000e+002) (13, 6.85877125622822550000e-001) (14, 5.43383626027413270000e+001) (15, -2.17095447520701010000e-001) (16, 3.75901210973280580000e+001) (17, -1.82801964455292270000e+000) (18, 3.05994565748245220000e+001) (19, -2.21749065972468710000e+000) (20, 6.60298920295600450000e+001) (21, -1.73873437399657020000e+000) (22, -2.04537567944448460000e+000) (23, -2.24261791165908080000e+000) (24, 8.52755566170710320000e-003) (25, 2.28173493186953280000e+000) (26, 2.06677504903504470000e+000) (22, 3.77228763687227220000e+000) (23, 1.89040976502933810000e+000) (24, -3.36972335379733460000e+000) (25, 8.47220023201815110000e-002) (26, 1.36203152027575620000e+000) (22, -2.16269590978290530000e+000) (23, 2.67472242844286150000e-003) (24, 3.94048136539037320000e+000) (25, -2.11043474342222170000e+000) (26, 2.78454462613125510000e-001) 

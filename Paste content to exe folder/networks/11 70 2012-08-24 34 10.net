FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=22 6 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (6, 5, 5.00000000000000000000e-001) (6, 5, 5.00000000000000000000e-001) (6, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 3.03663699429257420000e+000) (1, 4.28181866167700490000e-001) (2, 1.39150427067579140000e+000) (3, 4.10092381746534340000e-001) (4, 4.57283665609836520000e-001) (5, 4.88749173619300480000e-001) (6, 4.85573688123572270000e-002) (7, -1.36961840276563000000e-001) (8, -5.09342427785298700000e-001) (9, 6.50551717925686450000e-002) (10, -4.17898480927068830000e-001) (11, 6.29526763607675120000e-001) (12, -1.00831754742564340000e-001) (13, 3.27327568745660880000e-001) (14, 4.36331150821263840000e-001) (15, 5.72745756902197200000e-001) (16, 5.66426223751048670000e-002) (17, 3.71425469793395800000e-001) (18, -2.44923220875136040000e-001) (19, 4.91860309311000220000e-002) (20, -1.52561250021641610000e-001) (21, 5.96439604911513040000e-001) (0, -3.03445803918198020000e+000) (1, -3.18454349063717770000e+000) (2, -4.51608922553360270000e-001) (3, 5.67502596600378270000e-002) (4, -1.50110005204150630000e+000) (5, -4.01516644745460450000e-001) (6, 1.25457536074481010000e-001) (7, 8.16562091011683200000e-001) (8, 3.83830164089456530000e-001) (9, 3.68195350080624980000e-001) (10, 3.21052005352425860000e+000) (11, 5.14420176473759480000e-001) (12, 2.01612827912229340000e+000) (13, 1.18268671602739770000e-001) (14, 3.07535159447884170000e+000) (15, -5.09360738332446550000e-001) (16, 1.49181551366348950000e-001) (17, 3.23469091606876260000e-001) (18, 1.00866341158203830000e+000) (19, 7.28195390476166770000e-001) (20, 3.20556399876290770000e+000) (21, -3.52633587376421140000e-001) (0, -7.93446522392775800000e-001) (1, 2.08937166915133590000e-001) (2, -1.50178854343402000000e-001) (3, 1.17332145226779370000e-001) (4, 1.15088364237331260000e+000) (5, -8.29966450088712860000e-002) (6, 1.13441635697168100000e+000) (7, -8.57889309114771680000e-001) (8, -8.91183973947164310000e-001) (9, -4.18411611515622010000e-001) (10, 2.15920035052136920000e-001) (11, 6.97606350879769600000e-002) (12, -3.15970828518897930000e+000) (13, -3.44552967836917470000e-001) (14, 7.46113084224315770000e-002) (15, 3.24015929039316590000e-002) (16, -3.04594485575939710000e+000) (17, -2.09660453320873320000e-001) (18, 1.22508809609070400000e+000) (19, -1.37614667272793790000e-002) (20, -3.04751956281411210000e+000) (21, 1.03726428907553530000e-001) (0, -3.31430409047626110000e-001) (1, 9.83563071185529630000e-002) (2, -1.51754864842832650000e+000) (3, 1.86565209389233290000e-001) (4, 3.39321903818352810000e-001) (5, -4.30850576544727030000e-001) (6, 2.30058862381089220000e-001) (7, -5.24544434911361770000e-002) (8, -1.63501819557000440000e-001) (9, -9.51871482752396910000e-002) (10, 3.18005740658595480000e+000) (11, 2.35901709302870190000e-001) (12, -4.93150759323273350000e-001) (13, -4.00522206952174110000e-001) (14, 2.24828522666459420000e-001) (15, 1.16215070715349780000e-001) (16, -9.78484953395375450000e-001) (17, 5.19930296991658190000e-002) (18, 1.22762314546994530000e+000) (19, 2.36406518725676670000e-001) (20, -5.72533084724912160000e-001) (21, -5.11727051907867880000e-002) (0, -1.82078665800497670000e-001) (1, 1.57346085611851870000e-001) (2, 3.46556534661773330000e-001) (3, 1.38531397785720510000e-002) (4, 4.52375491606989700000e-001) (5, -1.75242728198634290000e-001) (6, 5.48341069208863270000e-001) (7, -1.42499667789461380000e-001) (8, -9.46124113069411350000e-001) (9, -1.79096348022294400000e-001) (10, -1.46231359152920890000e+000) (11, 7.38316800705152570000e-002) (12, -3.21153323713310620000e+000) (13, -3.01105627184124310000e-001) (14, 1.43715313358411450000e-001) (15, 1.48520979399870330000e-001) (16, -9.10955655514110510000e-001) (17, -6.14731952533361520000e-002) (18, 3.00659948683644040000e-001) (19, 5.53904482779271760000e-002) (20, -1.16878403803342090000e+000) (21, 6.47249634826364130000e-002) (22, 2.41243583087909550000e-001) (23, 5.60979251083588040000e-001) (24, -2.02678453119784940000e-001) (25, 3.07275384388065120000e-001) (26, -1.85912095448071750000e-001) (27, 2.99873955055320160000e-001) (22, 5.22195654901378200000e-001) (23, -3.60726849215770140000e-001) (24, -4.09938131979685090000e-001) (25, -6.83886611438311310000e-001) (26, -3.44581685693298170000e-001) (27, 5.80881743351557840000e-001) (22, 4.75668554598695450000e-001) (23, -4.21908490752447630000e-001) (24, 4.01687292444045160000e-001) (25, 3.50790074888742530000e-001) (26, 3.94399694679201500000e-001) (27, 5.08579496180420310000e-001) 

FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=22 7 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (7, 5, 5.00000000000000000000e-001) (7, 5, 5.00000000000000000000e-001) (7, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 3.03663699429257420000e+000) (1, 6.74110869811578080000e-001) (2, 3.20510623508421140000e+000) (3, 6.59093166712128450000e-002) (4, 1.47942586889449550000e+000) (5, 1.97615652750302710000e-001) (6, 3.43068576017274200000e-001) (7, -4.99839505496442900000e-001) (8, -3.09557254204578220000e+000) (9, -3.85418288480912780000e-002) (10, -3.20112063932169640000e+000) (11, -1.28166802036333380000e-001) (12, -3.04553592020642850000e+000) (13, -8.87172344166236300000e-002) (14, 6.76778453431562400000e-002) (15, 2.44287426578089660000e-001) (16, 1.38722637502764780000e-001) (17, -9.51345749816148440000e-002) (18, -3.08792469257169520000e-001) (19, -1.38184425237362430000e-001) (20, -3.17463138111447570000e+000) (21, 1.95647240774566110000e-001) (0, -9.27349698727150780000e-001) (1, -3.18454349063717770000e+000) (2, -1.72419333946812110000e-001) (3, -2.58137458736378890000e-001) (4, -3.10923221021807720000e+000) (5, -4.01516644745460450000e-001) (6, 1.25457536074481010000e-001) (7, 4.76159798168770080000e-001) (8, 7.06966223492880520000e-001) (9, 6.51863916177743200000e-001) (10, 1.22341171962594020000e+000) (11, 1.82975851070486530000e-001) (12, 1.33532369343646740000e+000) (13, 1.62344176343986070000e-001) (14, 7.65601521417090920000e-001) (15, -5.09360738332446550000e-001) (16, 2.91015834414908080000e-001) (17, 3.23469091606876260000e-001) (18, 1.00866341158203830000e+000) (19, 3.87793097633253590000e-001) (20, 3.20556399876290770000e+000) (21, -5.49625631510565200000e-001) (0, 5.06228537792227980000e-001) (1, 1.46798293685117740000e-001) (2, 2.26018334977130670000e-001) (3, 7.65513372651239890000e-002) (4, 1.15047804930748590000e-001) (5, 1.93218912708010480000e+000) (6, -1.24145249614751530000e+000) (7, 3.51713222108917810000e-001) (8, 7.62471148547345660000e-002) (9, 1.01972263947790940000e-001) (10, -3.05911013915870010000e+000) (11, -3.91525624786864590000e-001) (12, 5.36788840981986980000e-001) (13, 1.29250753068908190000e+000) (14, 6.02004248341557750000e-001) (15, -1.64662422459459470000e-002) (16, 3.18414065859992500000e+000) (17, -4.30374502318882970000e-001) (18, -3.14604251350096840000e+000) (19, -8.97934797048803720000e-002) (20, 3.18256595154521000000e+000) (21, -6.05027362545001860000e-002) (0, -1.28472055692722580000e-001) (1, -1.51940418190837370000e-002) (2, 1.16158428644082430000e-001) (3, -1.13311980968420950000e-001) (4, -4.55917715590131500000e-001) (5, 1.50285653265458690000e+000) (6, 5.41714831108396290000e-002) (7, 3.76481288884240760000e-001) (8, 1.35762460850990640000e-001) (9, 5.01594998100266310000e-001) (10, 1.43294908580065840000e+000) (11, 1.62461697393861400000e-001) (12, 7.01934314164225800000e-001) (13, 3.45227426429384700000e-001) (14, 6.53725440518513560000e-001) (15, 6.73472355654721880000e-002) (16, 6.02422122794884940000e-001) (17, 5.48960635049352690000e-002) (18, -7.28747091252784360000e-002) (19, 7.77415228297341360000e-002) (20, 3.04884402572447310000e+000) (21, 5.74839541684978440000e-002) (0, 2.02973419630434120000e+000) (1, 8.39060737028431010000e-002) (2, -3.00937155356864160000e-001) (3, 3.90695408801246920000e-001) (4, -1.04609764417122330000e-001) (5, 7.06032675783667130000e-002) (6, -1.62306208682921470000e+000) (7, 5.55174703341418720000e-001) (8, 2.77003918033777270000e-001) (9, 5.32148548436528030000e-001) (10, 1.28555528477619460000e+000) (11, 3.91668161506486530000e-004) (12, 1.26231527954563920000e+000) (13, 2.92358773297830330000e-001) (14, -4.33351889494826280000e-003) (15, 9.96531442499927340000e-002) (16, 3.65049188365835720000e-001) (17, 5.23282080698817210000e-001) (18, -4.58363598710948010000e-002) (19, 1.14488063866498550000e-001) (20, 2.28119831072802090000e+000) (21, 4.85251074272736120000e-001) (0, -3.14899661510748800000e+000) (1, 3.24161892609624720000e-002) (2, -3.17094485762204230000e+000) (3, 4.79923613680655200000e-002) (4, 2.64556877892134780000e-001) (5, 8.94074124692103980000e-002) (6, 5.03237586926012390000e-001) (7, -1.65729326618273410000e-001) (8, 8.09589904016602690000e-003) (9, -1.04937118492041260000e+000) (10, 1.03558905318765060000e+000) (11, 5.22640439691790640000e-001) (12, -1.79255785815373000000e+000) (13, 3.73486931099821780000e-002) (14, -8.38639144986100820000e-002) (15, 1.33983661246241560000e-001) (16, -1.25907334601946140000e+000) (17, -7.00201308947044240000e-001) (18, 1.42658311890892310000e+000) (19, 2.95837395936375810000e-001) (20, -3.10867068677220980000e+000) (21, 7.55619418570093040000e-002) (22, -5.58677222248322500000e-001) (23, 3.85267404568908570000e-001) (24, -3.09772354455603580000e-001) (25, 3.92623788412807540000e-001) (26, 5.00992041304169740000e-001) (27, 1.17637321012576850000e-001) (28, 3.44626072177260320000e-001) (22, 3.29799355908504900000e-001) (23, -4.02958049165513950000e-002) (24, 6.70860054660447510000e-001) (25, 1.52829050734053310000e-001) (26, 4.47348241676689910000e-001) (27, -2.38770421177132370000e-001) (28, 4.67642431432221890000e-001) (22, 5.89578317415928370000e-001) (23, -3.23194832752684290000e-001) (24, -2.91557652707131740000e-001) (25, -5.05465791127040380000e-001) (26, 1.19528731420702830000e-001) (27, 1.73190164171311720000e-001) (28, 7.79952720683183110000e-001) 

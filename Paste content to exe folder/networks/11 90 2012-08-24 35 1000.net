FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=22 5 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 7.47234303263149260000e+001) (1, -1.22444538356296150000e+001) (2, 6.27081267237998760000e+002) (3, 3.91887039255958980000e+000) (4, 1.11887662420127310000e+002) (5, -8.56047045370266210000e-001) (6, 7.92660429424765760000e+000) (7, -1.81812867779116850000e+000) (8, -3.20761010247327310000e+002) (9, -4.81937021115084540000e+000) (10, 1.28558319695932280000e+002) (11, -1.07087362444473280000e+001) (12, 7.16253322711728370000e+001) (13, -1.22588335898945310000e+000) (14, 6.46284727992970650000e-001) (15, -2.94860868980811610000e+000) (16, 4.21322876180958870000e+001) (17, 5.69388181048305330000e+000) (18, 1.19473262653798830000e+002) (19, -9.61913920863065640000e+000) (20, 1.29245327608699000000e+001) (21, -9.43230184533458170000e+000) (0, -1.08274455903265320000e+001) (1, 2.49953023150688550000e+001) (2, -2.98981533668676210000e+001) (3, -1.00628430548452720000e+000) (4, 2.50310835186202410000e+000) (5, -4.02138064074173850000e-001) (6, -2.90246009041662170000e+001) (7, -4.19284617762963400000e+000) (8, -1.60939103500016530000e+002) (9, 1.23016818580148040000e+000) (10, -1.08952048402798670000e+001) (11, 1.33692738723112400000e+001) (12, -7.55686467821436390000e+002) (13, -2.18124466181506310000e+001) (14, 1.09966089907659180000e+002) (15, 1.24330004859762640000e+001) (16, -6.24602630486648920000e+001) (17, 3.22182239098079110000e+001) (18, -1.05197474158826520000e+001) (19, 1.64539965128377550000e+001) (20, 5.50449050128358270000e+001) (21, 1.59082126734784570000e+000) (0, 6.88464993203667670000e+000) (1, 6.49485419254280490000e-001) (2, -3.13341334015892410000e+000) (3, -9.48563712255837690000e-001) (4, -8.78380238893914140000e-001) (5, -2.25191201540316240000e-001) (6, -6.52646760695688770000e+000) (7, -9.66883670361667000000e-001) (8, 7.67827166592192700000e+000) (9, 8.67017689648473810000e-001) (10, -4.91134593166756730000e+001) (11, 1.64960104716777290000e+000) (12, -3.64361159320803550000e+001) (13, -1.38674656224268360000e-001) (14, -8.62326048734893820000e+000) (15, 1.28898973246504280000e+000) (16, -4.51450994029814510000e+001) (17, 4.94905911079237040000e-001) (18, -2.08645709022020110000e+001) (19, 1.61918806086159940000e+000) (20, 3.21580853460786500000e+001) (21, 3.83736816465069720000e+000) (0, 1.09518217573037530000e+001) (1, -8.55347270677549680000e-001) (2, -5.61136684605351290000e+000) (3, 1.91106657173996330000e-001) (4, -7.55439642757024910000e-001) (5, 1.04081177897778110000e+000) (6, 6.11921783163347670000e-001) (7, -8.03910688785289040000e-001) (8, 5.28729791609785240000e+001) (9, 1.00984124952920990000e+000) (10, 8.07319225700856260000e+000) (11, -2.00424956396820030000e+000) (12, 9.80426243570298080000e+001) (13, 4.90795257773759930000e-001) (14, 6.64391594193444210000e+001) (15, -2.66422254453034020000e-001) (16, 2.56992811309021470000e+001) (17, -1.46658263212954700000e+000) (18, 3.93280712677962400000e+001) (19, 8.34542586342630010000e-001) (20, 5.52219992033916750000e+001) (21, -7.66650450201873390000e-001) (22, 4.11910127776665970000e-002) (23, 2.74722708728189200000e+000) (24, -2.93557298221812110000e+000) (25, 2.83209977511566270000e+000) (26, 2.89300782912604330000e+000) (22, 1.03558368916098020000e+000) (23, -1.92221921690508160000e+000) (24, 2.94545794440246930000e+000) (25, -9.45055901523986360000e-002) (26, 1.23161067881020180000e-002) (22, -1.46666370456096430000e+000) (23, -8.94308025139394220000e-002) (24, -2.72058785976797330000e-001) (25, -1.74094164749820070000e+000) (26, 4.59580717655266370000e-001) 

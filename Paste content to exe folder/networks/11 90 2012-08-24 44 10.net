FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=22 6 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (22, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (6, 5, 5.00000000000000000000e-001) (6, 5, 5.00000000000000000000e-001) (6, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -4.48612346720816120000e-002) (1, 2.60245286251917840000e-001) (2, -3.87268690086855550000e-001) (3, 3.35892909373176780000e-001) (4, 6.05959326525493000000e-001) (5, 8.85186208817085040000e-002) (6, 7.59425797386603370000e-002) (7, -1.54046575933362850000e-001) (8, 2.84906977471466320000e-001) (9, 1.39013094183577130000e-001) (10, -8.11157219575172390000e-001) (11, -3.52057801714094070000e-002) (12, 6.42416283962208510000e-001) (13, 6.03047636330778400000e-001) (14, -1.42044957723351150000e+000) (15, 1.02109615704097880000e-001) (16, -1.09546294229714100000e-001) (17, 1.95070486696751780000e-001) (18, -2.14089043173194550000e-002) (19, 2.24857102130842810000e-001) (20, -1.51848288977525470000e+000) (21, 4.92074804843167620000e-001) (0, -1.49255824418604020000e-001) (1, 2.79532395914318170000e-001) (2, 1.01224038726986760000e-001) (3, 3.22410243156644400000e-001) (4, -2.73850773530460890000e-001) (5, 8.61992849096477050000e-002) (6, 1.26644484791052590000e-001) (7, 1.14804610256557940000e-001) (8, 3.31440181289865020000e-001) (9, 3.86594814425569030000e-001) (10, -3.69500570996401380000e-001) (11, 2.61025077523301790000e-001) (12, 6.44186303519833860000e-001) (13, 5.40699573954573240000e-001) (14, -2.33123147346489560000e-001) (15, 1.52781003177910380000e-001) (16, -2.51203864883060290000e-001) (17, 3.48244317104219710000e-001) (18, -4.75468874294167730000e-001) (19, 1.61392745716400250000e-001) (20, 3.26244508646630790000e-001) (21, 6.71187285783638290000e-001) (0, 7.26339014943758630000e-001) (1, -4.07631565307310860000e-002) (2, 1.17285019543501480000e+000) (3, -1.02911108996305120000e-001) (4, -1.00984481714392380000e+000) (5, 5.24479876684115440000e-001) (6, 5.64411260733846730000e-001) (7, -3.02570539026754400000e-001) (8, 2.60585481603822110000e-001) (9, 4.40234475888805540000e-001) (10, -1.47620471664099920000e-001) (11, -1.05488876312763090000e+000) (12, 2.14054855394411600000e-001) (13, 2.17796956158179820000e-001) (14, -3.02723126919653140000e-001) (15, 3.58769942356434060000e-001) (16, 5.87166649588678810000e-001) (17, -9.90942228971628420000e-001) (18, -4.05020232989659890000e-001) (19, -1.35456164842212620000e-001) (20, 1.07390460372369880000e-001) (21, 8.89859978407337300000e-002) (0, 1.77543146276580630000e-001) (1, 6.73626543129615670000e-002) (2, 3.88256908814503750000e-001) (3, -2.99696170910066560000e-001) (4, 1.42771569634508120000e+000) (5, -2.56079991747565070000e-001) (6, 3.20543582493287270000e+000) (7, -3.19143912578645220000e-001) (8, 6.09124314693573420000e-001) (9, -6.12750958512456980000e-001) (10, 6.66829057278417460000e-002) (11, -1.16288813721653480000e-002) (12, -3.04445559792470540000e+000) (13, -4.16889776172012340000e-001) (14, 5.40647754765234190000e-001) (15, 2.96458189880523110000e-002) (16, -1.03526596010085340000e+000) (17, -6.41559552691738480000e-001) (18, 5.17523427584297480000e-001) (19, -1.09017527687871570000e-001) (20, -3.15902469142879290000e+000) (21, 7.60325384292682840000e-002) (0, -1.04241317700423020000e+000) (1, -1.50387157210577080000e-001) (2, -9.98516691975121720000e-001) (3, -4.84845900411724480000e-002) (4, 8.95331747959596800000e-001) (5, -2.52839024902395900000e-001) (6, 8.84132830998897830000e-001) (7, 4.58825157809035910000e-002) (8, -7.79575446590156320000e-001) (9, -2.40033885044991240000e-002) (10, 3.18720462348933160000e+000) (11, 1.77342797142433620000e-001) (12, -2.21299571239336680000e+000) (13, -1.17912949717195530000e+000) (14, 2.80382135303488410000e+000) (15, 3.80337939818175680000e-002) (16, -3.08672244425765550000e+000) (17, 1.99248531853923660000e-001) (18, 2.02324497844709050000e+000) (19, 3.76495879291719740000e-001) (20, -5.34482809019294570000e-001) (21, 1.19230887244207680000e-001) (22, 2.37302170675663780000e-001) (23, 2.86392747579045370000e-001) (24, 6.93334416168014940000e-002) (25, 2.23234230125461470000e-002) (26, 4.09415536942596410000e-001) (27, 1.93784103620944090000e-001) (22, 3.63173516545432120000e-001) (23, 3.53597100387107400000e-001) (24, 3.44071862715761680000e-001) (25, 1.21681621623833890000e-001) (26, -6.32472348958308950000e-001) (27, 5.61118559463313460000e-001) (22, 4.92135840000327120000e-001) (23, 4.65524511478787430000e-001) (24, -2.23572667044418190000e-001) (25, 8.80354247847043640000e-001) (26, 3.46832740638031320000e-001) (27, 4.35232762980530210000e-001) 

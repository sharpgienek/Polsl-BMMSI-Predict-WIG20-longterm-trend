FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=24 5 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (24, 5, 5.00000000000000000000e-001) (24, 5, 5.00000000000000000000e-001) (24, 5, 5.00000000000000000000e-001) (24, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 3.10226457246450910000e+001) (1, -3.79538875320615700000e-001) (2, -2.08608997898872660000e-001) (3, 4.99337172334826370000e+000) (4, 4.67062171939885530000e-001) (5, 3.41734788338060280000e+000) (6, -3.28896856081594300000e-001) (7, -1.07234426380968610000e+001) (8, 7.02477797275998350000e-001) (9, 6.83432107054969370000e-001) (10, -1.11773730333702480000e+000) (11, 1.22326591423753500000e-001) (12, 2.06610743083619600000e-001) (13, 1.93818909592330260000e+000) (14, -4.69505161053867580000e-001) (15, -7.26519429516654820000e-001) (16, 8.15442914419675650000e-001) (17, 1.93703243906166710000e+000) (18, -5.67189185313244690000e-001) (19, -3.17763189463778350000e-002) (20, 1.01742781701074780000e+000) (21, 1.08308846349278460000e+000) (22, -6.07917511912425480000e-002) (23, 2.44674119035382830000e-001) (0, 2.12921277717315190000e+000) (1, 9.51930598823392930000e-001) (2, 5.60307603668995570000e-001) (3, -3.46607567795970890000e-002) (4, -1.84251181000658920000e-003) (5, 6.35181268874662200000e-001) (6, 5.87736897140220700000e-001) (7, 4.73594584909397820000e-001) (8, 5.78607865076755210000e-001) (9, 6.16486683518074010000e-001) (10, -1.14523158672248490000e+000) (11, 3.73614114852072520000e-001) (12, 4.10916862834340040000e-001) (13, 1.04345988630498020000e-001) (14, 6.21737543077367390000e-001) (15, -1.78375717268112110000e-001) (16, 1.80804576724608900000e-002) (17, 1.03525359422973390000e-001) (18, 2.75942345073154680000e-001) (19, 1.16966008016808700000e-001) (20, 4.78761069776742760000e-002) (21, 2.58157846207561940000e-001) (22, 2.51869161959175790000e+000) (23, 2.20621438520687580000e-001) (0, -5.01669665871255520000e+000) (1, 4.92838377358560400000e-001) (2, 4.40866295972553110000e-001) (3, 2.13424306455555380000e-001) (4, -5.84483296144826110000e-002) (5, 2.98326147956021800000e-003) (6, 1.67859123116748400000e+000) (7, 2.25291164238812510000e-001) (8, -7.22520673658812670000e-001) (9, 9.75998589822745650000e-002) (10, 8.28221906377501040000e-001) (11, 6.03728453741462780000e-001) (12, 1.71547341368488880000e+000) (13, 7.54966112581423600000e-002) (14, 1.79738537354738300000e-001) (15, 1.08224353972028810000e+000) (16, 6.21100386298010680000e-001) (17, 1.52694499375416510000e-001) (18, 5.18100196642776040000e-002) (19, 2.67126649497505360000e-001) (20, 1.69394589982082290000e-001) (21, -1.60807908187355710000e-001) (22, -1.66441205619341040000e+000) (23, 1.85954820057170400000e-001) (0, 5.99592419205307150000e-001) (1, -4.37847044158775070000e-002) (2, 4.07568123129708260000e-002) (3, 4.76287560336623010000e+000) (4, 1.27901022158178890000e-001) (5, 1.41230117721259040000e+000) (6, 5.77787927225871360000e-001) (7, -1.46168161197606380000e+000) (8, -3.81733108010630650000e-001) (9, 1.80497059224065000000e-001) (10, -6.30825331697692390000e-001) (11, 1.27596373439576970000e-001) (12, 8.66839795187338420000e-002) (13, 1.47030841840076620000e+000) (14, -4.61571943171102230000e-001) (15, 6.29712121730282350000e-001) (16, 5.92633665254405640000e-002) (17, 2.69877570117091190000e+000) (18, -1.69912097207700540000e-001) (19, 4.36274044056479790000e-001) (20, 1.45476964458740590000e+000) (21, 9.60476176618622720000e-001) (22, -5.02507451272345000000e-002) (23, 8.35860190112921790000e-001) (24, -5.63897227367928360000e-001) (25, -2.14564010172759550000e+000) (26, 1.17311442586538580000e+000) (27, 5.50441420617159420000e-002) (28, 3.12318625058450260000e-001) (24, -9.22927507202726210000e-001) (25, 2.49697489171664970000e+000) (26, 7.97331117356733010000e-001) (27, -2.67497554846783560000e-001) (28, 6.14026379548603280000e-001) (24, 2.63397095652435410000e+000) (25, 1.80727474716845580000e-001) (26, 3.12305477867065420000e-001) (27, 1.33548852124851410000e+000) (28, 1.20413034092703790000e+000) 

FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=24 5 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (24, 5, 5.00000000000000000000e-001) (24, 5, 5.00000000000000000000e-001) (24, 5, 5.00000000000000000000e-001) (24, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -5.81332121001440160000e-001) (1, 1.82747726797406520000e+000) (2, 1.52274028881362880000e+000) (3, 1.57115917280707530000e-001) (4, 1.10598860815929360000e-001) (5, -4.54555272201167330000e-002) (6, 9.08279529822358160000e-001) (7, 3.69015512185250030000e-001) (8, 1.99809183111158440000e-001) (9, 1.01702422474033040000e-001) (10, 4.88324923497454490000e-001) (11, 8.38722760038739800000e-001) (12, 1.93129698465811630000e+000) (13, -2.18539816754068190000e-001) (14, 4.13523634428576920000e-001) (15, 9.86197865373580120000e-001) (16, 3.04183919792604670000e-001) (17, 2.27718068873164760000e-001) (18, 1.63182995816624770000e-001) (19, 3.41098603569480740000e-001) (20, 2.46234847905570010000e-001) (21, 1.50273217997758720000e-001) (22, 1.34205123607988420000e+000) (23, -3.89980388771757280000e-001) (0, 1.35851706152598200000e-001) (1, 4.86145517156887860000e-001) (2, 4.66374435125555760000e-001) (3, -4.44997365461888510000e-001) (4, 2.86882041305592310000e-001) (5, -1.17542458779412380000e-001) (6, -3.59901212463457280000e-002) (7, 2.92955177040879720000e+000) (8, 3.25895353594783900000e+000) (9, 7.04665289133134200000e-001) (10, 5.43439264844049030000e-001) (11, 6.94604915548420080000e-001) (12, 4.08982515065905550000e-001) (13, -1.63843469711757700000e-001) (14, 5.43784753841464410000e-001) (15, -3.25172178668798830000e-001) (16, 1.10851149833725550000e+000) (17, 2.48529236180126620000e-001) (18, 4.82253296272558750000e-001) (19, -1.39212696518484810000e-001) (20, -4.39284549191452660000e-001) (21, 1.21143977244308940000e+000) (22, -2.46739159722348230000e+000) (23, -3.49258731631771680000e-001) (0, 6.15916637031671210000e-001) (1, 1.14016389142748450000e+000) (2, 9.89089961391034330000e-001) (3, 1.29118590276155080000e-002) (4, -5.98024742001190490000e-001) (5, 3.63272084835470470000e-001) (6, 4.32895891557754320000e-001) (7, 1.60397369335550520000e-001) (8, 8.55230760726397250000e-001) (9, 1.19377501138081570000e+000) (10, -2.79757222493514700000e-001) (11, 4.86296609108488260000e-001) (12, -7.77668327949585510000e-001) (13, 1.51278582949076370000e-001) (14, 8.32704879615462470000e-001) (15, -1.54145504280441460000e-001) (16, 4.36284117784357270000e-001) (17, 1.31049256508493780000e-001) (18, 6.98538586011746010000e-001) (19, -1.58389354014378260000e-001) (20, -2.17590588854905150000e-001) (21, 5.11845952379353260000e-001) (22, 5.88459581873921510000e+000) (23, 2.47772358434161230000e-001) (0, 6.36478004291986110000e+001) (1, 1.20820708034136540000e+000) (2, -8.87350246183015170000e-001) (3, 2.83081738526151530000e+002) (4, 1.51726856769695480000e+000) (5, 3.52738412420470930000e+000) (6, -4.64444322259656260000e-001) (7, -1.23835848020612120000e+000) (8, 1.35111947063076590000e+000) (9, 1.14006201314589490000e+000) (10, -1.11216272622489510000e+003) (11, -4.70683074465039370000e-001) (12, 2.43854715187939440000e-001) (13, 1.70393419016225000000e+001) (14, -3.97500261980998730000e+000) (15, -4.11345622664095950000e-002) (16, -5.23714832621797630000e-001) (17, 3.54618810394701920000e+001) (18, -1.48774165590719210000e+000) (19, 1.36382152259871590000e+001) (20, 2.76116299483427110000e+000) (21, 1.96136862886205380000e-001) (22, 2.46905663095483520000e+000) (23, 3.40771287590988900000e-001) (24, 1.92392902051492180000e+000) (25, -5.20039360725323970000e-001) (26, -2.23004214009559340000e+000) (27, -2.06764047107486530000e-001) (28, 5.65194429503841800000e-001) (24, 1.65818626547912130000e+000) (25, 5.06236975132971610000e-001) (26, 2.75822428205560440000e+000) (27, -2.83503466330486120000e-001) (28, 1.00229112324281780000e+000) (24, -1.52758360920385140000e-001) (25, -7.19462467594990220000e-001) (26, 1.82017291429528020000e-001) (27, 3.43331943346425160000e+000) (28, 3.04410886588309100000e+000) 

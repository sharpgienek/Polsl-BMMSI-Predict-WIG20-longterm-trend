FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=24 7 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (24, 5, 5.00000000000000000000e-001) (24, 5, 5.00000000000000000000e-001) (24, 5, 5.00000000000000000000e-001) (24, 5, 5.00000000000000000000e-001) (24, 5, 5.00000000000000000000e-001) (24, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (7, 5, 5.00000000000000000000e-001) (7, 5, 5.00000000000000000000e-001) (7, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -9.05250156783053710000e-002) (1, 2.71661493569099520000e-001) (2, 5.71825198510830250000e-001) (3, 1.33514583074727800000e-001) (4, 2.46304522621315460000e+000) (5, 9.62098307984160790000e-002) (6, -7.88329684113549980000e-001) (7, 4.51073256157002310000e-003) (8, 2.98785938164263690000e-001) (9, -1.50061866820600100000e-001) (10, 2.95768945776780760000e-001) (11, 2.69183466188424040000e-001) (12, 1.74182778359988910000e-002) (13, 1.04095573103422810000e-001) (14, 1.33192274750833230000e-001) (15, 3.72705960549330420000e-001) (16, 1.84772386103696060000e-001) (17, 2.50781366304836440000e-001) (18, 2.72186350444292680000e-001) (19, 2.69433710332777960000e-001) (20, -3.02443782577745790000e-001) (21, 1.88025017713443990000e-001) (22, 3.14703440921596690000e-001) (23, 2.43560907212868220000e-001) (0, -5.24675055138560720000e-001) (1, 1.75615898723600620000e-001) (2, 3.08210818637639770000e+000) (3, 5.58423457293090730000e-002) (4, 3.16059939848350790000e+000) (5, 9.95740358340870270000e-002) (6, -1.23429307221270660000e+000) (7, 2.91369838579269740000e-001) (8, 1.13824326132558130000e+000) (9, -1.00955184838955820000e+000) (10, 3.21072757305860090000e+000) (11, -7.27910928115107770000e-002) (12, -1.44532991489191080000e-001) (13, -1.82965655000114720000e-001) (14, 3.30681445310076430000e-001) (15, 2.67016810872631350000e-001) (16, 1.04908920726270740000e+000) (17, 1.83839173808485470000e-001) (18, 6.75289209495557260000e-001) (19, -9.86303557027506720000e-001) (20, -1.78651644341348610000e-001) (21, -1.80225176443653410000e-001) (22, -1.73024072077199940000e-001) (23, -1.27448076047838340000e-001) (0, -9.49291837725989130000e-001) (1, 2.33950722583275590000e-001) (2, 6.71317369961033190000e-001) (3, -3.60389308820244090000e-002) (4, -6.83697402451116880000e-001) (5, -2.06122040945148600000e-001) (6, -1.09718612703916070000e+000) (7, 1.58578407007015590000e-001) (8, -9.34289396096185330000e-001) (9, -3.54629752951175440000e-001) (10, 1.05721909019635210000e+000) (11, 1.32266150755557420000e-001) (12, 6.88720320086193110000e-001) (13, -4.13014043692384420000e-001) (14, 1.14005600549321830000e+000) (15, 1.65151893433093170000e-001) (16, 5.65050884649625160000e-001) (17, -2.24397097522397980000e-001) (18, 1.09361435441055880000e+000) (19, -2.31330691375716570000e-001) (20, -1.73496252088013030000e-001) (21, -3.49824345485159560000e-001) (22, 1.15625473620334820000e+000) (23, -3.38892948837894060000e-001) (0, -5.54123922593219960000e-001) (1, 9.67568324212101930000e-002) (2, 6.03880863050996860000e-001) (3, -2.53748904600399010000e-001) (4, -5.75364157284724120000e-001) (5, -1.00653289373541750000e-001) (6, -3.21197269026465460000e+000) (7, 2.72473353669853650000e-001) (8, -5.79007956167145950000e-001) (9, -1.58716034379369070000e-001) (10, 1.34937974861787820000e+000) (11, 3.64712742398450030000e-001) (12, 8.57398925239357680000e-001) (13, 6.90623234816539890000e-002) (14, 2.27840070471342970000e-001) (15, 3.82144383283201720000e-001) (16, 3.63581999984641140000e-001) (17, 1.50291287056258160000e-001) (18, 5.24394777882186910000e-001) (19, 4.51742772992173520000e-001) (20, -3.06548831308186730000e+000) (21, 1.75431149999798860000e-001) (22, 1.42963220027988940000e+000) (23, 3.71808422669894780000e-003) (0, -1.03622585361560790000e+000) (1, 4.90121679814063790000e-001) (2, -6.15918781042720580000e-001) (3, 1.07140535114889080000e-001) (4, -3.55598454629582670000e-001) (5, 1.77573484016097130000e-001) (6, -9.59704435537398930000e-001) (7, 7.06044706320638320000e-001) (8, -2.68769783113562870000e-001) (9, 5.51405789858129940000e-001) (10, 4.69084093044627280000e-001) (11, 3.58352879022430680000e-001) (12, 4.17769012510840530000e-001) (13, 2.18322177396624570000e-001) (14, 1.44149008690305180000e+000) (15, 4.63473730198228410000e-001) (16, 5.12434541265216810000e-001) (17, 3.95431736996823710000e-001) (18, -9.57584522273842440000e-002) (19, 4.44949560000321760000e-001) (20, -2.78141152774347280000e-002) (21, 3.41774857855802750000e-002) (22, 1.25899496699616260000e+000) (23, 4.00631932386812660000e-001) (0, -3.81640345337460440000e-001) (1, -7.91672873138736660000e-002) (2, 4.93211916089401010000e-001) (3, 5.16608997423720680000e-001) (4, 3.03251101766859230000e+000) (5, 2.22695266237628590000e-002) (6, 6.17684180215831940000e-001) (7, -1.99372264749638340000e-001) (8, 3.08613040323320840000e+000) (9, -1.32471786489358170000e-002) (10, -8.00168708615769830000e-001) (11, -8.10410666386701530000e-002) (12, -4.21047895036498750000e-001) (13, 1.47409909347874850000e-001) (14, -1.50447196746558380000e+000) (15, 3.60348728661546160000e-001) (16, 2.62003596753889460000e-001) (17, 3.40201023283196960000e-001) (18, 4.54158662825270150000e-002) (19, -9.21681476708672150000e-002) (20, 6.15990826814939220000e-001) (21, -2.48890939339562150000e-002) (22, -3.15041873426930420000e+000) (23, 5.05179053085311080000e-001) (24, -1.30667838542105040000e-001) (25, -8.31368169952141630000e-002) (26, 2.27952018354178780000e-001) (27, 4.88162451269244010000e-001) (28, 4.91140966938627350000e-001) (29, 7.52801830776328450000e-002) (30, 4.46621923306491920000e-001) (24, -7.40699373081055990000e-002) (25, 1.24534293771599750000e-001) (26, 2.56785026596324070000e-001) (27, 2.71330475397398300000e-002) (28, 3.79464939883899900000e-001) (29, 4.92959814621980190000e-001) (30, 4.82620458999161890000e-001) (24, -2.39363149228566360000e-001) (25, -1.86144097822137390000e-001) (26, 2.82702378684653450000e-002) (27, -5.67509009489301390000e-001) (28, 8.38819180590125520000e-003) (29, 3.72838722515261570000e-001) (30, 5.51735379706791210000e-001) 

FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=24 7 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (24, 5, 5.00000000000000000000e-001) (24, 5, 5.00000000000000000000e-001) (24, 5, 5.00000000000000000000e-001) (24, 5, 5.00000000000000000000e-001) (24, 5, 5.00000000000000000000e-001) (24, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (7, 5, 5.00000000000000000000e-001) (7, 5, 5.00000000000000000000e-001) (7, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 7.80190339249991370000e-001) (1, -1.16448741897116560000e-001) (2, 3.02112354229456150000e-001) (3, -3.36639041394643350000e-001) (4, -4.15403566955683700000e-001) (5, -1.73518698441192780000e-001) (6, 3.42297901703267460000e-001) (7, -2.79416605001332890000e-002) (8, -1.76267189401786840000e-002) (9, -3.75687156362642660000e-002) (10, 4.83866996040767170000e-002) (11, -9.22544055990928560000e-002) (12, 5.66378479843353990000e-001) (13, -2.34395135721611470000e-001) (14, 5.03291929787997860000e-001) (15, -4.40732525373877390000e-002) (16, -7.11525946909733480000e-002) (17, -1.17193370814462390000e-001) (18, 4.59854990819202840000e-001) (19, -1.62487560942523420000e-001) (20, 1.45333808128520590000e-001) (21, -1.44638389170202140000e-001) (22, 6.08482963144282230000e-002) (23, -6.48984481602073310000e-002) (0, 5.39424830790933730000e-001) (1, -1.00056906123530340000e-001) (2, 3.31560196547685480000e-001) (3, 2.59049023111538930000e-001) (4, 1.58036562873449070000e+000) (5, -3.21387656180619490000e-002) (6, 3.15516726949631290000e+000) (7, -1.65259724166632040000e-002) (8, 7.56697632257771090000e-001) (9, -6.92924598261776690000e-002) (10, -4.56947933510823970000e-002) (11, -2.04328514109795940000e-001) (12, -7.36391911705785000000e-001) (13, -2.93494589358730410000e-002) (14, -2.94263665215295340000e-001) (15, -3.87385214403475120000e-001) (16, 1.84282362321824790000e-001) (17, 9.17015682585528010000e-002) (18, 2.59135879062229070000e-001) (19, 1.14116013968548270000e-002) (20, 5.25564714785452750000e-001) (21, 1.04489694636550500000e-001) (22, -3.56361994511976700000e-002) (23, -4.56702599603219240000e-002) (0, -1.03377224029779620000e+000) (1, 3.34129805134561530000e-001) (2, -1.26329814191725450000e-001) (3, 1.36109530846850600000e-001) (4, -4.35532961786885050000e-001) (5, 3.27202286326497560000e-001) (6, -1.30050938989938850000e+000) (7, 4.12970973766968120000e-001) (8, -1.71705925803094460000e-001) (9, 1.28217685026127930000e-001) (10, 3.47873997165546120000e-001) (11, 2.91114282498037000000e-001) (12, -9.73214780950916460000e-003) (13, 2.92662519360400830000e-001) (14, 8.49698629147406010000e-001) (15, 3.10682153883371000000e-001) (16, -5.48166541179680890000e-001) (17, 3.94920821665418250000e-001) (18, 9.20146299416835230000e-002) (19, 2.92817163382786900000e-001) (20, -4.79678991331011880000e-001) (21, 1.51757191231389060000e-001) (22, 3.83772885038275390000e-001) (23, 4.14411403475932190000e-001) (0, -2.18226451076356120000e+000) (1, 2.78514569930829960000e-001) (2, -1.21489726228977540000e-001) (3, 1.03693758879442960000e-001) (4, 4.93679741955096510000e-001) (5, 3.07860245022653690000e-001) (6, -3.07798831326813180000e+000) (7, 4.24860622381637690000e-001) (8, -5.97059728501835030000e-003) (9, 1.18250643861982450000e-001) (10, 4.84403917491660450000e-001) (11, 4.32966091252418580000e-001) (12, 1.60928283594615310000e-001) (13, 4.12584396147376020000e-001) (14, 4.15783924115135710000e-001) (15, 5.23695616672694930000e-001) (16, -1.59320571493937140000e-001) (17, 2.95746850926442650000e-001) (18, 4.23564260767186410000e-002) (19, 4.05311061543451530000e-001) (20, -1.37770055315900140000e+000) (21, 1.51714466621377410000e-001) (22, 2.90273285536235880000e-001) (23, 4.36225368644735600000e-001) (0, 2.43811476817127950000e-001) (1, -1.84059441612176190000e-001) (2, 3.40844010105366750000e-001) (3, 5.22934459589818550000e-001) (4, 3.85141095798866740000e-001) (5, -5.33383893941990340000e-002) (6, 1.39638902666212110000e+000) (7, -7.85306300220700580000e-001) (8, 8.40287491107997610000e-001) (9, 1.20999268538419070000e-001) (10, -1.21608262510678560000e+000) (11, -5.06266840661309000000e-001) (12, -4.87357214101676320000e-001) (13, -1.48164665686677350000e-001) (14, -1.42349194702036110000e-001) (15, -4.16691530132189180000e-001) (16, -9.37783274499980330000e-002) (17, -5.02787847781165920000e-001) (18, -1.03984843155634460000e-001) (19, -1.93115002805157860000e-001) (20, 8.02301885778936800000e-001) (21, -3.11464165910696480000e-003) (22, -1.37346177784254260000e+000) (23, -6.34030868097997110000e-002) (0, 9.84089212616658940000e-001) (1, -6.12385461700917090000e-002) (2, 2.76240014698268140000e-001) (3, 2.63533353251605820000e-001) (4, 8.38951716578534290000e-002) (5, -7.08228482257496910000e-002) (6, 3.08823611615521100000e+000) (7, 3.23371803600969450000e-002) (8, 7.38706301745047030000e-001) (9, 2.44221829526341690000e-001) (10, -3.59998529476071140000e-001) (11, -1.02153788440849620000e-001) (12, 1.09449374199583310000e-001) (13, -6.25813196276005930000e-002) (14, -3.91962641280498510000e-001) (15, -1.73393340520157890000e-002) (16, 3.19806909878715460000e-001) (17, -1.00642843632261490000e-001) (18, -1.44791474515237940000e-001) (19, 4.32204322388888420000e-003) (20, 7.39833655623268420000e-001) (21, -9.81193645526070160000e-002) (22, -6.60392830598413140000e-001) (23, -1.39232646415242630000e-001) (24, 3.58496632413397350000e-001) (25, -1.59020244366350820000e-001) (26, 3.79149337055404820000e-001) (27, 3.99974532678224410000e-001) (28, -1.27881645415872990000e-001) (29, -1.41374980431540900000e-001) (30, 3.89983077451215130000e-001) (24, 3.26917042099074790000e-001) (25, -1.04023637719485720000e-002) (26, 3.80359889296650100000e-001) (27, 4.53315212649394120000e-001) (28, 1.12877263633448620000e-001) (29, -6.24958704075773000000e-002) (30, 3.64368678120862490000e-001) (24, 4.11359182029235710000e-001) (25, 9.53771456726379420000e-002) (26, 3.92487575024241700000e-001) (27, 3.82837916677325580000e-001) (28, 9.12609779209277440000e-001) (29, -7.61161903966091760000e-002) (30, 4.67737820286182720000e-001) 

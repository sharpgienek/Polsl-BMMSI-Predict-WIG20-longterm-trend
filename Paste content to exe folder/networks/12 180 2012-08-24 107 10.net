FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=24 7 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (24, 5, 5.00000000000000000000e-001) (24, 5, 5.00000000000000000000e-001) (24, 5, 5.00000000000000000000e-001) (24, 5, 5.00000000000000000000e-001) (24, 5, 5.00000000000000000000e-001) (24, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (7, 5, 5.00000000000000000000e-001) (7, 5, 5.00000000000000000000e-001) (7, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -3.20328738740085850000e+000) (1, 3.89931669009440040000e-001) (2, -3.71479686793125610000e-002) (3, -5.97505307421634440000e-001) (4, 3.06154544192936400000e+000) (5, 6.68523831462957820000e-002) (6, -3.19499881305859910000e+000) (7, 1.17328184625474250000e+000) (8, 4.64693647615702480000e-001) (9, 4.39545309325346520000e-002) (10, 3.15917117580597570000e+000) (11, 4.91507026892052330000e-001) (12, 3.04133534594545400000e-001) (13, 3.57391351931726220000e-001) (14, 3.34430701139992510000e-001) (15, 1.32705919164465100000e+000) (16, 1.11722275319985130000e+000) (17, 7.03867813972040570000e-002) (18, 6.71946032079775990000e-001) (19, 3.25720208881664420000e-001) (20, -5.38744295779539640000e-001) (21, 1.27175486800305600000e-001) (22, 1.30297506014193520000e+000) (23, -1.25406607994956590000e-001) (0, -2.67610126686411760000e-001) (1, 2.55174660073912150000e-001) (2, -9.82071426473367870000e-001) (3, -4.30304915930144970000e-001) (4, -6.04844138192760060000e-001) (5, 1.99982114571305660000e-001) (6, -3.04897830307022400000e+000) (7, 1.35980455346071990000e+000) (8, -1.63943177383927940000e-001) (9, 2.91552727903779290000e-001) (10, 3.08050296174310300000e+000) (11, 5.84196235009680990000e-001) (12, 3.15724856835545160000e+000) (13, 3.82299799568516010000e-001) (14, 1.91582310184638030000e+000) (15, 3.97111946518778400000e-001) (16, 7.70571875306014120000e-001) (17, 5.04324012723451400000e-001) (18, -1.63209685359567920000e-001) (19, 3.33297398846831350000e-001) (20, -3.73765504101916560000e-001) (21, 3.40226937291608410000e-001) (22, 3.18036258237175230000e+000) (23, -5.89385877608265550000e-001) (0, 3.07298343038105330000e+000) (1, 2.97770788114654520000e-001) (2, 1.19715449859613120000e-001) (3, -4.07072877181237970000e-001) (4, -8.51950574925041120000e-001) (5, 5.76666495741288030000e-001) (6, 3.03222415242994270000e+000) (7, 2.47319127206616170000e-001) (8, 3.21052005352425860000e+000) (9, -7.59274660160925150000e-002) (10, -3.09004275680713200000e+000) (11, -4.20221919045344560000e-001) (12, -3.09322268849514170000e+000) (13, 1.74193789676710510000e+000) (14, -3.07876956327977340000e+000) (15, -1.82465442886455440000e-001) (16, -3.09159915331469910000e+000) (17, 3.45235801309877880000e-002) (18, -1.45904072524289850000e+000) (19, 7.06108415080898060000e-001) (20, 1.21441915061922430000e+000) (21, 5.64883323479376950000e-002) (22, -1.45012959229761230000e+000) (23, 1.65348911141414780000e-001) (0, 6.49156742631909920000e-001) (1, 1.72564876625372510000e-001) (2, -1.54629554052849730000e-002) (3, 1.76493954184817610000e-002) (4, -3.42270252959152370000e-001) (5, 7.76734535690775390000e-002) (6, 5.65990237486382550000e-001) (7, 1.60513162029284670000e-001) (8, 1.51086389094346060000e+000) (9, 5.93913393998602160000e-002) (10, -1.19463843617960090000e-001) (11, 1.10665749177125350000e-001) (12, -9.16888272790013390000e-001) (13, 1.46976789923735110000e-001) (14, 3.33022368667102890000e-001) (15, -9.63397998721619850000e-002) (16, -1.10617050217303840000e+000) (17, 6.50798160471251220000e-002) (18, -1.22662085853117610000e-001) (19, 3.77594876637805940000e-001) (20, 4.64899540501006570000e-001) (21, 1.69531365094117090000e-001) (22, 1.74480215410472650000e-001) (23, 2.43750116200062650000e-001) (0, 7.43787556248287520000e-001) (1, -1.10519818887166250000e-001) (2, 6.73797106713698750000e-001) (3, 3.57880227444239640000e-001) (4, -3.18857077498891810000e-001) (5, -9.57628018319442030000e-002) (6, 3.15638186912378680000e+000) (7, -3.40846200203210160000e-001) (8, 8.15863973337934700000e-001) (9, -4.20321576666300770000e-001) (10, -8.46084296152828760000e-001) (11, -2.90327400622296390000e-001) (12, -6.04509403560771430000e-001) (13, -1.13768173193995140000e-001) (14, -7.26479802183083060000e-001) (15, -8.24802676880943830000e-002) (16, -5.10326052547955160000e-001) (17, 1.43850942878724080000e-001) (18, -3.58657582181773020000e-001) (19, -1.39647079829620910000e-001) (20, 7.10000327062686230000e-001) (21, 1.89204334346049440000e-001) (22, -4.42159780691677610000e-001) (23, 2.06086658816365680000e-001) (0, -3.21728018232273040000e-001) (1, 1.19499441935709450000e-001) (2, -1.57222099191262290000e+000) (3, -1.40882051277358510000e-001) (4, 1.85274201184721540000e-001) (5, 5.82131306749892890000e-002) (6, -9.14391934862190060000e-001) (7, 1.05112198114756320000e-001) (8, -3.97448484164440120000e-001) (9, 2.06451624170428260000e-001) (10, 1.14188706020800310000e+000) (11, 9.12938385338466910000e-002) (12, 6.92897804383477260000e-001) (13, 2.13701815999650820000e-001) (14, 9.04033785830890890000e-001) (15, 1.33469132131057540000e-001) (16, 6.46797950180910910000e-001) (17, 3.35583814657261840000e-001) (18, 3.74260309042342250000e-001) (19, 4.53848485914176090000e-001) (20, 4.39592190107414070000e-001) (21, 4.01099962499377130000e-001) (22, 1.01963437349097390000e+000) (23, 6.21944120987357960000e-001) (24, 1.09494522398808040000e-001) (25, 2.51329614651173090000e-001) (26, -3.53342030467884700000e-001) (27, -4.87361556983016840000e-002) (28, -2.34285128193572330000e-001) (29, 4.89826770462134460000e-001) (30, 4.27656359379473270000e-001) (24, 1.73874406170643040000e-001) (25, 1.08653011300757770000e-002) (26, -1.30116523507185470000e-002) (27, -1.72306022756312320000e-001) (28, -1.06933989503287720000e-001) (29, 3.80301121536984980000e-001) (30, 5.18673926333275360000e-001) (24, -2.30440509167321060000e-001) (25, -5.96049567920873820000e-001) (26, 1.87588081963064340000e-001) (27, 1.16548396430612430000e-001) (28, -2.15549327528340440000e-002) (29, 5.21465304654567250000e-002) (30, 6.90519082456291900000e-001) 

FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=24 5 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (24, 5, 5.00000000000000000000e-001) (24, 5, 5.00000000000000000000e-001) (24, 5, 5.00000000000000000000e-001) (24, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (5, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -3.14726932015987430000e+000) (1, 1.53805701158307190000e-001) (2, -2.14029643724491510000e-002) (3, -4.64379031258932370000e-001) (4, -4.17898916195482250000e-001) (5, -1.63035287806914540000e-002) (6, -3.12598025734264250000e+000) (7, 4.02196244527588120000e-001) (8, -3.19328982865813330000e+000) (9, 5.70327049728374220000e-001) (10, 3.06172854740084240000e+000) (11, 8.39430595814955160000e-001) (12, 3.03481814660922120000e+000) (13, 2.49624869553295850000e-001) (14, 3.18475711368723590000e+000) (15, 3.60655716564836030000e-001) (16, 3.14845340220876850000e+000) (17, 2.75152830092683100000e-001) (18, 3.11681888025300240000e+000) (19, 3.66254422857978990000e-001) (20, -5.71948133207797920000e-001) (21, 1.95038599994164510000e-001) (22, 3.06397464118431180000e+000) (23, -1.82878226359287050000e-001) (0, -1.60317524887504500000e+000) (1, 6.16414335748707740000e-001) (2, -4.87307831872090650000e-001) (3, 1.28661531529326910000e-001) (4, 3.07706057887930750000e+000) (5, 4.51474218300671750000e-001) (6, -3.07211062763367250000e+000) (7, 4.95150976764006050000e-001) (8, -4.78054556617176440000e-001) (9, 1.11965277514342270000e+000) (10, 1.60395645005007380000e+000) (11, 7.26638510904364490000e-001) (12, 4.52797455471425530000e-001) (13, 3.24899509383311640000e-001) (14, 8.31320392012849020000e-001) (15, 7.41640952534168290000e-001) (16, 1.35355063246765180000e+000) (17, 4.94363623236648560000e-001) (18, 8.50793158165176200000e-001) (19, 4.53793554272732550000e-001) (20, -4.24775701754153970000e-001) (21, 4.69589452945609780000e-001) (22, 1.09362656144199070000e+000) (23, 3.78360203539313040000e-001) (0, 7.59608902858081860000e-002) (1, 1.29722488963044500000e-001) (2, -1.26015611209165670000e+000) (3, 8.85696502303130880000e-002) (4, -1.32354478252703570000e+000) (5, 1.45776546148070290000e-001) (6, -7.02796353270301880000e-002) (7, -1.11224849756141400000e-002) (8, -1.54976758745758500000e-001) (9, 2.95942043389833930000e-001) (10, 3.40573210920847910000e-001) (11, 1.18290358749662470000e-001) (12, 3.69140697592150010000e-001) (13, 1.76849480437540510000e-001) (14, 2.21987862005129990000e-001) (15, -4.23447011981280320000e-001) (16, 7.75905038569937870000e-002) (17, 4.20698997662941750000e-002) (18, -2.47210194856246010000e-001) (19, 1.31732356485670550000e-001) (20, 5.23515871619090190000e-001) (21, 1.93095228421319320000e-001) (22, 1.03120961105674810000e-001) (23, 1.21624934460058220000e-001) (0, 2.48821471416318160000e-001) (1, -2.24659675034396040000e-001) (2, 2.21146922026749240000e-003) (3, -8.16689818779338380000e-003) (4, 3.92307922661294660000e-001) (5, -3.31489510610659620000e-001) (6, 2.52868042709486320000e-001) (7, -3.46656747164794050000e-001) (8, 8.27614824899829180000e-001) (9, -4.57828655451899820000e-002) (10, 1.26767941728775590000e-001) (11, -2.43409675313792820000e-001) (12, 3.79668108929291810000e-001) (13, 3.49361297982417750000e-002) (14, -8.64297187049221980000e-001) (15, -1.77438765196836240000e-001) (16, 8.80628378599874530000e-001) (17, -1.83772223253250520000e-001) (18, 9.03070387703004410000e-002) (19, -1.56507818550104230000e-001) (20, 1.25214174079479660000e+000) (21, 1.74891589992147930000e-001) (22, -7.19105790096309230000e-002) (23, -3.43977303765492240000e-001) (24, 7.32853030173451490000e-001) (25, 4.05514744959572240000e-001) (26, -5.98060210799316320000e-002) (27, -9.69547413077103400000e-002) (28, 3.28720310221496000000e-001) (24, -1.25675162686458370000e-001) (25, 5.04123144866451820000e-001) (26, -1.36539420660848430000e-001) (27, -9.63871143461270400000e-002) (28, 4.33072118417084100000e-001) (24, -7.72720322106051020000e-001) (25, 2.00426533443128360000e-002) (26, 1.53801776775929490000e-001) (27, -1.34585981355155010000e-001) (28, 6.17653349439045480000e-001) 

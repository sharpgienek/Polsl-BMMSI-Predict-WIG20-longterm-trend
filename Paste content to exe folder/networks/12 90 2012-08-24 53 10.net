FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=24 7 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (24, 5, 5.00000000000000000000e-001) (24, 5, 5.00000000000000000000e-001) (24, 5, 5.00000000000000000000e-001) (24, 5, 5.00000000000000000000e-001) (24, 5, 5.00000000000000000000e-001) (24, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (7, 5, 5.00000000000000000000e-001) (7, 5, 5.00000000000000000000e-001) (7, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 1.83810093271947970000e+000) (1, 2.07765945249883790000e-001) (2, 4.71776269389639350000e-002) (3, -2.19972174964546850000e-001) (4, -4.56204869887470220000e-001) (5, -2.23298591029739310000e-001) (6, -3.17573001394334660000e+000) (7, -1.40405669689554620000e-001) (8, -4.47317629249822270000e-001) (9, 5.14333706907184720000e-001) (10, -2.78174035794496180000e-002) (11, 1.98972589955279220000e-001) (12, 3.05502688714472990000e+000) (13, 2.11759613497907940000e-002) (14, 1.39945031506451920000e+000) (15, -3.43623299853967170000e-001) (16, 3.09138553026464090000e+000) (17, -5.00980611254447930000e-001) (18, 8.28695397190755580000e-001) (19, -1.71719770970991590000e-001) (20, -3.49948893229362960000e-001) (21, 1.81246764516281580000e-001) (22, 3.08434207312843520000e+000) (23, -3.01698250401110260000e-001) (0, 4.67076321376334540000e-001) (1, -1.74787302041491540000e-002) (2, 2.78174202773963760000e-001) (3, 5.07028418347243770000e-001) (4, 2.20088825093755780000e+000) (5, -1.72712391427093660000e-002) (6, 8.97871844875500090000e-001) (7, -3.37819163320945970000e-002) (8, 3.17963016048583840000e+000) (9, -2.13272300499324390000e-001) (10, -3.04224002171981580000e+000) (11, -4.38925911697062130000e-001) (12, -1.07292019009989610000e+000) (13, 6.28107004699480780000e-002) (14, -3.04729373273262190000e+000) (15, -5.99306325167268880000e-002) (16, 6.12193851456119930000e-001) (17, 9.18242106130882640000e-002) (18, -3.03932864472330790000e+000) (19, -5.84650372156225730000e-001) (20, -1.04100248032591300000e-002) (21, 6.78111027598843480000e-002) (22, -3.11406619466510910000e+000) (23, -7.10011551679544520000e-002) (0, -1.25885972296940320000e+000) (1, 1.53812484272861930000e-002) (2, 3.16263186921691910000e+000) (3, 7.64455200732742870000e-001) (4, 3.08683230754054260000e+000) (5, 1.78749940636603820000e-001) (6, 3.14623172248816290000e+000) (7, 4.38918738311336930000e-001) (8, -1.78492952932733930000e-001) (9, 2.76804819327056280000e-002) (10, 5.98569566142631570000e-001) (11, 1.10448527332608650000e-001) (12, -3.19875247522390800000e+000) (13, -4.63082977995487200000e-001) (14, -2.27745533035595420000e-001) (15, 1.34089586177270850000e+000) (16, -1.22773789633377680000e+000) (17, 2.01103350610961740000e-001) (18, 3.52650969604126620000e-001) (19, -7.70137724807569150000e-002) (20, -7.88665377477927200000e-001) (21, 3.74203161175124700000e-001) (22, -3.20227420379201090000e+000) (23, 3.16932279167239540000e-001) (0, 3.24397206771497640000e-001) (1, -1.22037731497024020000e-001) (2, -4.75059938741199120000e-001) (3, 1.35484825030810140000e-001) (4, 2.75588937022598230000e-001) (5, 1.34181749264581680000e+000) (6, 1.89910165081801500000e+000) (7, -1.13719711559821360000e-001) (8, 3.11564700523554010000e+000) (9, -9.47436431973938450000e-001) (10, -1.15202988739313960000e+000) (11, -1.13324257065256570000e+000) (12, 2.87976823559841000000e-001) (13, 4.98178169569585110000e-001) (14, -3.16775271890260070000e+000) (15, -5.61879086187637380000e-001) (16, 6.46901757900676280000e-001) (17, 1.80766836597900600000e-001) (18, -1.24932603142109010000e+000) (19, 4.60433042430600740000e-001) (20, 6.52561818932883670000e-001) (21, 7.31191014452384490000e-002) (22, -3.06512210213891030000e+000) (23, 4.60072935003359720000e-001) (0, 1.12446234552759080000e+000) (1, -2.31963023546672000000e+000) (2, 1.59039315505273420000e+000) (3, 3.06437794178116080000e-002) (4, -7.16634831333256560000e-001) (5, -4.35520055126117470000e-001) (6, 1.20411240293210800000e+000) (7, -1.64658307417921420000e+000) (8, 1.40932455907999810000e+000) (9, -4.32425572658131090000e-001) (10, 2.25207512071408540000e-001) (11, -3.62102991530706190000e-001) (12, -7.20016179039892570000e-001) (13, 8.28575739861420190000e-002) (14, 2.79713741232732460000e-001) (15, -8.13027939894181320000e-001) (16, 2.07223457488248220000e-001) (17, -4.45953955644236290000e-002) (18, -4.10217578342984780000e-001) (19, -5.30754185688999400000e-002) (20, 1.75456867636638340000e-001) (21, -2.06555679619670090000e-001) (22, -9.35418546503635980000e-001) (23, -3.01305989902097890000e-001) (0, -1.16435288562364160000e+000) (1, 6.67800147759493030000e-001) (2, 3.15931155666744260000e+000) (3, -2.58088280535878900000e-001) (4, 4.17498626446243250000e-001) (5, 7.29683779589627990000e-002) (6, -5.80427000400567270000e-001) (7, 3.32833883434803970000e-001) (8, -1.17479600101363110000e+000) (9, -2.28883799867975490000e+000) (10, 3.05741946530538210000e+000) (11, 4.93000929688176550000e-002) (12, -1.17301377442457390000e+000) (13, -9.23263341632652450000e-001) (14, 3.18107059019480240000e+000) (15, 5.53648928708732720000e-001) (16, -2.91958972912580880000e-001) (17, -3.34127727725662350000e-001) (18, 3.21406009263950930000e+000) (19, -1.35225499613920810000e+000) (20, -5.38367673601959470000e-001) (21, -3.40720377466847360000e-001) (22, 3.15105349990376290000e+000) (23, -3.96394353674988930000e-001) (24, 1.22499908561233810000e-001) (25, -6.80017903091621910000e-001) (26, -1.37361360522950450000e-001) (27, -1.57989269859868660000e-001) (28, -2.72520950477198150000e-001) (29, -2.51990250609197940000e-001) (30, 4.41736843336237970000e-001) (24, -3.94451624825238380000e-001) (25, -1.93019644952179410000e-001) (26, 1.60208634482632170000e-001) (27, -2.28213735442430040000e-001) (28, -3.26057381623464590000e-001) (29, 1.59194762234556140000e-001) (30, 5.67641141662555530000e-001) (24, -3.01173348049538600000e-001) (25, 4.50228552932991300000e-001) (26, 1.23935440582744490000e-001) (27, 2.18034936420668760000e-001) (28, 2.81077783448490380000e-001) (29, -4.38032537889196220000e-001) (30, 5.18477322570582540000e-001) 

FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=26 6 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (26, 5, 5.00000000000000000000e-001) (26, 5, 5.00000000000000000000e-001) (26, 5, 5.00000000000000000000e-001) (26, 5, 5.00000000000000000000e-001) (26, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (6, 5, 5.00000000000000000000e-001) (6, 5, 5.00000000000000000000e-001) (6, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 3.15378177142879230000e+000) (1, -8.51961293608024530000e-002) (2, 3.07624270777337030000e+000) (3, -1.01079399288949630000e-001) (4, -1.08595565812181370000e+000) (5, 4.55248746894670380000e-001) (6, -5.64790854333824030000e-001) (7, -6.84196570787968940000e-002) (8, -2.53959515958454560000e-004) (9, -6.73136095878210660000e-002) (10, -1.66589459790447860000e+000) (11, -1.64404415596781340000e-002) (12, -1.03854965155603420000e+000) (13, 2.67606594168586650000e-002) (14, 2.60409986749980280000e-001) (15, 4.69189176789898970000e-001) (16, -8.03060613871100950000e-001) (17, -9.46117772402275570000e-001) (18, 6.68089615686248690000e-001) (19, 5.90444783945109910000e-003) (20, -1.10044540443147780000e+000) (21, 1.00263608222205460000e+000) (22, 1.94347042804332460000e-001) (23, 6.20436913756319370000e-001) (24, 1.88395664629340810000e+000) (25, -1.57793437707444970000e-001) (0, 1.14254623990532570000e+000) (1, -3.58884311796655920000e-001) (2, 7.71913734226298720000e-001) (3, -1.39351182696943670000e+000) (4, -2.21943107497010070000e-001) (5, 3.24071337789703150000e-001) (6, -1.04195541332553400000e+000) (7, -2.01105240085327210000e-001) (8, 1.11066147380520560000e+000) (9, -2.21576060501269190000e+000) (10, -4.48816211730649230000e-001) (11, -8.72451048805268780000e-002) (12, 2.61066384276545550000e-001) (13, -2.29188495254509910000e-001) (14, -1.99710230483432670000e-001) (15, 3.54772021840928990000e-001) (16, -1.68856847823937810000e-001) (17, -9.00866306884226200000e-001) (18, 2.66525832394091170000e-001) (19, -5.05286584702177020000e-002) (20, -9.75207365455910580000e-001) (21, -7.63353952071578330000e-002) (22, 1.46085099115925100000e-001) (23, 3.08074023098199590000e-001) (24, 3.06279939895805100000e-001) (25, -1.40471660105580380000e-001) (0, -1.09980453528130310000e+000) (1, -9.73983954255511320000e-001) (2, 1.18600767652249720000e+000) (3, -7.21442627501863830000e-001) (4, 4.07233436599837790000e-001) (5, 3.76155608626248030000e-001) (6, 1.16974960945131250000e+000) (7, 3.74559525365768920000e-001) (8, 3.15966556057896760000e+000) (9, -1.04717609653675480000e-001) (10, 1.15931259757703890000e+000) (11, 2.40845694317489230000e-001) (12, -7.35025009044347090000e-001) (13, -6.01352927203236960000e-001) (14, -4.08609100230638180000e-001) (15, 4.67641205692614640000e-001) (16, -1.59469098356221180000e+000) (17, -1.49715410308173370000e-001) (18, 1.00869092547277160000e-002) (19, 4.85957856356179000000e-001) (20, -1.68622602468531890000e-001) (21, -5.70050689036368310000e-003) (22, -9.05753266123923150000e-002) (23, 1.46442616738898770000e-001) (24, -1.05070785486220550000e+000) (25, 2.06940664515389920000e-001) (0, 2.72259576588702860000e+000) (1, 1.40852449103717280000e-001) (2, 8.44868610712663500000e-002) (3, 1.86061190011755000000e-001) (4, -4.55697899850478820000e-001) (5, -2.90756580316538840000e-001) (6, -2.27948741260292030000e+000) (7, -2.78338639783066310000e-003) (8, -1.07346786496698110000e+000) (9, 1.53531994273126190000e-001) (10, -3.03356082237173560000e+000) (11, 4.24724580556768150000e-002) (12, 2.17102043200627650000e-001) (13, 5.58216557637269520000e-001) (14, 3.56666636331901150000e-001) (15, -3.55185292214101440000e-001) (16, 1.55134286150208940000e+000) (17, 9.61894356513195710000e-002) (18, 3.84993052769622650000e-001) (19, -2.56903966369218250000e-001) (20, 3.24896234260011240000e-001) (21, 5.18235980106699130000e-001) (22, 1.25213045054315130000e-002) (23, 1.39515779161924340000e-001) (24, 3.14173953492122400000e+000) (25, -6.26955251360520050000e-002) (0, -6.88320621926966970000e-001) (1, -9.79483735972592750000e-002) (2, 6.10384375419556370000e-001) (3, 5.43965604200387530000e-001) (4, -7.30880241320474240000e-001) (5, 5.61580350556617700000e-001) (6, -7.27420944625387920000e-002) (7, 5.15290638029075600000e-001) (8, -1.04204086254555730000e+000) (9, 4.92325833828846870000e-001) (10, -4.17387572721816810000e-003) (11, 4.94807239286349230000e-001) (12, -5.92873682493004650000e-001) (13, 1.84714597403758010000e-001) (14, 2.58625722099258710000e-001) (15, 6.71297149066525380000e-001) (16, 3.84508255982766560000e-001) (17, 1.55625241501542830000e-001) (18, -7.63126511563692320000e-002) (19, 5.34108426319129140000e-001) (20, -7.20536115882478570000e-002) (21, 4.09528917703103350000e-001) (22, 4.74516847451447790000e-001) (23, 4.27310399581227800000e-001) (24, 1.88342782245945670000e+000) (25, 5.61739041965232390000e-001) (26, 3.59808422062463270000e-001) (27, 9.58091564722226420000e-002) (28, -8.59270847280030560000e-001) (29, 1.23078432572995860000e-001) (30, 5.08255225006149610000e-001) (31, 3.96615819045717910000e-001) (26, -5.80554926487556690000e-001) (27, -1.58453822602539870000e-001) (28, 2.00876822638226930000e-001) (29, 9.41032895582277350000e-002) (30, 5.02589250294646290000e-001) (31, 4.65962052483233470000e-001) (26, 5.93708296797057500000e-002) (27, 2.38469908762067860000e-001) (28, 4.50071562074484010000e-001) (29, -5.22031713889761880000e-001) (30, 2.14063386921636070000e-001) (31, 7.45515598389631110000e-001) 

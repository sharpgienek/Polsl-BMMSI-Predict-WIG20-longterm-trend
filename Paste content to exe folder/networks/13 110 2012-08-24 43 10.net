FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=26 6 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (26, 5, 5.00000000000000000000e-001) (26, 5, 5.00000000000000000000e-001) (26, 5, 5.00000000000000000000e-001) (26, 5, 5.00000000000000000000e-001) (26, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (6, 5, 5.00000000000000000000e-001) (6, 5, 5.00000000000000000000e-001) (6, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -1.76540828035658800000e+000) (1, 4.24856786263416110000e-001) (2, -1.08258452413648760000e-001) (3, 2.04523355938678040000e-001) (4, -2.88550928843518820000e-002) (5, 5.16853787390139980000e-001) (6, 1.35956869896357800000e+000) (7, 4.37188117178978150000e-001) (8, -1.77958788725733740000e+000) (9, 6.07187896087300060000e-001) (10, 6.75426862856855160000e-001) (11, 1.51871232014645200000e-001) (12, 2.74411337409667500000e-001) (13, 2.69970039601170280000e-001) (14, 6.13974265039217430000e-003) (15, 2.03925390467010910000e-001) (16, -3.06880862563134380000e+000) (17, 3.39547947101590360000e-001) (18, 1.49354990350410110000e-001) (19, 1.13805473568692580000e-001) (20, 2.15107859140997170000e-002) (21, 1.98698998632540840000e-001) (22, 7.43702343759137240000e-002) (23, 3.07042380216163150000e-002) (24, 3.92856053364693040000e-001) (25, 6.31775130894596690000e-001) (0, 1.20853745182617130000e+000) (1, 2.75777138306636140000e-001) (2, -5.19426580782376200000e-002) (3, 2.17508495218820510000e-001) (4, -1.22456510924173310000e-001) (5, -7.64381451729845560000e-002) (6, -1.61966656987187710000e+000) (7, 3.94926906504464160000e-003) (8, -1.49177702728600090000e-001) (9, -7.68735109520646830000e-002) (10, -1.04634245033600860000e-001) (11, 6.55833145287518380000e-002) (12, 1.56476796575919700000e-001) (13, 3.08648891571122690000e-001) (14, 1.15816513662244040000e+000) (15, -2.38028780693669450000e-001) (16, 7.05194215927222420000e-001) (17, -9.67254484478028420000e-001) (18, 1.10089584865968650000e+000) (19, -2.59668927059345580000e-001) (20, 5.17060269005310770000e-001) (21, 4.94344764527874070000e-001) (22, 4.65989564384236670000e-001) (23, 3.73812536169303930000e-001) (24, 1.88987923857121530000e+000) (25, -3.11289279832210940000e-001) (0, -3.12182986665579690000e+000) (1, 9.87385499441933550000e-001) (2, -6.11652325683586070000e-001) (3, 4.64465406043848840000e-001) (4, 3.06516482674892200000e+000) (5, -2.61298062416288450000e-001) (6, 5.18578023108585700000e-001) (7, 1.00990796037655050000e-001) (8, -3.19736087364067160000e+000) (9, 1.44380625041911290000e-001) (10, -1.16076401838266330000e+000) (11, -8.71239841599007470000e-001) (12, 3.14482181035777850000e+000) (13, 1.88515147183941910000e-001) (14, 7.04781228534916700000e-002) (15, -1.82989955780656740000e-001) (16, 4.11723391947383440000e-001) (17, 6.34465580303619550000e-001) (18, 6.41719511907034250000e-001) (19, -5.56693502709634690000e-001) (20, 3.15947024807605730000e+000) (21, -8.89614374545172120000e-001) (22, 1.18608665407851880000e-001) (23, -4.99637738106153690000e-002) (24, 1.29083482587049090000e+000) (25, -2.30780483836541090000e-001) (0, 3.14561526740085200000e+000) (1, 2.15410424662810490000e-001) (2, 4.17060924341221160000e-001) (3, 7.26328801754844290000e-002) (4, -1.61482037839341320000e+000) (5, 8.70062208552091760000e-002) (6, -1.37830480121488040000e+000) (7, 3.03307579981978940000e-001) (8, 5.51270391397407520000e-001) (9, 3.47447654255748640000e-002) (10, -3.14428213435997470000e-001) (11, 2.18547819671995050000e-002) (12, -1.61834210696151600000e+000) (13, 4.03182988754332120000e-001) (14, 1.63288439606497780000e-001) (15, 2.28152323193436700000e-001) (16, 3.53178844201414650000e-001) (17, 3.68708159867337450000e-002) (18, 6.76361325628789670000e-002) (19, -1.19684877977443550000e-001) (20, -9.00386750185930840000e-002) (21, 1.52593936142231550000e-002) (22, 3.22176164832537250000e-001) (23, 2.93938683357996500000e-001) (24, -5.50654527310774130000e-001) (25, 2.70864739334362930000e-002) (0, 1.30798604654472970000e+000) (1, -1.15073350852014800000e+000) (2, 2.32385237816016940000e-001) (3, -5.40763326691006570000e-001) (4, 4.92569108825029970000e-001) (5, 4.01743901876046920000e-001) (6, -3.67605946696058620000e-001) (7, 3.21296240011998250000e-001) (8, 1.17391831068530590000e+000) (9, -1.09344764818457520000e+000) (10, -1.07068702117452040000e-001) (11, 8.10656488114282620000e-002) (12, 4.87559216998791480000e-002) (13, -5.91556784479138060000e-001) (14, -8.13619696059693530000e-001) (15, 4.01693239932424170000e-001) (16, 1.91649677730532150000e-001) (17, -3.15095524338541990000e-001) (18, -7.33468327677845200000e-001) (19, -2.11196201801217150000e-002) (20, -1.70204184180681560000e-001) (21, -3.32348352415452520000e-001) (22, -1.56678856989887260000e-001) (23, -9.52958590849970200000e-002) (24, -9.64312589902940780000e-001) (25, -1.31905794191803730000e-001) (26, 3.34313331316188510000e-001) (27, 2.55357935023699740000e-001) (28, -9.18290025079241400000e-002) (29, 2.88134759102847580000e-001) (30, -5.35007711370866220000e-001) (31, 1.39338781215405240000e-001) (26, 6.00068499794075170000e-001) (27, -3.19974110048788950000e-001) (28, 3.76561478520600350000e-001) (29, -4.55585010659637190000e-001) (30, -9.38493258603169960000e-002) (31, 5.25654436185219230000e-001) (26, 3.92874363911840880000e-001) (27, -3.29690907068580510000e-001) (28, -6.27134177587407530000e-001) (29, 1.13166323050293890000e-001) (30, 4.76854493970656430000e-001) (31, 4.25680760885069290000e-001) 

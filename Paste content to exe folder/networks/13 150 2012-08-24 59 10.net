FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=26 6 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (26, 5, 5.00000000000000000000e-001) (26, 5, 5.00000000000000000000e-001) (26, 5, 5.00000000000000000000e-001) (26, 5, 5.00000000000000000000e-001) (26, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (6, 5, 5.00000000000000000000e-001) (6, 5, 5.00000000000000000000e-001) (6, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -4.65574193517229340000e-001) (1, -1.66267175486914780000e-001) (2, 6.88933825689253430000e-001) (3, -2.82595744535900970000e-001) (4, -6.82458004221165320000e-002) (5, 9.14357092683562020000e-001) (6, 4.45978554364685030000e-001) (7, -1.12418993876143510000e-001) (8, 1.43984948558838880000e+000) (9, -2.31048716892422770000e+000) (10, 6.87609362778892400000e-001) (11, 1.08583476897406330000e-001) (12, -4.41489004437626950000e-001) (13, -5.28251119473310160000e-001) (14, -1.05402816741168200000e+000) (15, 1.22761833088599280000e-001) (16, -1.09985633968632210000e+000) (17, -2.12200295472214780000e-001) (18, -5.90915832759841720000e-001) (19, -5.43836260437357220000e-001) (20, 2.43272385126661720000e-001) (21, -1.30006552961601660000e-001) (22, 1.05250846499540250000e-001) (23, -1.30438628525409130000e-001) (24, -2.84528478499605550000e-001) (25, -3.89408244276106120000e-002) (0, 3.16025149808769880000e+000) (1, -2.93841054926099910000e-001) (2, 1.97262697409207710000e-001) (3, 1.36961736648585240000e-001) (4, 2.28145295573550680000e-001) (5, -3.50044332259072940000e-001) (6, -4.91413816591005250000e-001) (7, -1.55698356493213810000e-001) (8, -1.22853135337685030000e+000) (9, 1.14612464972322080000e-002) (10, -3.13420169301202640000e+000) (11, 4.13117086275285320000e-002) (12, 3.07108523699339300000e+000) (13, 4.47782236733054780000e-001) (14, 1.06177841643616630000e+000) (15, 9.64850232959521550000e-003) (16, 1.40861655125694800000e+000) (17, -1.62259635887859490000e-001) (18, 1.22648789154677870000e+000) (19, 8.92722412709046760000e-002) (20, 3.21748812593351750000e-001) (21, 2.29657640678788470000e-001) (22, -4.89529687463986730000e-001) (23, -1.34307238288222660000e-002) (24, 3.04614016826230750000e+000) (25, 1.76433172783630470000e-001) (0, 3.18360354921692150000e+000) (1, -1.69755611393370840000e-002) (2, 3.90848106722835490000e-001) (3, 3.40299532673740690000e-003) (4, -2.95556149468402200000e-001) (5, -2.00745518266767180000e-001) (6, -3.06002566651609250000e+000) (7, -1.46558505740567780000e-001) (8, -1.02718490529293630000e+000) (9, -3.88318592517357400000e-002) (10, -2.89743396707691230000e-001) (11, 9.40457907843574640000e-001) (12, 6.50765235396278060000e-001) (13, 4.97810169018596370000e-001) (14, 5.24809816950871480000e-001) (15, 1.62067536537462530000e-001) (16, 1.49346232990354680000e-001) (17, -1.06369843847316790000e-002) (18, 6.02297217095923320000e-001) (19, 2.19960323368914930000e-002) (20, -8.44749878015479960000e-002) (21, 4.93324113437834940000e-001) (22, 3.17176272872797950000e+000) (23, 4.11105565355381950000e-001) (24, 3.59265883188111370000e-001) (25, -4.33487295334017200000e-001) (0, 3.08146121371050710000e+000) (1, -4.08253734683002860000e-002) (2, 1.81827750088748630000e-001) (3, 3.30217303567142410000e-002) (4, -2.33673839001852630000e+000) (5, 1.38474362339724820000e-001) (6, -6.97785237340750710000e-001) (7, 9.44914919895461360000e-002) (8, 2.27667228904936180000e-001) (9, 1.13582961158920720000e-001) (10, 6.40436562478683970000e-002) (11, 1.36765585728721110000e+000) (12, -3.18573367620178780000e+000) (13, 5.10830908638280290000e-001) (14, 3.20633904500871430000e-001) (15, 1.08181677740420800000e-001) (16, 5.65381465209273130000e-002) (17, 4.39097352162262230000e-002) (18, -2.32809366840901140000e-001) (19, 8.54631373881520880000e-002) (20, -1.90419116059844580000e+000) (21, 7.18654569789789920000e-001) (22, 5.80852886838218340000e-001) (23, 4.84565726601393800000e-001) (24, 1.63250350807407210000e-001) (25, 2.00350868566973760000e-001) (0, -3.14908206432751130000e+000) (1, 1.31413412797405020000e+000) (2, 6.29824072974855430000e-001) (3, 2.57725510342030060000e-001) (4, 1.71059156456754960000e-001) (5, -3.09653064973118400000e-001) (6, 3.03014895708651990000e+000) (7, 1.08808467275211420000e-001) (8, -3.05276248281411270000e+000) (9, 1.93603794589760490000e+000) (10, 3.20860965310516640000e+000) (11, -4.21744131096530420000e-001) (12, 1.00799439552768620000e+000) (13, 8.82002216779041860000e-002) (14, 7.03304298679171100000e-001) (15, -4.88313827251755230000e-002) (16, -3.05956790283739630000e+000) (17, 6.24378641110421850000e-001) (18, 5.00273683772754610000e-001) (19, 3.04960618467763010000e-001) (20, 1.27229606592674290000e-001) (21, -2.92831775659961660000e-001) (22, -3.20144919314957140000e-001) (23, 7.30268816080537900000e-002) (24, 5.56964024017465030000e-002) (25, -1.12674157114006970000e-001) (26, -1.98749233359238280000e-001) (27, 6.17668079633701180000e-001) (28, 3.67090054551338860000e-001) (29, 2.72557209097582130000e-001) (30, 2.51949012951569410000e-001) (31, 6.60678066098841610000e-001) (26, 1.45488839528335910000e-001) (27, -3.62154157403079690000e-002) (28, -3.07329137317145120000e-001) (29, 1.14115063517615120000e-001) (30, 4.65145602193204940000e-001) (31, 5.93782393959436880000e-001) (26, 1.76759396519114960000e-001) (27, -1.19780052417340090000e-001) (28, -6.15545545966824980000e-001) (29, 3.32953241506952880000e-001) (30, -4.81234833734720920000e-001) (31, 6.53668441767707440000e-001) 

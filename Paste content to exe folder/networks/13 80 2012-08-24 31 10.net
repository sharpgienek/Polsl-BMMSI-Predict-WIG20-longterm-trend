FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=26 6 4 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (26, 5, 5.00000000000000000000e-001) (26, 5, 5.00000000000000000000e-001) (26, 5, 5.00000000000000000000e-001) (26, 5, 5.00000000000000000000e-001) (26, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) (6, 5, 5.00000000000000000000e-001) (6, 5, 5.00000000000000000000e-001) (6, 5, 5.00000000000000000000e-001) (0, 5, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -3.06352298102133160000e+000) (1, -3.44942982368034560000e-001) (2, -7.40444944037081940000e-002) (3, 6.17109078822529740000e-001) (4, 1.82227845659474810000e-001) (5, -8.70550399515478000000e-002) (6, 4.38092170648393230000e-001) (7, 3.03149623658215630000e-001) (8, -1.07515820538535420000e-001) (9, 1.16903016729071260000e+000) (10, -2.72242695114817470000e-001) (11, 3.85899825437244940000e-001) (12, 1.68852366078256540000e-001) (13, 2.00965300955428440000e-001) (14, 3.39920134739241320000e-001) (15, 6.60489246940773040000e-002) (16, 8.07195444804528430000e-001) (17, 5.39021544635444050000e-001) (18, -9.43508911906720820000e-002) (19, -1.83504050152486110000e-001) (20, 1.38989488594035080000e+000) (21, 5.42875037287474220000e-001) (22, 5.04910285192061940000e-001) (23, 1.65840979292087830000e-001) (24, 3.20114505338456020000e+000) (25, 3.11821003563121310000e-001) (0, 3.14465091191773190000e+000) (1, 2.87397358509194210000e-002) (2, 2.52758380139360380000e+000) (3, -2.91258000352348780000e-001) (4, 1.31279968081710190000e-002) (5, -9.48827733380157130000e-001) (6, -3.09916751280247650000e+000) (7, -2.01485348066156310000e-002) (8, -6.11237547734680200000e-001) (9, 8.08745272045838240000e-003) (10, -2.60109275903367910000e-001) (11, 4.02114973004922220000e-001) (12, 3.88378901221601680000e-001) (13, 3.54640841814714390000e-001) (14, 3.15928103908886280000e+000) (15, 4.58717865715896510000e-001) (16, 5.71205015198272830000e-003) (17, 1.98163194011976460000e-001) (18, 4.35891958672885330000e-001) (19, -3.41943919571333910000e-002) (20, 1.97419496276473040000e-001) (21, 2.14672967329279370000e-001) (22, 1.14377304656423150000e+000) (23, 3.33510965003120220000e-001) (24, 5.35423719963949880000e-001) (25, -4.58767235265426010000e-001) (0, -3.13240725939153730000e+000) (1, 8.98006122221250980000e-001) (2, 9.02260822105891200000e-001) (3, 6.57518679500858740000e-001) (4, 3.63943098411265260000e-001) (5, -1.70967830559555200000e-001) (6, 3.17514407643461550000e+000) (7, 1.78745645333500150000e-001) (8, -1.78136216425952120000e-001) (9, 1.70788834514907030000e+000) (10, 3.11179568681877590000e+000) (11, 2.62755263881787100000e-001) (12, -1.09624399161870610000e-001) (13, 1.12575883896342210000e+000) (14, 6.37416391363678000000e-001) (15, -1.38657893721131490000e-002) (16, 1.02740675891509570000e+000) (17, 5.82494849432596330000e-001) (18, -1.41573020269724540000e+000) (19, 3.16851767636653180000e-001) (20, 3.02897708206905760000e+000) (21, 6.34590734133151700000e-001) (22, 2.02226865792918820000e-001) (23, -1.90132920133285750000e-002) (24, 3.15855472071866480000e+000) (25, 2.59159869965909340000e-001) (0, -2.92282459245526200000e-001) (1, 3.72264603456571920000e-001) (2, 5.66950233258652350000e-002) (3, 4.99982548836284120000e-001) (4, 8.67292196760524940000e-001) (5, 4.57695447321333330000e-001) (6, 5.13862377188248010000e-001) (7, -1.28526707967446110000e-001) (8, 1.20875305682750680000e-001) (9, 1.94832531227358330000e-001) (10, -5.39683004409283520000e-001) (11, -4.27316205676778860000e-001) (12, 1.63518685689208730000e+000) (13, 1.93734929442509970000e-001) (14, 1.22698442278142810000e-001) (15, -1.95947940164692010000e+000) (16, 3.03108889850677610000e+000) (17, 2.98474411749097000000e-001) (18, 9.15155804853569620000e-002) (19, -3.27783829333370470000e-001) (20, 3.11908938809933560000e+000) (21, -2.43680581098966730000e-001) (22, -2.53611475456353930000e-001) (23, -1.55077129772683370000e-001) (24, 3.15169436905393760000e+000) (25, -2.22267305748867770000e-001) (0, -2.22173023186933550000e-001) (1, -8.09199043787146990000e-002) (2, 4.64779147993255430000e-001) (3, 6.10775733498815640000e-001) (4, 6.57424100127384950000e-001) (5, -8.65995977844102870000e-003) (6, 6.31798634079458470000e-003) (7, -3.78781773030651190000e-001) (8, -9.21380523382550170000e-001) (9, 6.40334327037720420000e-001) (10, -1.17135935496987310000e-001) (11, 7.13058671500612080000e-001) (12, 6.32698644261207370000e-001) (13, 7.11956465595917080000e-003) (14, 1.69451554221542500000e+000) (15, -6.14532989509123830000e-001) (16, 2.54356886855860110000e-001) (17, -7.07758612588066800000e-002) (18, 3.46505191979125270000e-001) (19, -1.65567059479161500000e-001) (20, 3.49438576098341280000e-001) (21, -1.70399070244832830000e-001) (22, 4.88184184464044880000e-001) (23, 4.68430273169756220000e-002) (24, 1.08398911012650640000e+000) (25, -1.00836641517129190000e+000) (26, 1.95671986807645500000e-001) (27, 7.79592862792685140000e-001) (28, 9.45611454572266450000e-002) (29, 1.13338403738893440000e-001) (30, -7.74162392441064130000e-002) (31, 4.08686305734371590000e-001) (26, 4.35873588363614280000e-001) (27, -5.61151764653627700000e-001) (28, 4.90621433534585480000e-001) (29, 3.30042578032953840000e-001) (30, -1.76274053552896100000e-001) (31, 3.85559254944195180000e-001) (26, -5.00972681734197510000e-002) (27, -6.04042172753416320000e-001) (28, -2.49295278725319510000e-001) (29, -3.98690308588960650000e-001) (30, -2.22828829194735120000e-001) (31, 6.24659564613635590000e-001) 
